# ğŸš€ Enhanced Agent System Design
## æ™ºèƒ½ä»£ç†ç³»çµ±å¢å¼·è¨­è¨ˆæ–¹æ¡ˆ

### ğŸ“‹ ç›®æ¨™æ¦‚è¿°
åŸºæ–¼ç¾æœ‰çš„ FastAPI + LangGraph æ¶æ§‹ï¼Œè¨­è¨ˆä¸€å€‹å®Œæ•´çš„è§£æ±ºæ–¹æ¡ˆä¾†è§£æ±ºï¼š
1. **å¾ªç’°é‡è¤‡ä»»å‹™å•é¡Œ** - è™•ç†å¤§é‡ä¿¡ä»¶æ™‚çš„æ–·é»çºŒå‚³å’Œä¸Šä¸‹æ–‡ç®¡ç†
2. **è¨˜æ†¶ç³»çµ±æ•´åˆ** - å¯¦ç¾ user memoryã€conversation memoryã€task memory
3. **é•·åºåˆ—ä»»å‹™ç®¡ç†** - è¤‡é›œä»»å‹™çš„è¦åŠƒå’Œ token é™åˆ¶è™•ç†
4. **åˆ†æ®µé–±è®€æ©Ÿåˆ¶** - å¤§å‹å…§å®¹çš„ä¸¦è¡Œè™•ç†å’Œåˆä½µ

---

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹è¨­è¨ˆ

### å››å±¤æ ¸å¿ƒæ¶æ§‹

```
Enhanced Agent System
â”œâ”€â”€ 1. TaskExecutionEngine/     # ä»»å‹™åŸ·è¡Œå¼•æ“
â”‚   â”œâ”€â”€ TaskManager            # ä»»å‹™ç”Ÿå‘½é€±æœŸç®¡ç†
â”‚   â”œâ”€â”€ ProgressTracker        # é€²åº¦è¿½è¹¤å’ŒæŒä¹…åŒ–
â”‚   â”œâ”€â”€ CheckpointManager      # æ–·é»çºŒå‚³æ©Ÿåˆ¶
â”‚   â”œâ”€â”€ DeduplicationService   # ä»»å‹™å»é‡
â”‚   â””â”€â”€ TaskScheduler          # ä»»å‹™èª¿åº¦å™¨
â”‚
â”œâ”€â”€ 2. MemoryManagementSystem/  # è¨˜æ†¶ç®¡ç†ç³»çµ±
â”‚   â”œâ”€â”€ WorkingMemory          # å·¥ä½œè¨˜æ†¶ (ç•¶å‰ä»»å‹™)
â”‚   â”œâ”€â”€ SessionMemory          # æœƒè©±è¨˜æ†¶ (å°è©±ä¸Šä¸‹æ–‡)
â”‚   â”œâ”€â”€ LongTermMemory         # é•·æœŸè¨˜æ†¶ (ç”¨æˆ¶åå¥½/æ­·å²)
â”‚   â”œâ”€â”€ MemoryCompressor       # æ™ºèƒ½å£“ç¸®å™¨
â”‚   â”œâ”€â”€ MemoryRetrieval        # èªç¾©æª¢ç´¢
â”‚   â””â”€â”€ MemoryLifecycle        # è¨˜æ†¶ç”Ÿå‘½é€±æœŸç®¡ç†
â”‚
â”œâ”€â”€ 3. ContentProcessingEngine/ # å…§å®¹è™•ç†å¼•æ“
â”‚   â”œâ”€â”€ ContentChunker         # æ™ºèƒ½åˆ†å¡Šå™¨
â”‚   â”œâ”€â”€ ParallelProcessor      # ä¸¦è¡Œè™•ç†å”èª¿å™¨
â”‚   â”œâ”€â”€ ResultAggregator       # çµæœåˆä½µå™¨
â”‚   â”œâ”€â”€ QualityAssurance       # ä¸€è‡´æ€§æª¢æŸ¥
â”‚   â””â”€â”€ WorkspaceManager       # è‡¨æ™‚å·¥ä½œç©ºé–“ç®¡ç†
â”‚
â””â”€â”€ 4. ContextManager/          # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    â”œâ”€â”€ TokenBudgetManager     # Token é ç®—ç®¡ç†
    â”œâ”€â”€ ContextWindow          # å‹•æ…‹ä¸Šä¸‹æ–‡çª—å£
    â”œâ”€â”€ PriorityQueue          # ä¿¡æ¯å„ªå…ˆç´šæ’åº
    â”œâ”€â”€ ContextOptimizer       # ä¸Šä¸‹æ–‡å„ªåŒ–
    â””â”€â”€ ToolResultManager      # Tool Result ç”Ÿå‘½é€±æœŸ
```

---

## ğŸ¯ æ ¸å¿ƒå•é¡Œè§£æ±ºæ–¹æ¡ˆ

### 1. å¾ªç’°é‡è¤‡ä»»å‹™å•é¡Œ

**å•é¡Œï¼š** è™•ç† 1000 å°ä¿¡ä»¶æ™‚å¦‚ä½•é¿å…æ–·é»å’Œç´¯ç©ç„¡æ„ç¾©ä¸Šä¸‹æ–‡

**è§£æ±ºæ–¹æ¡ˆï¼š**
- **ä»»å‹™ç‹€æ…‹æ©Ÿï¼š** `PENDING â†’ RUNNING â†’ PAUSED â†’ COMPLETED/FAILED`
- **æ‰¹æ¬¡è™•ç†ï¼š** å°‡ 1000 å°ä¿¡ä»¶åˆ†æˆ 50 å°ä¸€æ‰¹è™•ç†
- **æ–·é»çºŒå‚³ï¼š** æ¯è™•ç†ä¸€æ‰¹è‡ªå‹•ä¿å­˜æª¢æŸ¥é»
- **é€²åº¦æŒä¹…åŒ–ï¼š** ä»»å‹™ç‹€æ…‹å­˜å„²åˆ°æ•¸æ“šåº«ï¼Œé‡å•Ÿå¾Œå¯æ¢å¾©
- **æ™ºèƒ½å»é‡ï¼š** é¿å…é‡è¤‡è™•ç†ç›¸åŒä¿¡ä»¶

**å¯¦ç¾ç­–ç•¥ï¼š**
```python
class EmailProcessingTask:
    async def process_emails_batch(self, emails: List[Email], batch_size: int = 50):
        # 1. æª¢æŸ¥æœªå®Œæˆä»»å‹™
        checkpoint = await self.checkpoint_manager.load_checkpoint(task_id)
        start_index = checkpoint.get('last_processed_index', 0)
        
        # 2. åˆ†æ‰¹è™•ç†
        for i in range(start_index, len(emails), batch_size):
            batch = emails[i:i+batch_size]
            results = await self.parallel_process_batch(batch)
            
            # 3. ä¿å­˜æª¢æŸ¥é»
            await self.checkpoint_manager.save_checkpoint(
                task_id, {'last_processed_index': i + batch_size}
            )
            
            # 4. æ¸…ç†å·¥ä½œè¨˜æ†¶
            await self.memory_manager.cleanup_working_memory()
```

### 2. è¨˜æ†¶ç³»çµ±æ•´åˆ

**ä¸‰å±¤è¨˜æ†¶æ¶æ§‹ï¼š**

| è¨˜æ†¶å±¤ç´š | å®¹é‡é™åˆ¶ | ç”Ÿå‘½é€±æœŸ | ç”¨é€” |
|---------|---------|---------|------|
| **Working Memory** | 2K tokens | ä»»å‹™æœŸé–“ | ç•¶å‰ä»»å‹™çš„è‡¨æ™‚ä¿¡æ¯ |
| **Session Memory** | 5K tokens | å°è©±æœŸé–“ | ç•¶å‰å°è©±çš„ä¸Šä¸‹æ–‡ |
| **Long-term Memory** | Vector DB | æ°¸ä¹… | ç”¨æˆ¶åå¥½ã€è™•ç†ç¶“é©— |

**è¨˜æ†¶ç”Ÿå‘½é€±æœŸç®¡ç†ï¼š**
```python
class MemoryManager:
    async def manage_memory_lifecycle(self, context: Dict):
        # 1. å·¥ä½œè¨˜æ†¶ â†’ æœƒè©±è¨˜æ†¶
        if self.working_memory.is_full():
            important_items = self.filter_by_importance(
                self.working_memory.items, threshold=0.7
            )
            await self.session_memory.store(important_items)
            await self.working_memory.clear()
        
        # 2. æœƒè©±è¨˜æ†¶ â†’ é•·æœŸè¨˜æ†¶
        if self.session_memory.is_full():
            compressed_summary = await self.compress_session_memory()
            await self.long_term_memory.store_vector(compressed_summary)
```

### 3. é•·åºåˆ—ä»»å‹™ç®¡ç†

**Token é ç®—åˆ†é…ç­–ç•¥ï¼š**
```
ç¸½é ç®— 20K tokens:
â”œâ”€â”€ System Prompt: 4K tokens (20%)
â”œâ”€â”€ Context Memory: 8K tokens (40%)
â”œâ”€â”€ Tool Results: 6K tokens (30%)
â””â”€â”€ Response Generation: 2K tokens (10%)
```

**æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†ï¼š**
- **å‹•æ…‹èª¿æ•´ï¼š** æ ¹æ“šä»»å‹™é‡è¦æ€§é‡æ–°åˆ†é…é ç®—
- **æ™ºèƒ½å£“ç¸®ï¼š** ä¿ç•™é—œéµä¿¡æ¯ï¼Œå£“ç¸®å†—é¤˜å…§å®¹
- **å·¥å…·çµæœç®¡ç†ï¼š** åŠæ™‚æ¸…ç†éæœŸçš„ tool results
- **å„ªå…ˆç´šæ’åºï¼š** åŸºæ–¼é‡è¦æ€§ä¿ç•™æœ€ç›¸é—œä¿¡æ¯

### 4. åˆ†æ®µé–±è®€æ©Ÿåˆ¶

**ä¸¦è¡Œè™•ç†ç­–ç•¥ï¼ˆä½ çš„å¥½æƒ³æ³•ï¼ï¼‰ï¼š**

```
50å°ä¿¡ä»¶ â†’ ContentChunker â†’ [Chunk1, Chunk2, ..., ChunkN]
                                â†“
                        ParallelProcessor
                       â†™        â†“        â†˜
                LLM Instance1  LLM Instance2  LLM Instance3
                (è™•ç†Chunk1-3) (è™•ç†Chunk4-6) (è™•ç†Chunk7-N)
                       â†˜        â†“        â†™
                        ResultAggregator
                              â†“
                       [åˆä½µå¾Œçš„æ‘˜è¦]
```

**å¯¦ç¾ç´°ç¯€ï¼š**
```python
class ParallelContentProcessor:
    async def process_large_content(self, content: str, query: str):
        # 1. å‰µå»ºè‡¨æ™‚å·¥ä½œç©ºé–“
        workspace = await self.workspace_manager.create_workspace()
        
        # 2. æ™ºèƒ½åˆ†å¡Š
        chunks = await self.content_chunker.chunk_content(
            content, max_tokens=2000, overlap=200
        )
        
        # 3. ä¸¦è¡Œè™•ç†
        tasks = [
            self.process_chunk_with_llm(chunk, query, workspace.id)
            for chunk in chunks
        ]
        chunk_results = await asyncio.gather(*tasks)
        
        # 4. åˆä½µçµæœ
        final_result = await self.result_aggregator.merge_results(
            chunk_results, query
        )
        
        # 5. æ¸…ç†å·¥ä½œç©ºé–“
        await self.workspace_manager.cleanup_workspace(workspace.id)
        return final_result
```

---

## ğŸ”„ å®Œæ•´æ•¸æ“šæµè¨­è¨ˆ

### éšæ®µ 1ï¼šä»»å‹™æ¥æ”¶å’Œåˆ†è§£
```
User Query â†’ TaskAnalyzer â†’ TaskDecomposer â†’ TaskQueue
                â†“
        [Task: è™•ç†1000å°ä¿¡ä»¶]
                â†“
    [SubTask1: ä¿¡ä»¶1-50] [SubTask2: ä¿¡ä»¶51-100] ... [SubTask20: ä¿¡ä»¶951-1000]
```

### éšæ®µ 2ï¼šå…§å®¹è™•ç†å’Œä¸¦è¡Œåˆ†æ
```
WebpageData(50å°ä¿¡ä»¶) â†’ ContentChunker â†’ [èªç¾©åˆ†å¡Š]
                                              â†“
                                    ParallelProcessor
                                   (å¤šå€‹LLMå¯¦ä¾‹ä¸¦è¡Œè™•ç†)
                                              â†“
                                    ResultAggregator
                                          â†“
                                   [çµ±ä¸€æ‘˜è¦çµæœ]
```

### éšæ®µ 3ï¼šè¨˜æ†¶ç®¡ç†å’Œç‹€æ…‹æ›´æ–°
```
ProcessedData â†’ MemoryClassifier â†’ [Working/Session/LongTerm Memory]
                      â†“
              ProgressTracker â†’ CheckpointManager â†’ æŒä¹…åŒ–å­˜å„²
```

---

## ğŸ› ï¸ èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆ

### LangGraph æ“´å±•

**æ“´å±• State å®šç¾©ï¼š**
```python
class EnhancedAgentState(TypedDict):
    messages: Annotated[list, add_messages]
    # æ–°å¢å­—æ®µ
    task_context: Dict[str, Any]      # ä»»å‹™ä¸Šä¸‹æ–‡
    memory_context: Dict[str, Any]    # è¨˜æ†¶ä¸Šä¸‹æ–‡
    progress_state: Dict[str, Any]    # é€²åº¦ç‹€æ…‹
    workspace_id: str                 # å·¥ä½œç©ºé–“ID
    chunk_results: List[Dict]         # åˆ†å¡Šè™•ç†çµæœ
```

**æ–°å¢ Graph ç¯€é»ï¼š**
```python
# åœ¨ç¾æœ‰ supervisor_agent.py ä¸­æ·»åŠ 
workflow.add_node("memory_manager", self.memory_manager_node)
workflow.add_node("content_processor", self.content_processor_node)
workflow.add_node("checkpoint_manager", self.checkpoint_manager_node)
workflow.add_node("result_aggregator", self.result_aggregator_node)

# æ›´æ–°æµç¨‹æ§åˆ¶
workflow.add_conditional_edges(
    "supervisor",
    self.enhanced_should_continue,
    {
        "tools": "tools",
        "memory": "memory_manager",
        "process_content": "content_processor",
        "checkpoint": "checkpoint_manager",
        "respond": "response_generator",
        "__end__": END,
    },
)
```

### å·¥å…·ç³»çµ±æ“´å±•

**å¢å¼· ToolManagerï¼š**
```python
class EnhancedToolManager(ToolManager):
    async def execute_parallel_tools(self, tool_configs: List[Dict]) -> List[Dict]:
        """ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹å·¥å…·"""
        tasks = [
            self.execute_tool(config['name'], config['params'])
            for config in tool_configs
        ]
        return await asyncio.gather(*tasks)
    
    def manage_tool_result_lifecycle(self, result_id: str, ttl: int):
        """ç®¡ç†å·¥å…·çµæœç”Ÿå‘½é€±æœŸ"""
        self.result_cache[result_id] = {
            'data': result_data,
            'expires_at': time.time() + ttl
        }
```

---

## ğŸ“Š è©³ç´°å¯¦ç¾è¦ç¯„

### ä»»å‹™ç®¡ç†ç³»çµ±

**TaskManager é¡è¨­è¨ˆï¼š**
```python
class TaskManager:
    def __init__(self, db_connection, checkpoint_manager):
        self.db = db_connection
        self.checkpoint_manager = checkpoint_manager
        self.active_tasks = {}
    
    async def create_task(self, task_config: Dict) -> str:
        """å‰µå»ºæ–°ä»»å‹™"""
        task_id = str(uuid.uuid4())
        task = Task(
            id=task_id,
            type=task_config['type'],
            status=TaskStatus.PENDING,
            config=task_config,
            created_at=datetime.now()
        )
        await self.db.save_task(task)
        return task_id
    
    async def execute_task(self, task_id: str):
        """åŸ·è¡Œä»»å‹™"""
        task = await self.db.get_task(task_id)
        task.status = TaskStatus.RUNNING
        
        try:
            if task.type == 'email_processing':
                await self.execute_email_processing_task(task)
            elif task.type == 'content_analysis':
                await self.execute_content_analysis_task(task)
            
            task.status = TaskStatus.COMPLETED
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
        finally:
            await self.db.update_task(task)
```

**CheckpointManager é¡è¨­è¨ˆï¼š**
```python
class CheckpointManager:
    async def save_checkpoint(self, task_id: str, state: Dict):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = Checkpoint(
            task_id=task_id,
            state=state,
            timestamp=datetime.now()
        )
        await self.db.save_checkpoint(checkpoint)

    async def load_checkpoint(self, task_id: str) -> Optional[Dict]:
        """è¼‰å…¥æª¢æŸ¥é»"""
        checkpoint = await self.db.get_latest_checkpoint(task_id)
        return checkpoint.state if checkpoint else None

    async def cleanup_old_checkpoints(self, retention_days: int = 7):
        """æ¸…ç†èˆŠæª¢æŸ¥é»"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        await self.db.delete_checkpoints_before(cutoff_date)
```

### è¨˜æ†¶ç®¡ç†ç³»çµ±

**MemoryHierarchy é¡è¨­è¨ˆï¼š**
```python
class MemoryHierarchy:
    def __init__(self, vectordb_client, config):
        self.working_memory = WorkingMemory(max_tokens=2000)
        self.session_memory = SessionMemory(max_tokens=5000)
        self.long_term_memory = LongTermMemory(vectordb_client)
        self.compressor = MemoryCompressor()

    async def store_memory(self, content: Dict, memory_type: MemoryType):
        """å­˜å„²è¨˜æ†¶åˆ°æŒ‡å®šå±¤ç´š"""
        if memory_type == MemoryType.WORKING:
            await self.working_memory.store(content)
        elif memory_type == MemoryType.SESSION:
            await self.session_memory.store(content)
        elif memory_type == MemoryType.LONG_TERM:
            await self.long_term_memory.store_vector(content)

    async def recall_memory(self, query: str, memory_types: List[MemoryType]) -> List[Dict]:
        """å¾æŒ‡å®šè¨˜æ†¶å±¤ç´šæª¢ç´¢ç›¸é—œè¨˜æ†¶"""
        results = []

        if MemoryType.WORKING in memory_types:
            results.extend(await self.working_memory.search(query))
        if MemoryType.SESSION in memory_types:
            results.extend(await self.session_memory.search(query))
        if MemoryType.LONG_TERM in memory_types:
            results.extend(await self.long_term_memory.vector_search(query))

        return self._rank_by_relevance(results, query)
```

**MemoryCompressor é¡è¨­è¨ˆï¼š**
```python
class MemoryCompressor:
    async def compress_working_memory(self, memories: List[Dict]) -> Dict:
        """å£“ç¸®å·¥ä½œè¨˜æ†¶"""
        # 1. åˆ†é¡è¨˜æ†¶é¡å‹
        categorized = self._categorize_memories(memories)

        # 2. æå–é—œéµä¿¡æ¯
        key_insights = await self._extract_key_insights(categorized)

        # 3. ç”Ÿæˆå£“ç¸®æ‘˜è¦
        compressed = await self._generate_summary(key_insights)

        return {
            'type': 'compressed_working_memory',
            'original_count': len(memories),
            'summary': compressed,
            'key_insights': key_insights,
            'compression_ratio': len(compressed) / sum(len(str(m)) for m in memories)
        }

    async def compress_session_memory(self, session_data: Dict) -> Dict:
        """å£“ç¸®æœƒè©±è¨˜æ†¶"""
        # æå–å°è©±è¦é»ã€ç”¨æˆ¶åå¥½ã€é‡è¦æ±ºç­–
        conversation_summary = await self._summarize_conversation(session_data)
        user_preferences = self._extract_user_preferences(session_data)
        important_decisions = self._extract_decisions(session_data)

        return {
            'type': 'compressed_session_memory',
            'conversation_summary': conversation_summary,
            'user_preferences': user_preferences,
            'important_decisions': important_decisions,
            'session_duration': session_data.get('duration'),
            'task_completion_rate': session_data.get('completion_rate')
        }
```

### å…§å®¹è™•ç†å¼•æ“

**ContentChunker é¡è¨­è¨ˆï¼š**
```python
class ContentChunker:
    def __init__(self, max_chunk_tokens: int = 2000, overlap_tokens: int = 200):
        self.max_chunk_tokens = max_chunk_tokens
        self.overlap_tokens = overlap_tokens
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    async def chunk_content(self, content: str, content_type: str = 'email') -> List[ContentChunk]:
        """æ™ºèƒ½åˆ†å¡Šå…§å®¹"""
        if content_type == 'email':
            return await self._chunk_emails(content)
        elif content_type == 'webpage':
            return await self._chunk_webpage(content)
        else:
            return await self._chunk_generic(content)

    async def _chunk_emails(self, emails_content: str) -> List[ContentChunk]:
        """å°ˆé–€è™•ç†ä¿¡ä»¶å…§å®¹çš„åˆ†å¡Š"""
        # 1. è§£æä¿¡ä»¶çµæ§‹
        emails = self._parse_emails(emails_content)

        # 2. æŒ‰èªç¾©é‚Šç•Œåˆ†çµ„
        chunks = []
        current_chunk = []
        current_tokens = 0

        for email in emails:
            email_tokens = len(self.tokenizer.encode(str(email)))

            if current_tokens + email_tokens > self.max_chunk_tokens and current_chunk:
                # å‰µå»ºç•¶å‰å¡Š
                chunks.append(ContentChunk(
                    id=f"chunk_{len(chunks)}",
                    content=current_chunk,
                    token_count=current_tokens,
                    metadata={'type': 'email_group', 'email_count': len(current_chunk)}
                ))
                current_chunk = []
                current_tokens = 0

            current_chunk.append(email)
            current_tokens += email_tokens

        # è™•ç†æœ€å¾Œä¸€å¡Š
        if current_chunk:
            chunks.append(ContentChunk(
                id=f"chunk_{len(chunks)}",
                content=current_chunk,
                token_count=current_tokens,
                metadata={'type': 'email_group', 'email_count': len(current_chunk)}
            ))

        return chunks
```

**ParallelProcessor é¡è¨­è¨ˆï¼š**
```python
class ParallelProcessor:
    def __init__(self, max_workers: int = 3, llm_config: Dict = None):
        self.max_workers = max_workers
        self.llm_config = llm_config
        self.worker_pool = []

    async def process_chunks_parallel(self, chunks: List[ContentChunk], query: str, workspace_id: str) -> List[ProcessingResult]:
        """ä¸¦è¡Œè™•ç†å…§å®¹å¡Š"""
        # 1. åˆå§‹åŒ–å·¥ä½œæ± 
        await self._initialize_worker_pool()

        # 2. å‰µå»ºè™•ç†ä»»å‹™
        tasks = []
        for i, chunk in enumerate(chunks):
            worker = self.worker_pool[i % len(self.worker_pool)]
            task = self._process_single_chunk(worker, chunk, query, workspace_id)
            tasks.append(task)

        # 3. ä¸¦è¡ŒåŸ·è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 4. è™•ç†ç•°å¸¸çµæœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # é‡è©¦æ©Ÿåˆ¶
                retry_result = await self._retry_chunk_processing(chunks[i], query, workspace_id)
                processed_results.append(retry_result)
            else:
                processed_results.append(result)

        return processed_results

    async def _process_single_chunk(self, worker: LLMWorker, chunk: ContentChunk, query: str, workspace_id: str) -> ProcessingResult:
        """è™•ç†å–®å€‹å…§å®¹å¡Š"""
        prompt = self._build_chunk_processing_prompt(chunk, query)

        try:
            response = await worker.process(prompt)

            return ProcessingResult(
                chunk_id=chunk.id,
                summary=response.get('summary'),
                key_points=response.get('key_points'),
                relevance_score=response.get('relevance_score'),
                processing_time=response.get('processing_time'),
                worker_id=worker.id
            )
        except Exception as e:
            return ProcessingResult(
                chunk_id=chunk.id,
                error=str(e),
                processing_time=0,
                worker_id=worker.id
            )
```

**ResultAggregator é¡è¨­è¨ˆï¼š**
```python
class ResultAggregator:
    async def merge_chunk_results(self, results: List[ProcessingResult], original_query: str) -> AggregatedResult:
        """åˆä½µåˆ†å¡Šè™•ç†çµæœ"""
        # 1. éæ¿¾æœ‰æ•ˆçµæœ
        valid_results = [r for r in results if not r.error]

        # 2. æŒ‰ç›¸é—œæ€§æ’åº
        sorted_results = sorted(valid_results, key=lambda x: x.relevance_score, reverse=True)

        # 3. æå–é—œéµä¿¡æ¯
        all_summaries = [r.summary for r in sorted_results]
        all_key_points = []
        for r in sorted_results:
            all_key_points.extend(r.key_points)

        # 4. ç”Ÿæˆçµ±ä¸€æ‘˜è¦
        unified_summary = await self._generate_unified_summary(all_summaries, original_query)

        # 5. å»é‡å’Œæ’åºé—œéµé»
        deduplicated_points = await self._deduplicate_key_points(all_key_points)

        # 6. è³ªé‡è©•ä¼°
        quality_score = self._assess_result_quality(unified_summary, deduplicated_points)

        return AggregatedResult(
            unified_summary=unified_summary,
            key_points=deduplicated_points,
            source_chunks=len(valid_results),
            quality_score=quality_score,
            processing_stats={
                'total_chunks': len(results),
                'successful_chunks': len(valid_results),
                'failed_chunks': len(results) - len(valid_results),
                'average_relevance': sum(r.relevance_score for r in valid_results) / len(valid_results) if valid_results else 0
            }
        )
```

### ä¸Šä¸‹æ–‡ç®¡ç†å™¨

**TokenBudgetManager é¡è¨­è¨ˆï¼š**
```python
class TokenBudgetManager:
    def __init__(self, total_budget: int = 20000):
        self.total_budget = total_budget
        self.budget_allocation = {
            'system_prompt': 4000,      # 20%
            'context_memory': 8000,     # 40%
            'tool_results': 6000,       # 30%
            'response_generation': 2000  # 10%
        }
        self.current_usage = {key: 0 for key in self.budget_allocation}

    async def allocate_tokens(self, component: str, requested_tokens: int) -> int:
        """ç‚ºçµ„ä»¶åˆ†é… tokens"""
        available = self.budget_allocation[component] - self.current_usage[component]
        allocated = min(requested_tokens, available)
        self.current_usage[component] += allocated
        return allocated

    async def optimize_context_for_budget(self, context: Dict) -> Dict:
        """æ ¹æ“šé ç®—å„ªåŒ–ä¸Šä¸‹æ–‡"""
        current_tokens = self._calculate_tokens(context)
        budget_limit = self.budget_allocation['context_memory']

        if current_tokens <= budget_limit:
            return context

        # å£“ç¸®ç­–ç•¥
        optimized_context = await self._compress_context(context, budget_limit)
        return optimized_context

    async def _compress_context(self, context: Dict, target_tokens: int) -> Dict:
        """å£“ç¸®ä¸Šä¸‹æ–‡åˆ°ç›®æ¨™ token æ•¸"""
        # 1. æŒ‰é‡è¦æ€§æ’åºä¿¡æ¯
        prioritized_items = self._prioritize_context_items(context)

        # 2. é¸æ“‡æœ€é‡è¦çš„é …ç›®
        selected_items = []
        current_tokens = 0

        for item in prioritized_items:
            item_tokens = self._calculate_tokens(item)
            if current_tokens + item_tokens <= target_tokens:
                selected_items.append(item)
                current_tokens += item_tokens
            else:
                # å˜—è©¦å£“ç¸®è©²é …ç›®
                compressed_item = await self._compress_single_item(item, target_tokens - current_tokens)
                if compressed_item:
                    selected_items.append(compressed_item)
                break

        return {'compressed_context': selected_items, 'compression_ratio': current_tokens / self._calculate_tokens(context)}
```

---

## ğŸš€ å¯¦ç¾è·¯ç·šåœ–

### Phase 1: åŸºç¤æ¶æ§‹ (2-3 é€±)

**Week 1-2: ä»»å‹™ç®¡ç†ç³»çµ±**
- [ ] å¯¦ç¾ `TaskManager` é¡
- [ ] å¯¦ç¾ `ProgressTracker` é¡
- [ ] å¯¦ç¾ `CheckpointManager` é¡
- [ ] é›†æˆåˆ°ç¾æœ‰ `supervisor_agent.py`
- [ ] æ·»åŠ ä»»å‹™ç‹€æ…‹æ•¸æ“šåº«è¡¨
- [ ] å¯¦ç¾åŸºæœ¬çš„æ–·é»çºŒå‚³åŠŸèƒ½

**Week 2-3: è¨˜æ†¶åˆ†å±¤æ¶æ§‹**
- [ ] æ“´å±•ç¾æœ‰è¨˜æ†¶ç³»çµ±
- [ ] å¯¦ç¾ `MemoryHierarchy` é¡
- [ ] é›†æˆ vectordb ä½œç‚ºé•·æœŸè¨˜æ†¶
- [ ] å¯¦ç¾ `MemoryCompressor` é¡
- [ ] å¯¦ç¾è¨˜æ†¶ç”Ÿå‘½é€±æœŸç®¡ç†
- [ ] æ·»åŠ è¨˜æ†¶æª¢ç´¢å’Œèªç¾©æœç´¢

### Phase 2: æ ¸å¿ƒåŠŸèƒ½ (3-4 é€±)

**Week 4-5: ä¸¦è¡Œå…§å®¹è™•ç†**
- [ ] å¯¦ç¾ `ContentChunker` é¡
- [ ] å¯¦ç¾ `ParallelProcessor` é¡
- [ ] å¯¦ç¾ `WorkspaceManager` é¡
- [ ] æ·»åŠ  LLM å·¥ä½œæ± ç®¡ç†
- [ ] å¯¦ç¾éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

**Week 5-6: çµæœåˆä½µå’Œè³ªé‡ä¿è­‰**
- [ ] å¯¦ç¾ `ResultAggregator` é¡
- [ ] å¯¦ç¾ `QualityAssurance` é¡
- [ ] æ·»åŠ çµæœä¸€è‡´æ€§æª¢æŸ¥
- [ ] å¯¦ç¾æ™ºèƒ½å»é‡ç®—æ³•
- [ ] æ·»åŠ è³ªé‡è©•ä¼°æŒ‡æ¨™

**Week 6-7: ä¸Šä¸‹æ–‡ç®¡ç†**
- [ ] å¯¦ç¾ `TokenBudgetManager` é¡
- [ ] å¯¦ç¾ `ContextOptimizer` é¡
- [ ] æ·»åŠ æ™ºèƒ½å£“ç¸®ç®—æ³•
- [ ] å¯¦ç¾ `ToolResultManager` é¡
- [ ] å„ªåŒ–å·¥å…·çµæœç”Ÿå‘½é€±æœŸç®¡ç†

### Phase 3: å„ªåŒ–å’Œæ“´å±• (2-3 é€±)

**Week 8-9: æ€§èƒ½å„ªåŒ–**
- [ ] ä¸¦è¡Œè™•ç†æ€§èƒ½èª¿å„ª
- [ ] è¨˜æ†¶æª¢ç´¢å„ªåŒ–
- [ ] æ•¸æ“šåº«æŸ¥è©¢å„ªåŒ–
- [ ] ç·©å­˜ç­–ç•¥å¯¦ç¾
- [ ] è² è¼‰å‡è¡¡å„ªåŒ–

**Week 9-10: ç›£æ§å’Œèª¿è©¦**
- [ ] æ·»åŠ æ€§èƒ½ç›£æ§
- [ ] å¯¦ç¾èª¿è©¦å·¥å…·
- [ ] æ·»åŠ æ—¥èªŒåˆ†æ
- [ ] å¯¦ç¾å¥åº·æª¢æŸ¥
- [ ] æ·»åŠ éŒ¯èª¤è¿½è¹¤

**Week 10: é«˜ç´šåŠŸèƒ½**
- [ ] æ™ºèƒ½ä»»å‹™èª¿åº¦
- [ ] è‡ªé©æ‡‰å£“ç¸®ç®—æ³•
- [ ] ç”¨æˆ¶åå¥½å­¸ç¿’
- [ ] é æ¸¬æ€§è¨˜æ†¶ç®¡ç†
- [ ] è‡ªå‹•åŒ–æ¸¬è©¦å¥—ä»¶

---

## ğŸ“Š æ•¸æ“šåº«è¨­è¨ˆ

### ä»»å‹™ç®¡ç†è¡¨

```sql
-- ä»»å‹™è¡¨
CREATE TABLE tasks (
    id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL,
    type VARCHAR(50) NOT NULL,
    status ENUM('PENDING', 'RUNNING', 'PAUSED', 'COMPLETED', 'FAILED') DEFAULT 'PENDING',
    config JSON,
    progress_data JSON,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    completed_at TIMESTAMP NULL,
    INDEX idx_user_status (user_id, status),
    INDEX idx_type_status (type, status)
);

-- æª¢æŸ¥é»è¡¨
CREATE TABLE checkpoints (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    task_id VARCHAR(36) NOT NULL,
    state JSON NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE,
    INDEX idx_task_timestamp (task_id, timestamp)
);

-- ä»»å‹™ä¾è³´è¡¨
CREATE TABLE task_dependencies (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    parent_task_id VARCHAR(36) NOT NULL,
    child_task_id VARCHAR(36) NOT NULL,
    dependency_type ENUM('SEQUENTIAL', 'PARALLEL', 'CONDITIONAL') DEFAULT 'SEQUENTIAL',
    FOREIGN KEY (parent_task_id) REFERENCES tasks(id) ON DELETE CASCADE,
    FOREIGN KEY (child_task_id) REFERENCES tasks(id) ON DELETE CASCADE,
    UNIQUE KEY unique_dependency (parent_task_id, child_task_id)
);
```

### è¨˜æ†¶ç®¡ç†è¡¨

```sql
-- å·¥ä½œè¨˜æ†¶è¡¨
CREATE TABLE working_memory (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    session_id VARCHAR(36) NOT NULL,
    content JSON NOT NULL,
    importance_score FLOAT DEFAULT 0.5,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    INDEX idx_session_importance (session_id, importance_score),
    INDEX idx_expires (expires_at)
);

-- æœƒè©±è¨˜æ†¶è¡¨
CREATE TABLE session_memory (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL,
    session_id VARCHAR(36) NOT NULL,
    compressed_content JSON NOT NULL,
    summary TEXT,
    key_insights JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_session (user_id, session_id)
);

-- é•·æœŸè¨˜æ†¶å…ƒæ•¸æ“šè¡¨ (å‘é‡å­˜å„²åœ¨ Weaviate)
CREATE TABLE long_term_memory_metadata (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL,
    vector_id VARCHAR(100) NOT NULL,
    content_type VARCHAR(50),
    tags JSON,
    importance_score FLOAT,
    access_count INT DEFAULT 0,
    last_accessed TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_type (user_id, content_type),
    INDEX idx_importance (importance_score),
    UNIQUE KEY unique_vector (vector_id)
);
```

### å…§å®¹è™•ç†è¡¨

```sql
-- å·¥ä½œç©ºé–“è¡¨
CREATE TABLE workspaces (
    id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL,
    task_id VARCHAR(36),
    status ENUM('ACTIVE', 'COMPLETED', 'EXPIRED') DEFAULT 'ACTIVE',
    config JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE SET NULL,
    INDEX idx_user_status (user_id, status),
    INDEX idx_expires (expires_at)
);

-- å…§å®¹å¡Šè¡¨
CREATE TABLE content_chunks (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    workspace_id VARCHAR(36) NOT NULL,
    chunk_id VARCHAR(100) NOT NULL,
    content LONGTEXT NOT NULL,
    metadata JSON,
    token_count INT,
    processing_status ENUM('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED') DEFAULT 'PENDING',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (workspace_id) REFERENCES workspaces(id) ON DELETE CASCADE,
    INDEX idx_workspace_status (workspace_id, processing_status)
);

-- è™•ç†çµæœè¡¨
CREATE TABLE processing_results (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    chunk_id BIGINT NOT NULL,
    worker_id VARCHAR(100),
    summary TEXT,
    key_points JSON,
    relevance_score FLOAT,
    processing_time_ms INT,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (chunk_id) REFERENCES content_chunks(id) ON DELETE CASCADE,
    INDEX idx_chunk_relevance (chunk_id, relevance_score)
);
```

---

## ğŸ”§ é…ç½®æ–‡ä»¶æ“´å±•

### å¢å¼·çš„ config.yaml

```yaml
# æ‡‰ç”¨åŸºæœ¬é…ç½®
app:
  name: "Enhanced SupervisorAgent"
  version: "2.0.0"
  debug: false
  host: "localhost"
  port: 8000

# ä»»å‹™åŸ·è¡Œå¼•æ“é…ç½®
task_execution:
  max_concurrent_tasks: 5
  checkpoint_interval: 50  # æ¯è™•ç†50å€‹é …ç›®ä¿å­˜æª¢æŸ¥é»
  task_timeout: 3600      # ä»»å‹™è¶…æ™‚æ™‚é–“(ç§’)
  retry_attempts: 3
  batch_size: 50          # æ‰¹æ¬¡è™•ç†å¤§å°

# è¨˜æ†¶ç®¡ç†ç³»çµ±é…ç½®
memory:
  # å·¥ä½œè¨˜æ†¶é…ç½®
  working_memory:
    max_tokens: 2000
    cleanup_interval: 300  # 5åˆ†é˜æ¸…ç†ä¸€æ¬¡
    importance_threshold: 0.3

  # æœƒè©±è¨˜æ†¶é…ç½®
  session_memory:
    max_tokens: 5000
    compression_threshold: 0.8
    retention_hours: 24

  # é•·æœŸè¨˜æ†¶é…ç½®
  long_term_memory:
    vector_db_collection: "agent_memories"
    embedding_model: "text-embedding-ada-002"
    similarity_threshold: 0.7
    max_results: 10

# å…§å®¹è™•ç†å¼•æ“é…ç½®
content_processing:
  # åˆ†å¡Šé…ç½®
  chunking:
    max_chunk_tokens: 2000
    overlap_tokens: 200
    chunk_strategy: "semantic"  # semantic, fixed, adaptive

  # ä¸¦è¡Œè™•ç†é…ç½®
  parallel_processing:
    max_workers: 3
    worker_timeout: 120
    retry_failed_chunks: true
    load_balancing: "round_robin"  # round_robin, least_loaded, random

  # å·¥ä½œç©ºé–“é…ç½®
  workspace:
    cleanup_interval: 3600  # 1å°æ™‚æ¸…ç†ä¸€æ¬¡
    max_workspace_age: 86400  # 24å°æ™‚éæœŸ
    auto_cleanup: true

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨é…ç½®
context_management:
  # Token é ç®—é…ç½®
  token_budget:
    total_budget: 20000
    allocation:
      system_prompt: 4000
      context_memory: 8000
      tool_results: 6000
      response_generation: 2000

  # å£“ç¸®é…ç½®
  compression:
    enable_auto_compression: true
    compression_threshold: 0.9  # ç•¶ä½¿ç”¨ç‡è¶…é90%æ™‚å£“ç¸®
    compression_ratio_target: 0.7
    preserve_recent_items: 10

# å·¥å…·çµæœç®¡ç†é…ç½®
tool_result_management:
  default_ttl: 1800  # 30åˆ†é˜
  max_cache_size: 100
  cleanup_interval: 600  # 10åˆ†é˜æ¸…ç†ä¸€æ¬¡
  compression_enabled: true

# æ€§èƒ½ç›£æ§é…ç½®
monitoring:
  enable_metrics: true
  metrics_interval: 60  # 1åˆ†é˜æ”¶é›†ä¸€æ¬¡æŒ‡æ¨™
  log_slow_operations: true
  slow_operation_threshold: 5000  # 5ç§’

# æ•¸æ“šåº«é…ç½®
database:
  # ä¸»æ•¸æ“šåº«
  primary:
    host: "localhost"
    port: 3306
    database: "enhanced_agent"
    username: "agent_user"
    password: "${DB_PASSWORD}"
    pool_size: 10

  # å‘é‡æ•¸æ“šåº«
  vector_db:
    type: "weaviate"
    host: "localhost"
    port: 8080
    scheme: "http"
    timeout: 30

# å®‰å…¨é…ç½®
security:
  enable_rate_limiting: true
  rate_limit_requests: 100
  rate_limit_window: 60
  enable_input_validation: true
  max_content_size: 10485760  # 10MB
```

---

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦

```python
# tests/test_task_manager.py
import pytest
from enhanced_agent.task_execution import TaskManager, TaskStatus

class TestTaskManager:
    @pytest.fixture
    async def task_manager(self):
        # è¨­ç½®æ¸¬è©¦æ•¸æ“šåº«
        db = await setup_test_database()
        checkpoint_manager = CheckpointManager(db)
        return TaskManager(db, checkpoint_manager)

    async def test_create_task(self, task_manager):
        """æ¸¬è©¦ä»»å‹™å‰µå»º"""
        task_config = {
            'type': 'email_processing',
            'emails': ['email1', 'email2'],
            'batch_size': 50
        }

        task_id = await task_manager.create_task(task_config)
        assert task_id is not None

        task = await task_manager.get_task(task_id)
        assert task.status == TaskStatus.PENDING
        assert task.config == task_config

    async def test_checkpoint_recovery(self, task_manager):
        """æ¸¬è©¦æª¢æŸ¥é»æ¢å¾©"""
        # å‰µå»ºä»»å‹™ä¸¦è¨­ç½®æª¢æŸ¥é»
        task_id = await task_manager.create_task({'type': 'test'})
        await task_manager.checkpoint_manager.save_checkpoint(
            task_id, {'processed_count': 25}
        )

        # æ¨¡æ“¬é‡å•Ÿå¾Œæ¢å¾©
        checkpoint = await task_manager.checkpoint_manager.load_checkpoint(task_id)
        assert checkpoint['processed_count'] == 25
```

### é›†æˆæ¸¬è©¦

```python
# tests/test_parallel_processing.py
import pytest
from enhanced_agent.content_processing import ParallelProcessor, ContentChunker

class TestParallelProcessing:
    @pytest.fixture
    async def processor(self):
        return ParallelProcessor(max_workers=2)

    async def test_email_batch_processing(self, processor):
        """æ¸¬è©¦ä¿¡ä»¶æ‰¹æ¬¡ä¸¦è¡Œè™•ç†"""
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        emails = [f"Email {i} content..." for i in range(50)]
        chunker = ContentChunker()
        chunks = await chunker.chunk_content('\n'.join(emails), 'email')

        # ä¸¦è¡Œè™•ç†
        query = "ç¸½çµé€™äº›ä¿¡ä»¶çš„ä¸»è¦å…§å®¹"
        workspace_id = "test_workspace"
        results = await processor.process_chunks_parallel(chunks, query, workspace_id)

        # é©—è­‰çµæœ
        assert len(results) == len(chunks)
        assert all(r.summary for r in results if not r.error)
```

### æ€§èƒ½æ¸¬è©¦

```python
# tests/test_performance.py
import pytest
import time
from enhanced_agent.memory import MemoryHierarchy

class TestPerformance:
    async def test_memory_compression_performance(self):
        """æ¸¬è©¦è¨˜æ†¶å£“ç¸®æ€§èƒ½"""
        memory_hierarchy = MemoryHierarchy()

        # æº–å‚™å¤§é‡æ¸¬è©¦æ•¸æ“š
        large_memories = [
            {'content': f'Memory {i}' * 100, 'importance': 0.5}
            for i in range(1000)
        ]

        start_time = time.time()
        compressed = await memory_hierarchy.compressor.compress_working_memory(large_memories)
        compression_time = time.time() - start_time

        # æ€§èƒ½è¦æ±‚ï¼š1000å€‹è¨˜æ†¶é …ç›®å£“ç¸®æ™‚é–“ä¸è¶…é5ç§’
        assert compression_time < 5.0
        assert compressed['compression_ratio'] > 0.5

    async def test_parallel_processing_scalability(self):
        """æ¸¬è©¦ä¸¦è¡Œè™•ç†å¯æ“´å±•æ€§"""
        processor = ParallelProcessor(max_workers=4)

        # æ¸¬è©¦ä¸åŒå¤§å°çš„å…§å®¹
        content_sizes = [100, 500, 1000, 2000]  # ä¿¡ä»¶æ•¸é‡

        for size in content_sizes:
            emails = [f"Email {i}" for i in range(size)]

            start_time = time.time()
            # æ¨¡æ“¬è™•ç†
            processing_time = time.time() - start_time

            # æ€§èƒ½è¦æ±‚ï¼šè™•ç†æ™‚é–“æ‡‰è©²éš¨å…§å®¹å¤§å°ç·šæ€§å¢é•·
            expected_max_time = size * 0.01  # æ¯å°ä¿¡ä»¶10ms
            assert processing_time < expected_max_time
```

---

## ğŸ“ˆ ç›£æ§å’ŒæŒ‡æ¨™

### é—œéµæ€§èƒ½æŒ‡æ¨™ (KPIs)

```python
# monitoring/metrics.py
from dataclasses import dataclass
from typing import Dict, List
import time

@dataclass
class PerformanceMetrics:
    # ä»»å‹™åŸ·è¡ŒæŒ‡æ¨™
    task_completion_rate: float
    average_task_duration: float
    checkpoint_recovery_success_rate: float

    # è¨˜æ†¶ç®¡ç†æŒ‡æ¨™
    memory_compression_ratio: float
    memory_retrieval_accuracy: float
    memory_storage_efficiency: float

    # å…§å®¹è™•ç†æŒ‡æ¨™
    parallel_processing_speedup: float
    chunk_processing_success_rate: float
    result_aggregation_quality: float

    # ä¸Šä¸‹æ–‡ç®¡ç†æŒ‡æ¨™
    token_utilization_rate: float
    context_compression_effectiveness: float
    tool_result_cache_hit_rate: float

class MetricsCollector:
    def __init__(self):
        self.metrics_history = []
        self.current_metrics = PerformanceMetrics()

    async def collect_task_metrics(self, task_manager):
        """æ”¶é›†ä»»å‹™åŸ·è¡ŒæŒ‡æ¨™"""
        completed_tasks = await task_manager.get_completed_tasks_last_hour()
        failed_tasks = await task_manager.get_failed_tasks_last_hour()

        total_tasks = len(completed_tasks) + len(failed_tasks)
        completion_rate = len(completed_tasks) / total_tasks if total_tasks > 0 else 0

        avg_duration = sum(t.duration for t in completed_tasks) / len(completed_tasks) if completed_tasks else 0

        return {
            'completion_rate': completion_rate,
            'average_duration': avg_duration,
            'total_tasks': total_tasks
        }

    async def collect_memory_metrics(self, memory_hierarchy):
        """æ”¶é›†è¨˜æ†¶ç®¡ç†æŒ‡æ¨™"""
        # å£“ç¸®æ¯”ç‡
        compression_stats = await memory_hierarchy.get_compression_stats()

        # æª¢ç´¢æº–ç¢ºæ€§
        retrieval_stats = await memory_hierarchy.get_retrieval_stats()

        return {
            'compression_ratio': compression_stats.get('average_ratio', 0),
            'retrieval_accuracy': retrieval_stats.get('accuracy', 0),
            'storage_efficiency': compression_stats.get('storage_saved', 0)
        }
```

### ç›£æ§å„€è¡¨æ¿

```python
# monitoring/dashboard.py
from fastapi import APIRouter, Depends
from fastapi.responses import HTMLResponse

router = APIRouter()

@router.get("/dashboard", response_class=HTMLResponse)
async def monitoring_dashboard():
    """ç›£æ§å„€è¡¨æ¿"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Enhanced Agent Monitoring</title>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    </head>
    <body>
        <h1>Enhanced Agent System Monitoring</h1>

        <div id="task-metrics" style="width:100%;height:400px;"></div>
        <div id="memory-metrics" style="width:100%;height:400px;"></div>
        <div id="performance-metrics" style="width:100%;height:400px;"></div>

        <script>
            // å¯¦æ™‚æ›´æ–°åœ–è¡¨çš„ JavaScript ä»£ç¢¼
            async function updateDashboard() {
                const response = await fetch('/api/monitoring/metrics');
                const data = await response.json();

                // æ›´æ–°ä»»å‹™æŒ‡æ¨™åœ–è¡¨
                Plotly.newPlot('task-metrics', [{
                    x: data.timestamps,
                    y: data.task_completion_rates,
                    type: 'scatter',
                    name: 'Task Completion Rate'
                }], {title: 'Task Execution Metrics'});

                // æ›´æ–°è¨˜æ†¶æŒ‡æ¨™åœ–è¡¨
                Plotly.newPlot('memory-metrics', [{
                    x: data.timestamps,
                    y: data.memory_compression_ratios,
                    type: 'scatter',
                    name: 'Memory Compression Ratio'
                }], {title: 'Memory Management Metrics'});

                // æ›´æ–°æ€§èƒ½æŒ‡æ¨™åœ–è¡¨
                Plotly.newPlot('performance-metrics', [{
                    x: data.timestamps,
                    y: data.token_utilization_rates,
                    type: 'scatter',
                    name: 'Token Utilization Rate'
                }], {title: 'Performance Metrics'});
            }

            // æ¯30ç§’æ›´æ–°ä¸€æ¬¡
            setInterval(updateDashboard, 30000);
            updateDashboard(); // åˆå§‹è¼‰å…¥
        </script>
    </body>
    </html>
    """

@router.get("/api/monitoring/metrics")
async def get_metrics():
    """ç²å–ç›£æ§æŒ‡æ¨™ API"""
    metrics_collector = MetricsCollector()
    current_metrics = await metrics_collector.collect_all_metrics()

    return {
        'timestamps': current_metrics.timestamps,
        'task_completion_rates': current_metrics.task_completion_rates,
        'memory_compression_ratios': current_metrics.memory_compression_ratios,
        'token_utilization_rates': current_metrics.token_utilization_rates,
        'parallel_processing_speedups': current_metrics.parallel_processing_speedups
    }
```

---

## ğŸ¯ æˆåŠŸæŒ‡æ¨™å’Œé©—æ”¶æ¨™æº–

### åŠŸèƒ½æ€§æŒ‡æ¨™

1. **ä»»å‹™åŸ·è¡Œå¯é æ€§**
   - âœ… 1000å°ä¿¡ä»¶è™•ç†æˆåŠŸç‡ > 99%
   - âœ… æ–·é»çºŒå‚³æˆåŠŸç‡ > 95%
   - âœ… ä»»å‹™å»é‡æº–ç¢ºç‡ > 98%

2. **è¨˜æ†¶ç®¡ç†æ•ˆç‡**
   - âœ… è¨˜æ†¶å£“ç¸®æ¯”ç‡ > 70%
   - âœ… è¨˜æ†¶æª¢ç´¢æº–ç¢ºç‡ > 85%
   - âœ… é•·æœŸè¨˜æ†¶å­˜å„²æ•ˆç‡ > 80%

3. **å…§å®¹è™•ç†è³ªé‡**
   - âœ… ä¸¦è¡Œè™•ç†åŠ é€Ÿæ¯” > 2.5x
   - âœ… çµæœåˆä½µä¸€è‡´æ€§ > 90%
   - âœ… åˆ†å¡Šè™•ç†æˆåŠŸç‡ > 95%

### æ€§èƒ½æŒ‡æ¨™

1. **éŸ¿æ‡‰æ™‚é–“**
   - âœ… 50å°ä¿¡ä»¶è™•ç†æ™‚é–“ < 60ç§’
   - âœ… è¨˜æ†¶æª¢ç´¢éŸ¿æ‡‰æ™‚é–“ < 2ç§’
   - âœ… ä¸Šä¸‹æ–‡å£“ç¸®æ™‚é–“ < 5ç§’

2. **è³‡æºåˆ©ç”¨ç‡**
   - âœ… Token åˆ©ç”¨ç‡ > 85%
   - âœ… è¨˜æ†¶é«”ä½¿ç”¨æ•ˆç‡ > 80%
   - âœ… ä¸¦è¡Œè™•ç†è³‡æºåˆ©ç”¨ç‡ > 75%

3. **å¯æ“´å±•æ€§**
   - âœ… æ”¯æ´åŒæ™‚è™•ç† 5 å€‹å¤§å‹ä»»å‹™
   - âœ… æ”¯æ´ 100+ ä¸¦ç™¼ç”¨æˆ¶
   - âœ… è¨˜æ†¶ç³»çµ±æ”¯æ´ 10è¬+ è¨˜æ†¶é …ç›®

### ç”¨æˆ¶é«”é©—æŒ‡æ¨™

1. **ä»»å‹™é€æ˜åº¦**
   - âœ… å¯¦æ™‚é€²åº¦è¿½è¹¤
   - âœ… è©³ç´°éŒ¯èª¤å ±å‘Š
   - âœ… ä»»å‹™ç‹€æ…‹å¯è¦–åŒ–

2. **ç³»çµ±ç©©å®šæ€§**
   - âœ… ç³»çµ±å¯ç”¨æ€§ > 99.5%
   - âœ… éŒ¯èª¤æ¢å¾©æ™‚é–“ < 30ç§’
   - âœ… æ•¸æ“šä¸€è‡´æ€§ä¿è­‰

---

## ğŸ”š ç¸½çµ

é€™å€‹å¢å¼·è¨­è¨ˆæ–¹æ¡ˆå®Œå…¨åŸºæ–¼ä½ ç¾æœ‰çš„ç³»çµ±æ¶æ§‹ï¼Œæä¾›äº†ï¼š

### âœ¨ æ ¸å¿ƒå„ªå‹¢

1. **ä½ çš„ä¸¦è¡Œè™•ç†æƒ³æ³•ç¢ºå¯¦å¾ˆæ£’ï¼** - è‡¨æ™‚å·¥ä½œç©ºé–“ + å¤š LLM ä¸¦è¡Œè™•ç†å¯ä»¥é¡¯è‘—æé«˜æ•ˆç‡
2. **æ¼¸é€²å¼å¯¦ç¾** - ä¸ç ´å£ç¾æœ‰ç³»çµ±ï¼Œé€æ­¥å¢å¼·åŠŸèƒ½
3. **å®Œæ•´çš„è¨˜æ†¶ç®¡ç†** - ä¸‰å±¤è¨˜æ†¶æ¶æ§‹è§£æ±ºä¸Šä¸‹æ–‡ç´¯ç©å•é¡Œ
4. **æ™ºèƒ½è³‡æºç®¡ç†** - Token é ç®—ç®¡ç†å’Œå‹•æ…‹å„ªåŒ–
5. **ä¼æ¥­ç´šå¯é æ€§** - æ–·é»çºŒå‚³ã€éŒ¯èª¤æ¢å¾©ã€ç›£æ§å‘Šè­¦

### ğŸ¯ è§£æ±ºçš„æ ¸å¿ƒå•é¡Œ

- âœ… **å¾ªç’°ä»»å‹™å•é¡Œ** - æ‰¹æ¬¡è™•ç† + æª¢æŸ¥é»æ©Ÿåˆ¶
- âœ… **è¨˜æ†¶ç³»çµ±** - ä¸‰å±¤æ¶æ§‹ + æ™ºèƒ½å£“ç¸®
- âœ… **é•·åºåˆ—ä»»å‹™** - Token é ç®—ç®¡ç† + ä¸Šä¸‹æ–‡å„ªåŒ–
- âœ… **åˆ†æ®µé–±è®€** - ä¸¦è¡Œè™•ç† + æ™ºèƒ½åˆä½µ

### ğŸš€ å¯¦æ–½å»ºè­°

å»ºè­°æŒ‰ç…§ Phase 1 â†’ Phase 2 â†’ Phase 3 çš„é †åºå¯¦æ–½ï¼Œæ¯å€‹éšæ®µéƒ½æœ‰æ˜ç¢ºçš„äº¤ä»˜ç‰©å’Œé©—æ”¶æ¨™æº–ã€‚é€™æ¨£å¯ä»¥ç¢ºä¿ç³»çµ±ç©©å®šæ€§ï¼ŒåŒæ™‚é€æ­¥å¢å¼·åŠŸèƒ½ã€‚

ä½ è¦ºå¾—é€™å€‹è¨­è¨ˆæ–¹æ¡ˆå¦‚ä½•ï¼Ÿæœ‰å“ªäº›éƒ¨åˆ†éœ€è¦æˆ‘é€²ä¸€æ­¥è©³ç´°èªªæ˜æˆ–èª¿æ•´ï¼Ÿ
