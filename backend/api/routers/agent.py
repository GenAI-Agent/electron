"""
Agent API è·¯ç”±

è™•ç†èˆ‡ Agent ç›¸é—œçš„è«‹æ±‚ï¼Œåªæä¾›æµå¼æ¥å£ã€‚
"""

import json
import numpy as np
from typing import AsyncGenerator, Dict, Any, Optional, List
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from supervisor_agent.core.supervisor_agent import SupervisorAgent

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å·¥å…·
import os
import sys
from pathlib import Path

current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

# å°å…¥ LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·
from supervisor_agent.tools.langchain_local_file_tools import (
    get_langchain_local_file_tools,
)
from supervisor_agent.utils.logger import get_logger

logger = get_logger(__name__)


def convert_numpy_types(obj):
    """
    éæ­¸è½‰æ›numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹ï¼Œè§£æ±ºJSONåºåˆ—åŒ–å•é¡Œ
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj


def compress_tool_result(tool_result: dict, max_data_items: int = 5) -> dict:
    """
    å£“ç¸®å·¥å…·çµæœï¼Œé¿å…å°è©±æ­·å²éé•·

    Args:
        tool_result: å·¥å…·åŸ·è¡Œçµæœ
        max_data_items: æœ€å¤§ä¿ç•™çš„æ•¸æ“šé …ç›®æ•¸é‡

    Returns:
        å£“ç¸®å¾Œçš„çµæœ
    """
    if not isinstance(tool_result, dict):
        return tool_result

    compressed = tool_result.copy()

    # å£“ç¸®å¤§æ•¸æ“šé‡å­—æ®µ
    for key in ["data", "filtered_data", "sample_data", "results"]:
        if key in compressed and isinstance(compressed[key], list):
            original_length = len(compressed[key])
            if original_length > max_data_items:
                compressed[key] = compressed[key][:max_data_items]
                compressed[f"{key}_truncated"] = True
                compressed[f"{key}_original_count"] = original_length
                compressed[f"{key}_truncated_message"] = (
                    f"æ•¸æ“šå·²æˆªæ–·ï¼ŒåŸæœ‰ {original_length} é …ï¼Œåªé¡¯ç¤ºå‰ {max_data_items} é …"
                )

    # ç§»é™¤æˆ–å£“ç¸®å…¶ä»–å¤§å­—æ®µ
    large_fields_to_remove = ["raw_data", "full_results", "detailed_analysis"]
    for field in large_fields_to_remove:
        if field in compressed:
            compressed[f"{field}_removed"] = f"å¤§å­—æ®µ {field} å·²ç§»é™¤ä»¥ç¯€çœç©ºé–“"
            del compressed[field]

    return compressed


def _determine_request_type(context_data: dict) -> str:
    """
    åˆ¤æ–·è«‹æ±‚é¡å‹

    Args:
        context_data: ä¸Šä¸‹æ–‡æ•¸æ“š

    Returns:
        è«‹æ±‚é¡å‹: 'multi_file', 'file', 'local_file', 'web', 'gmail'
    """
    if not context_data:
        return "local_file"

    type = context_data.get("type")

    # è©³ç´°è¨˜éŒ„åˆ¤æ–·éç¨‹
    print(f"ğŸ” _determine_request_type åˆ¤æ–·éç¨‹:")
    # print(f"  - context_data: {context_data}")
    # print(f"  - context_data keys: {list(context_data.keys()) if context_data else 'None'}")
    print(f"  - type: {type}")

    logger.info(f"ğŸ” åˆ¤æ–·è«‹æ±‚é¡å‹:")
    logger.info(f"  - context_data keys: {list(context_data.keys())}")
    logger.info(f"  - type: {type}")

    # åˆ¤æ–·æ˜¯å¦ç‚ºå¤šæª”æ¡ˆé¡å‹
    if type == "multi_file":
        files = context_data.get("files", [])
        print(f"  - æª¢æ¸¬åˆ° multi_file é¡å‹ï¼Œæª”æ¡ˆæ•¸é‡: {len(files)}")
        logger.info(f"  - æª¢æ¸¬åˆ° multi_file é¡å‹ï¼Œæª”æ¡ˆæ•¸é‡: {len(files)}")
        return "multi_file"
    elif type == "file":
        file_path = context_data.get("file_path")
        print(f"  - æª¢æ¸¬åˆ° file é¡å‹ï¼Œæª”æ¡ˆè·¯å¾‘: {file_path}")
        logger.info(f"  - æª¢æ¸¬åˆ° file é¡å‹ï¼Œæª”æ¡ˆè·¯å¾‘: {file_path}")
        return "file"
    elif type == "gmail" or context_data.get("email_address"):
        print(f"  - æª¢æ¸¬åˆ° gmail é¡å‹")
        logger.info(f"  - æª¢æ¸¬åˆ° gmail é¡å‹")
        return "gmail"
    elif type == "web" or type == "page":
        print(f"  - æª¢æ¸¬åˆ° web é¡å‹")
        logger.info(f"  - æª¢æ¸¬åˆ° web é¡å‹")
        return "web"
    else:
        print(f"  - é è¨­ç‚º local_file é¡å‹")
        logger.info(f"  - é è¨­ç‚º local_file é¡å‹")
        return "local_file"


# ç§»é™¤ session summary ç›¸é—œå‡½æ•¸

router = APIRouter()


# Session-based Agent ç®¡ç†
class AgentManager:
    """Agentç®¡ç†å™¨ï¼Œç‚ºæ¯å€‹sessionç¶­è­·ç¨ç«‹çš„agentå¯¦ä¾‹"""

    def __init__(self):
        self.agents: Dict[str, SupervisorAgent] = {}
        # å¾ backend/api/routers/agent.py åˆ° data/rules çš„æ­£ç¢ºè·¯å¾‘
        self.rules_dir = Path(__file__).parent.parent.parent.parent / "data" / "rules"
        logger.info(f"ğŸ“ AgentManager rules_dir: {self.rules_dir}")
        logger.info(f"ğŸ“ rules_dir æ˜¯å¦å­˜åœ¨: {self.rules_dir.exists()}")
        if self.rules_dir.exists():
            rule_files = list(self.rules_dir.glob("*.json"))
            logger.info(f"ğŸ“ æ‰¾åˆ°çš„ rule æ–‡ä»¶: {[f.name for f in rule_files]}")

    def get_agent(self, session_id: str, stream_callback=None) -> SupervisorAgent:
        """ç²å–æŒ‡å®šsessionçš„Agentå¯¦ä¾‹"""
        if session_id not in self.agents:
            logger.info(f"ğŸ†• ç‚ºsession {session_id} å‰µå»ºæ–°çš„Agentå¯¦ä¾‹")
            self.agents[session_id] = SupervisorAgent(
                str(self.rules_dir), stream_callback
            )
        else:
            # æ›´æ–°ç¾æœ‰agentçš„stream_callback
            self.agents[session_id].stream_callback = stream_callback
        return self.agents[session_id]

    def cleanup_agent(self, session_id: str):
        """æ¸…ç†æŒ‡å®šsessionçš„Agentå¯¦ä¾‹"""
        if session_id in self.agents:
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†session {session_id} çš„Agentå¯¦ä¾‹")
            del self.agents[session_id]

    def get_active_sessions(self) -> List[str]:
        """ç²å–æ´»èºçš„sessionåˆ—è¡¨"""
        return list(self.agents.keys())


# å…¨åŸŸAgentç®¡ç†å™¨å¯¦ä¾‹
_agent_manager = AgentManager()


def get_agent(session_id: str = "default", stream_callback=None) -> SupervisorAgent:
    """ç²å–æŒ‡å®šsessionçš„Agentå¯¦ä¾‹"""
    return _agent_manager.get_agent(session_id, stream_callback)


def set_agent(agent: SupervisorAgent, session_id: str = "default"):
    """è¨­ç½®æŒ‡å®šsessionçš„Agentå¯¦ä¾‹"""
    _agent_manager.agents[session_id] = agent
    logger.info(f"âœ… Session {session_id} çš„Agentå¯¦ä¾‹å·²è¨­ç½®")


class StreamRequest(BaseModel):
    """æµå¼è«‹æ±‚æ¨¡å‹"""

    message: str = Field(..., description="ç”¨æˆ¶æ¶ˆæ¯")
    user_id: str = Field(default="default_user", description="ç”¨æˆ¶ID")
    session_id: str = Field(default="default_session", description="æœƒè©±ID")
    context_data: Optional[Dict[str, Any]] = Field(
        default=None, description="ä¸Šä¸‹æ–‡è³‡æ–™ï¼ˆé é¢æˆ–æ–‡ä»¶ï¼‰"
    )


async def generate_stream_response(
    message: str,
    agent: SupervisorAgent,
    session_id: str = "default_session",
    context_data: dict = None,
    request_type: str = "default",
) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼éŸ¿æ‡‰"""

    # ç”¨æ–¼å­˜å„²streamäº‹ä»¶çš„åˆ—è¡¨
    stream_events = []

    # å®šç¾©streamå›èª¿å‡½æ•¸
    async def stream_callback(event_data):
        """æ”¶é›†å·¥å…·åŸ·è¡Œçµæœ"""
        stream_events.append(event_data)

    try:
        logger.info(f"ğŸš€ é–‹å§‹ç”Ÿæˆæµå¼éŸ¿æ‡‰")
        logger.info(f"  - message: {message}")
        logger.info(f"  - session_id: {session_id}")
        logger.info(f"  - request_type: {request_type}")

        # è©³ç´°è¨˜éŒ„ context_data
        if context_data:
            logger.info(f"ğŸ“‹ Context Data è©³ç´°è³‡è¨Š:")
            logger.info(f"  - keys: {list(context_data.keys())}")
            logger.info(f"  - type: {context_data.get('type')}")

            if context_data.get("type") == "multi_file":
                files = context_data.get("files", [])
                logger.info(f"  - å¤šæª”æ¡ˆæ•¸é‡: {len(files)}")
                for i, file_info in enumerate(files):
                    logger.info(
                        f"    æª”æ¡ˆ {i+1}: {file_info.get('filename')} ({len(file_info.get('data', []))} è¡Œ)"
                    )
            elif context_data.get("type") == "file":
                logger.info(f"  - å–®æª”æ¡ˆè·¯å¾‘: {context_data.get('file_path')}")
        else:
            logger.info(f"ğŸ“‹ Context Data: None")

        # ğŸ¯ æ ¹æ“šè«‹æ±‚é¡å‹é¸æ“‡å·¥å…·é›†å’Œè™•ç†æ–¹å¼
        available_tools = []
        file_summary = None
        final_context = None

        if request_type == "local_file":
            logger.info("ğŸ“ LOCAL FILE æ¨¡å¼ - ç›´æ¥è™•ç†æ–‡ä»¶")
            available_tools = get_langchain_local_file_tools()

            # ğŸ”„ **è™•ç†å–®æª”æ¡ˆæˆ–å¤šæª”æ¡ˆ**
            if context_data and (
                context_data.get("file_path") or context_data.get("file_paths")
            ):
                # æ”¯æŒå–®æª”æ¡ˆè·¯å¾‘
                if context_data.get("file_path"):
                    file_path = context_data.get("file_path")
                    file_paths = [file_path]
                    logger.info(f"ğŸ“„ è™•ç†å–®å€‹æ–‡ä»¶: {file_path}")
                    print(f"ğŸ“„ è™•ç†å–®å€‹æ–‡ä»¶: {file_path}")
                # æ”¯æŒå¤šæª”æ¡ˆè·¯å¾‘
                elif context_data.get("file_paths"):
                    file_paths = context_data.get("file_paths")
                    file_path = file_paths[0]  # å‘å¾Œå…¼å®¹ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æª”æ¡ˆä½œç‚ºä¸»æª”æ¡ˆ
                    logger.info(f"ğŸ“ è™•ç†å¤šå€‹æ–‡ä»¶: {len(file_paths)} å€‹æª”æ¡ˆ")
                    logger.info(f"ğŸ“ æª”æ¡ˆåˆ—è¡¨: {file_paths}")
                    print(f"ğŸ“ è™•ç†å¤šå€‹æ–‡ä»¶: {len(file_paths)} å€‹æª”æ¡ˆ")
                    print(f"ğŸ“ æª”æ¡ˆåˆ—è¡¨: {file_paths}")

                # ğŸ”§ **å°å…¥å¿…è¦çš„æ¨¡çµ„**
                from pathlib import Path
                import os

                # ğŸ¯ æ ¹æ“šæª”æ¡ˆæ•¸é‡æ±ºå®šå‚³éçµ¦ Agent çš„ context
                if len(file_paths) > 1:
                    # å¤šæª”æ¡ˆï¼šå…ˆè®€å–æ‰€æœ‰æª”æ¡ˆï¼Œç„¶å¾Œå‚³éé è™•ç†çš„æ•¸æ“šçµ¦ Agent
                    logger.info(f"âœ… å¤šæª”æ¡ˆæ¨¡å¼ï¼šå…ˆè®€å– {len(file_paths)} å€‹æª”æ¡ˆ")
                    print(f"âœ… å¤šæª”æ¡ˆæ¨¡å¼ï¼šå…ˆè®€å– {len(file_paths)} å€‹æª”æ¡ˆ")
                    print(f"ğŸ“ æª”æ¡ˆåˆ—è¡¨: {file_paths}")

                    try:
                        # ğŸ”§ ç›´æ¥èª¿ç”¨å¤šæª”æ¡ˆè®€å–å‡½æ•¸ï¼ˆç¹é LangChain å·¥å…·åŒ…è£ï¼‰
                        # å°å…¥åŸå§‹å‡½æ•¸è€Œä¸æ˜¯å·¥å…·å°è±¡
                        import sys
                        import os

                        sys.path.append(
                            os.path.join(os.path.dirname(__file__), "..", "..")
                        )

                        # ğŸ”§ ä½¿ç”¨èˆ‡ local file ç›¸åŒçš„æ•¸æ“šåˆ†æå·¥å…·é€²è¡Œæ‘˜è¦åŒ–è™•ç†
                        from src.tools.data_analysis_tools import data_analysis_tools

                        print(f"ğŸ“¤ å¤šæª”æ¡ˆæ‘˜è¦åŒ–è™•ç†ï¼Œæª”æ¡ˆæ•¸é‡: {len(file_paths)}")

                        # ä¸¦è¡Œè™•ç†å¤šå€‹æª”æ¡ˆçš„æ‘˜è¦
                        import asyncio

                        async def process_single_file(file_path):
                            try:
                                print(f"ğŸ“„ è™•ç†æª”æ¡ˆæ‘˜è¦: {file_path}")

                                # ğŸ”§ è­˜åˆ¥å¹³å°é¡å‹
                                filename = file_path.split("/")[-1].lower()
                                if "thread" in filename:
                                    platform_type = "ç¤¾ç¾¤è¨è«–ä¸²"
                                    platform_name = "Threads"
                                elif "ptt" in filename:
                                    platform_type = "PTTè«–å£‡"
                                    platform_name = "PTT"
                                elif "facebook" in filename or "fb" in filename:
                                    platform_type = "ç¤¾ç¾¤åª’é«”"
                                    platform_name = "Facebook"
                                elif "twitter" in filename:
                                    platform_type = "ç¤¾ç¾¤åª’é«”"
                                    platform_name = "Twitter"
                                elif "petition" in filename:
                                    platform_type = "é™³æƒ…ç³»çµ±"
                                    platform_name = "Petition"
                                elif "marketing_dashboard" in filename:
                                    platform_type = "ç‡Ÿé‹å„€è¡¨æ¿"
                                    platform_name = "Marketing Dashboard"
                                elif "marketing_strategy" in filename:
                                    platform_type = "ç­–ç•¥æ¨æ¼”"
                                    platform_name = "Strategy Simulation"
                                elif "marketing_action" in filename:
                                    platform_type = "è¡Œå‹•å»ºè­°"
                                    platform_name = "Action Recommendation"
                                elif "marketing_intelligence" in filename:
                                    platform_type = "æ™ºåº«"
                                    platform_name = "Intelligence Hub"
                                elif "marketing_competitor" in filename:
                                    platform_type = "ç«¶çˆ­è€…åˆ†æ"
                                    platform_name = "Competitor Analysis"
                                elif "marketing_complaints" in filename:
                                    platform_type = "å®¢è¨´åˆ†æ"
                                    platform_name = "Complaints Analysis"
                                elif "marketing_china_airlines" in filename:
                                    platform_type = "è¯èˆªä¼æ¥­è³‡æ–™"
                                    platform_name = "China Airlines Data"
                                else:
                                    platform_type = "æœªçŸ¥å¹³å°"
                                    platform_name = "Unknown"

                                print(f"ğŸ·ï¸ è­˜åˆ¥å¹³å°: {platform_name} ({platform_type})")

                                # ä½¿ç”¨ data_analysis_tools ç²å–æª”æ¡ˆæ‘˜è¦ï¼ˆèˆ‡ local file ç›¸åŒçš„æ–¹å¼ï¼‰
                                data_info_result = (
                                    await data_analysis_tools.get_data_info(
                                        file_path, session_id
                                    )
                                )

                                if isinstance(
                                    data_info_result, dict
                                ) and data_info_result.get("success", True):
                                    # æ·»åŠ å¹³å°è­˜åˆ¥ä¿¡æ¯
                                    enhanced_result = {
                                        "filename": file_path,
                                        "platform_name": platform_name,
                                        "platform_type": platform_type,
                                        "success": True,
                                        "data_summary": data_info_result,  # æ‘˜è¦åŒ–çš„æ•¸æ“šä¿¡æ¯
                                        "sample_data": data_info_result.get(
                                            "sample_data", []
                                        )[
                                            :1
                                        ],  # åªä¿ç•™1è¡Œæ¨£æœ¬ç”¨æ–¼æ ¼å¼åƒè€ƒ
                                        "total_rows": data_info_result.get(
                                            "total_rows", 0
                                        ),
                                        "columns": data_info_result.get("columns", []),
                                    }
                                    print(
                                        f"âœ… æˆåŠŸè™•ç†æ‘˜è¦: {file_path} ({platform_name}, {data_info_result.get('total_rows', 0)} è¡Œ)"
                                    )
                                    return enhanced_result
                                else:
                                    print(f"âŒ æ‘˜è¦è™•ç†å¤±æ•—: {file_path}")
                                    return {
                                        "filename": file_path,
                                        "platform_name": platform_name,
                                        "platform_type": platform_type,
                                        "success": False,
                                        "error": "æ•¸æ“šæ‘˜è¦è™•ç†å¤±æ•—",
                                    }

                            except Exception as e:
                                print(f"âŒ è™•ç†ç•°å¸¸: {file_path} - {e}")
                                return {
                                    "filename": file_path,
                                    "success": False,
                                    "error": str(e),
                                }

                        # ä¸¦è¡Œè™•ç†æ‰€æœ‰æª”æ¡ˆ
                        results = await asyncio.gather(
                            *[process_single_file(fp) for fp in file_paths]
                        )
                        successful_files = [r for r in results if r.get("success")]

                        print(
                            f"ğŸ“Š æ‘˜è¦è™•ç†å®Œæˆ: {len(successful_files)}/{len(file_paths)} æˆåŠŸ"
                        )

                        # æ§‹å»ºå¤šæª”æ¡ˆæ‘˜è¦çµ±è¨ˆ
                        platforms = list(
                            set(
                                r.get("platform_name", "Unknown")
                                for r in successful_files
                            )
                        )
                        platform_types = list(
                            set(
                                r.get("platform_type", "æœªçŸ¥å¹³å°")
                                for r in successful_files
                            )
                        )

                        summary = {
                            "total_files": len(file_paths),
                            "successful_reads": len(successful_files),
                            "failed_reads": len(file_paths) - len(successful_files),
                            "total_rows": sum(
                                r.get("total_rows", 0) for r in successful_files
                            ),
                            "platforms": platforms,
                            "platform_types": platform_types,
                            "analysis_context": f"æ¯”è¼ƒåˆ†æ {' vs '.join(platform_types)} çš„æ•¸æ“šå·®ç•°",
                        }

                        print(f"ğŸ“‹ è­˜åˆ¥çš„å¹³å°: {platforms}")
                        print(f"ğŸ“‹ å¹³å°é¡å‹: {platform_types}")
                        print(f"ğŸ“‹ åˆ†æä¸Šä¸‹æ–‡: {summary['analysis_context']}")

                        reader_data = {
                            "success": True,
                            "results": results,
                            "summary": summary,
                            "session_id": session_id,
                        }

                        # ğŸ”§ å®‰å…¨çš„ JSON åºåˆ—åŒ–æª¢æŸ¥
                        try:
                            reader_result = json.dumps(reader_data, ensure_ascii=False)
                            print(f"âœ… JSON åºåˆ—åŒ–æˆåŠŸ")
                        except TypeError as json_error:
                            print(f"âŒ JSON åºåˆ—åŒ–å¤±æ•—: {json_error}")
                            # æª¢æŸ¥å“ªå€‹çµæœæœ‰å•é¡Œ
                            for i, result in enumerate(results):
                                try:
                                    json.dumps(result)
                                except TypeError:
                                    print(f"âŒ çµæœ {i} ç„¡æ³•åºåˆ—åŒ–: {result.keys()}")
                                    # ç§»é™¤æœ‰å•é¡Œçš„å­—æ®µ
                                    for key in list(result.keys()):
                                        try:
                                            json.dumps(result[key])
                                        except TypeError:
                                            print(f"âŒ ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„å­—æ®µ: {key}")
                                            del result[key]

                            # é‡æ–°å˜—è©¦åºåˆ—åŒ–
                            reader_result = json.dumps(reader_data, ensure_ascii=False)
                            print(f"âœ… æ¸…ç†å¾Œ JSON åºåˆ—åŒ–æˆåŠŸ")

                        print(
                            f"ğŸ“¥ multi_file_reader_tool çµæœ: {reader_result[:500]}..."
                        )

                        # è§£æçµæœ
                        reader_data = json.loads(reader_result)

                        if reader_data.get("success"):
                            # æˆåŠŸè®€å–æª”æ¡ˆï¼Œæ§‹å»ºåŒ…å«æ‘˜è¦æ•¸æ“šå’Œå¹³å°ä¿¡æ¯çš„ context
                            final_context = {
                                "mode": "multi_file_analysis",
                                "total_files": len(file_paths),
                                "file_paths": file_paths,
                                "files_summary": reader_data,  # æ‘˜è¦åŒ–çš„æª”æ¡ˆæ•¸æ“š
                                "platforms": summary["platforms"],
                                "platform_types": summary["platform_types"],
                                "analysis_context": summary["analysis_context"],
                                "message": f"å·²æˆåŠŸåˆ†æ {len(successful_files)} å€‹æª”æ¡ˆçš„æ‘˜è¦ï¼š{', '.join(summary['platforms'])}",
                            }
                            print(
                                f"âœ… å¤šæª”æ¡ˆæ‘˜è¦åˆ†æå®Œæˆï¼š{summary['analysis_context']}"
                            )
                            logger.info(
                                f"âœ… å¤šæª”æ¡ˆæ‘˜è¦åˆ†æå®Œæˆï¼š{summary['analysis_context']}"
                            )
                        else:
                            # è®€å–å¤±æ•—
                            error_msg = reader_data.get("error", "å¤šæª”æ¡ˆè®€å–å¤±æ•—")
                            print(f"âŒ å¤šæª”æ¡ˆè®€å–å¤±æ•—: {error_msg}")
                            final_context = {"error": f"å¤šæª”æ¡ˆè®€å–å¤±æ•—: {error_msg}"}

                    except Exception as e:
                        print(f"âŒ å¤šæª”æ¡ˆé è™•ç†å¤±æ•—: {e}")
                        logger.error(f"âŒ å¤šæª”æ¡ˆé è™•ç†å¤±æ•—: {e}")
                        final_context = {"error": f"å¤šæª”æ¡ˆé è™•ç†å¤±æ•—: {str(e)}"}
                else:
                    # å–®æª”æ¡ˆï¼šä½¿ç”¨åŸæœ‰é‚è¼¯
                    logger.info(f"âœ… å–®æª”æ¡ˆæ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰é‚è¼¯è™•ç†")
                    print(f"âœ… å–®æª”æ¡ˆæ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰é‚è¼¯è™•ç†")

                    # ğŸ”§ **ä¿®å¾©è·¯å¾‘å•é¡Œï¼šå°‡ç›¸å°è·¯å¾‘è½‰æ›ç‚ºçµ•å°è·¯å¾‘**
                    # å¦‚æœæ˜¯ç›¸å°è·¯å¾‘ï¼ˆå¦‚ sandbox/xxx.csvï¼‰ï¼Œè½‰æ›ç‚ºçµ•å°è·¯å¾‘
                    if file_path.startswith("sandbox/"):
                        # ç²å–é …ç›®æ ¹ç›®éŒ„ï¼šbackend/api/routers/agent.py -> backend/api/routers -> backend/api -> backend -> project_root
                        current_dir = Path(
                            __file__
                        ).parent.parent.parent.parent  # æ­£ç¢ºçš„è·¯å¾‘å±¤ç´š
                        absolute_file_path = current_dir / "data" / file_path
                        logger.info(f"ğŸ”§ è·¯å¾‘è½‰æ›: {file_path} -> {absolute_file_path}")
                        print(f"ğŸ”§ è·¯å¾‘è½‰æ›: {file_path} -> {absolute_file_path}")
                        file_path = str(absolute_file_path)

                    # ç¹¼çºŒå–®æª”æ¡ˆè™•ç†é‚è¼¯...

                    # æª¢æŸ¥æ˜¯å¦ç‚ºåˆä½µè³‡æ–™é›†çš„è™›æ“¬è·¯å¾‘
                    if file_path.endswith("combined_datasets"):
                        logger.info(f"ğŸ”„ è™•ç†åˆä½µè³‡æ–™é›†: {file_path}")
                        # å°æ–¼åˆä½µè³‡æ–™é›†ï¼Œç›´æ¥ä½¿ç”¨context_dataä¸­çš„è³‡æ–™
                        if context_data and "file_summary" in context_data:
                            final_context = {
                                "file_path": file_path,
                                "data_info": context_data["file_summary"],
                                "is_combined_dataset": True,
                            }
                            logger.info(
                                f"âœ… æˆåŠŸè™•ç†åˆä½µè³‡æ–™é›†ï¼ŒåŒ…å« {len(context_data['file_summary'].get('segments', []))} å€‹è³‡æ–™é›†"
                            )
                        else:
                            logger.error(f"âŒ åˆä½µè³‡æ–™é›†ç¼ºå°‘å¿…è¦çš„ä¸Šä¸‹æ–‡è³‡æ–™")
                            final_context = {
                                "error": f"åˆä½µè³‡æ–™é›†ç¼ºå°‘ä¸Šä¸‹æ–‡è³‡æ–™: {file_path}"
                            }
                    # æª¢æŸ¥å¯¦é«”æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    elif not Path(file_path).exists():
                        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        final_context = {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
                    else:
                        # ç›´æ¥èª¿ç”¨åº•å±¤çš„æ•¸æ“šåˆ†æå‡½æ•¸ç²å–æ•¸æ“šä¿¡æ¯
                        from src.tools.data_analysis_tools import data_analysis_tools

                        try:
                            data_info_result = await data_analysis_tools.get_data_info(
                                file_path, session_id
                            )
                            logger.info(
                                f"ğŸ“Š get_data_info_tool åŸ·è¡Œçµæœ: {str(data_info_result)[:500]}..."
                            )

                            # æ§‹å»º final_contextï¼ŒåªåŒ…å« data_info
                            final_context = {
                                "file_path": file_path,
                                "data_info": data_info_result,
                            }

                            # è©³ç´°è¨˜éŒ„å‚³çµ¦ agent çš„å…§å®¹
                            logger.info("ğŸ“‹ å‚³çµ¦ Agent çš„ final_context å…§å®¹:")
                            logger.info(f"  - file_path: {final_context['file_path']}")
                            logger.info(
                                f"  - data_info é¡å‹: {type(final_context['data_info'])}"
                            )

                            if isinstance(data_info_result, dict):
                                # è¨˜éŒ„ data_info çš„é—œéµä¿¡æ¯
                                sample_data = data_info_result.get("sample_data", [])
                                total_rows = data_info_result.get("total_rows", 0)
                                columns = data_info_result.get("columns", [])

                                logger.info(f"  - sample_data æ•¸é‡: {len(sample_data)}")
                                logger.info(f"  - total_rows: {total_rows}")
                                logger.info(f"  - columns: {columns}")
                                logger.info(f"  - sample_data å…§å®¹: {sample_data}")

                                # ç¢ºä¿æœ‰ sample_data
                                if sample_data:
                                    logger.info("âœ… æˆåŠŸç²å– sample_dataï¼Œå°‡å‚³çµ¦ Agent")
                                else:
                                    logger.warning("âš ï¸ sample_data ç‚ºç©º")

                        except Exception as e:
                            logger.error(f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {e}")
                            final_context = {"error": f"æ–‡ä»¶è™•ç†å¤±æ•—: {str(e)}"}
            else:
                logger.error("âŒ æ²’æœ‰æä¾› file_path")
                logger.error(f"âŒ å®Œæ•´çš„ context_data: {context_data}")

                # ğŸš« ç›´æ¥è¿”å›éŒ¯èª¤ï¼Œä¸è®“ Agent ç¹¼çºŒåŸ·è¡Œ
                error_message = (
                    "è«‹å…ˆåœ¨ AI Sandbox é é¢é¸æ“‡è¦åˆ†æçš„è³‡æ–™é›†ï¼Œç„¶å¾Œå†æå‡ºåˆ†æå•é¡Œã€‚"
                )

                # ç›´æ¥è¿”å›éŒ¯èª¤éŸ¿æ‡‰ï¼Œä¸èª¿ç”¨ Agent
                yield f"data: {json.dumps({'type': 'error', 'message': error_message}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'content', 'content': error_message}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                return

        elif request_type == "gmail":
            logger.info("ğŸ“§ GMAIL æ¨¡å¼ - æ‰¹é‡æŠ“å–éƒµä»¶ä¸¦è½‰æ›ç‚º local file è™•ç†")

            # æå– Gmail ç›¸é—œä¿¡æ¯
            email_address = context_data.get("email_address", "")
            oauth_tokens = context_data.get("oauth_tokens", {})
            access_token = oauth_tokens.get("access_token", "")

            if not access_token:
                logger.error("âŒ ç¼ºå°‘ OAuth access token")
                raise ValueError("Gmail æ¨¡å¼éœ€è¦ OAuth access token")

            logger.info(f"ğŸ“§ é–‹å§‹æŠ“å– Gmail éƒµä»¶: {email_address}")

            # èª¿ç”¨ Gmail æ‰¹é‡æŠ“å–å·¥å…·
            from src.tools.gmail_tools import fetch_gmail_emails_batch

            gmail_result = await fetch_gmail_emails_batch(
                access_token=access_token,
                email_address=email_address,
                total_emails=500,
                session_id=session_id,
            )

            if not gmail_result.get("success"):
                logger.error(f"âŒ Gmail æŠ“å–å¤±æ•—: {gmail_result.get('error')}")
                raise ValueError(f"Gmail æŠ“å–å¤±æ•—: {gmail_result.get('error')}")

            csv_path = gmail_result.get("csv_path")
            logger.info(f"âœ… Gmail æ•¸æ“šå·²ä¿å­˜åˆ°: {csv_path}")
            logger.info(f"ğŸ“Š æŠ“å–äº† {gmail_result.get('total_emails')} å°éƒµä»¶")

            # ğŸ”„ **é‡æ–°è¨­ç½®ç‚º file æ¨¡å¼ï¼Œè®“å®ƒèµ°å®Œæ•´çš„ local file è™•ç†æµç¨‹**
            logger.info("ğŸ”„ Gmail æŠ“å–å®Œæˆï¼Œè½‰æ›ç‚º local file æ¨¡å¼è™•ç†...")

            # é‡æ–°è¨­ç½® context_data å’Œ request_type
            context_data = {
                "type": "file",
                "file_path": csv_path,
                # ä¿ç•™ Gmail ç›¸é—œä¿¡æ¯
                "original_query": message,
                "email_address": email_address,
                "gmail_metadata": {
                    "total_emails": gmail_result.get("total_emails"),
                    "successful_batches": gmail_result.get("successful_batches"),
                    "failed_batches": gmail_result.get("failed_batches"),
                },
            }
            request_type = "file"

            logger.info(f"ğŸ“Š å·²è½‰æ›ç‚º file æ¨¡å¼ï¼Œå°‡é‡æ–°è™•ç†:")
            logger.info(f"  - æ–‡ä»¶è·¯å¾‘: {csv_path}")
            logger.info(f"  - æ–°çš„ request_type: {request_type}")

            # ğŸ”„ **é‡æ–°æ¨¡æ“¬ local file è«‹æ±‚ï¼Œèµ°å®Œæ•´çš„ local file è™•ç†æµç¨‹**
            logger.info("ğŸ”„ Gmail æ¨¡å¼è½‰æ›ç‚º local file æ¨¡å¼ï¼Œé‡æ–°è™•ç†...")

            # ğŸ”„ **é‡æ–°èµ° local file çš„å®Œæ•´è™•ç†é‚è¼¯**
            logger.info("ğŸ“„ é‡æ–°è™•ç† Gmail CSV æ–‡ä»¶ï¼Œç²å– data_info...")

            # ç›´æ¥èª¿ç”¨åº•å±¤çš„æ•¸æ“šåˆ†æå‡½æ•¸ç²å–æ•¸æ“šä¿¡æ¯ï¼ˆå’Œ local file æ¨¡å¼ä¸€æ¨£ï¼‰
            from src.tools.data_analysis_tools import data_analysis_tools

            try:
                data_info_result = await data_analysis_tools.get_data_info(
                    csv_path, session_id
                )
                logger.info(
                    f"ğŸ“Š get_data_info_tool åŸ·è¡Œçµæœ: {str(data_info_result)[:500]}..."
                )

                # æ§‹å»º final_contextï¼Œå’Œ local file æ¨¡å¼å®Œå…¨ä¸€æ¨£
                final_context = {
                    "file_path": csv_path,
                    "data_info": data_info_result,
                    # ä¿ç•™ Gmail ç›¸é—œçš„é¡å¤–ä¿¡æ¯
                    "original_query": message,
                    "email_address": email_address,
                    "gmail_metadata": {
                        "total_emails": gmail_result.get("total_emails"),
                        "successful_batches": gmail_result.get("successful_batches"),
                        "failed_batches": gmail_result.get("failed_batches"),
                    },
                }

                # ä½¿ç”¨ local file å·¥å…·é›†
                available_tools = get_langchain_local_file_tools()

                logger.info("ï¿½ Gmail æ¨¡å¼å·²å®Œå…¨è½‰æ›ç‚º local file è™•ç†æ¨¡å¼:")
                logger.info(f"  - æ–‡ä»¶è·¯å¾‘: {csv_path}")
                logger.info(f"  - éƒµä»¶æ•¸é‡: {gmail_result.get('total_emails')}")
                logger.info(f"  - data_info é¡å‹: {type(final_context['data_info'])}")

            except Exception as e:
                logger.error(f"âŒ è™•ç† Gmail CSV æ–‡ä»¶å¤±æ•—: {e}")
                raise ValueError(f"Gmail æ•¸æ“šè™•ç†å¤±æ•—: {e}")

        elif request_type == "multi_file":
            print("ğŸ“Š MULTI_FILE æ¨¡å¼ - è™•ç†å¤šå€‹è³‡æ–™é›†")
            logger.info("ğŸ“Š MULTI_FILE æ¨¡å¼ - è™•ç†å¤šå€‹è³‡æ–™é›†")

            # å¾å‰ç«¯å‚³ä¾†çš„å¤šæª”æ¡ˆè³‡æ–™
            files = context_data.get("files", [])
            total_files = context_data.get("total_files", 0)

            print(f"ğŸ“Š è™•ç† {total_files} å€‹æª”æ¡ˆ")
            print(f"ğŸ“Š æª”æ¡ˆè©³ç´°è³‡è¨Š:")
            for i, file_info in enumerate(files):
                print(f"  æª”æ¡ˆ {i+1}: {file_info}")

            logger.info(f"ğŸ“Š è™•ç† {total_files} å€‹æª”æ¡ˆ")

            # ç›´æ¥è®€å–ç¾æœ‰çš„ CSV æª”æ¡ˆ
            import pandas as pd
            import os
            from pathlib import Path

            created_files = []

            for file_info in files:
                source = file_info.get("source")
                filename = file_info.get("filename")
                file_path = file_info.get("file_path")

                if not file_path:
                    logger.warning(f"âš ï¸ æª”æ¡ˆ {filename} æ²’æœ‰æä¾›è·¯å¾‘")
                    continue

                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œè™•ç†ç›¸å°è·¯å¾‘
                if file_path.startswith("../"):
                    # ç›¸å°æ–¼ backend/ çš„è·¯å¾‘
                    full_path = Path(file_path)
                elif file_path.startswith("data/"):
                    # ç›¸å°æ–¼é …ç›®æ ¹ç›®éŒ„çš„è·¯å¾‘
                    full_path = Path("..") / file_path
                else:
                    # çµ•å°è·¯å¾‘æˆ–å…¶ä»–æƒ…æ³
                    full_path = Path(file_path)

                logger.info(f"ğŸ” æª¢æŸ¥æª”æ¡ˆè·¯å¾‘: {file_path} -> {full_path.absolute()}")

                if not full_path.exists():
                    logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {full_path.absolute()}")
                    continue

                try:
                    # è®€å–å®Œæ•´çš„ CSV æª”æ¡ˆ
                    df = pd.read_csv(full_path, encoding="utf-8-sig")

                    print(f"âœ… æˆåŠŸè®€å–æª”æ¡ˆ: {full_path}")
                    print(f"   - ä¾†æº: {source}")
                    print(f"   - æª”å: {filename}")
                    print(f"   - è¡Œæ•¸: {len(df)}")
                    print(f"   - æ¬„ä½æ•¸: {len(df.columns)}")
                    print(f"   - æ¬„ä½åç¨±: {list(df.columns)}")
                    print(f"   - å‰3è¡Œè³‡æ–™:")
                    print(df.head(3).to_string())

                    created_files.append(
                        {
                            "source": source,
                            "filename": filename,
                            "file_path": str(full_path),
                            "row_count": len(df),
                            "columns": list(df.columns),
                        }
                    )

                    logger.info(
                        f"âœ… è®€å–æª”æ¡ˆ: {full_path} ({len(df)} è¡Œ, {len(df.columns)} åˆ—)"
                    )
                    logger.info(f"   æ¬„ä½: {list(df.columns)}")

                except Exception as e:
                    print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {filename}: {e}")
                    logger.error(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {filename}: {e}")
                    continue

            if not created_files:
                raise ValueError("æ²’æœ‰æˆåŠŸå»ºç«‹ä»»ä½•æª”æ¡ˆ")

            # æ§‹å»º final_contextï¼ŒåŒ…å«æ‰€æœ‰æª”æ¡ˆçš„è·¯å¾‘
            final_context = {
                "type": "multi_file",
                "files": created_files,
                "total_files": len(created_files),
                "session_id": session_id,
            }

            # ä½¿ç”¨ local file å·¥å…·é›†
            available_tools = get_langchain_local_file_tools()

            logger.info(f"âœ… å¤šæª”æ¡ˆæ¨¡å¼è¨­ç½®å®Œæˆ:")
            logger.info(f"  - å»ºç«‹æª”æ¡ˆæ•¸é‡: {len(created_files)}")
            for file_info in created_files:
                logger.info(
                    f"  - {file_info['source']}: {file_info['file_path']} ({file_info['row_count']} è¡Œ)"
                )

        elif request_type == "web":
            logger.info("ğŸŒ WEB æ¨¡å¼ - ä½¿ç”¨ Web Tools")
            # TODO: æ·»åŠ  Web Tools
            available_tools = []  # æš«æ™‚ç‚ºç©ºï¼Œç­‰å¾…å¯¦ç¾ Web Tools
            final_context = context_data
            logger.info(
                f"  - ä½¿ç”¨ä¸Šä¸‹æ–‡: {json.dumps(final_context, ensure_ascii=False, indent=2)}"
            )

        else:
            logger.info("ğŸ”§ DEFAULT æ¨¡å¼ - ä½¿ç”¨é»˜èªå·¥å…·é›†")
            available_tools = get_langchain_local_file_tools()
            final_context = context_data
            if final_context:
                logger.info(
                    f"  - ä½¿ç”¨ä¸Šä¸‹æ–‡: {json.dumps(final_context, ensure_ascii=False, indent=2)}"
                )
            else:
                logger.info(f"  - ç„¡ä¸Šä¸‹æ–‡æ•¸æ“š")

        logger.info(f"ğŸ”§ é¸æ“‡çš„å·¥å…·æ•¸é‡: {len(available_tools)}")

        # ç™¼é€é–‹å§‹äº‹ä»¶
        if request_type == "local_file":
            start_event = {
                "type": "start",
                "message": "ğŸ“ Local File æ¨¡å¼ï¼šæ–‡ä»¶é è™•ç†å·²å®Œæˆï¼Œé–‹å§‹åˆ†æ...",
            }
        elif request_type == "gmail":
            start_event = {
                "type": "start",
                "message": f"ğŸ“§ Gmail æ¨¡å¼ï¼šå·²æˆåŠŸæŠ“å– {final_context.get('gmail_metadata', {}).get('total_emails', 0)} å°éƒµä»¶ï¼Œé–‹å§‹åˆ†æ...",
            }
        elif request_type == "web":
            start_event = {
                "type": "start",
                "message": "ğŸŒ Web æ¨¡å¼ï¼šé–‹å§‹è™•ç†ç¶²é å…§å®¹...",
            }
        else:
            start_event = {
                "type": "start",
                "message": "ğŸ”§ Default æ¨¡å¼ï¼šé–‹å§‹è™•ç†ä»»å‹™...",
            }

        yield f"data: {json.dumps(start_event, ensure_ascii=False)}\n\n"

        # è§£æè¦å‰‡
        rule_name = None
        query = message
        logger.info(f"ğŸ” é–‹å§‹è§£æè¦å‰‡:")
        logger.info(f"  - åŸå§‹ message: '{message}'")
        logger.info(f"  - message.startswith('/'): {message.startswith('/')}")

        if message.startswith("/"):
            parts = message[1:].split(" ", 1)
            logger.info(f"  - åˆ†å‰²å¾Œçš„ parts: {parts}")
            logger.info(f"  - parts é•·åº¦: {len(parts)}")
            if len(parts) >= 1:
                rule_name = parts[0]
                query = parts[1] if len(parts) > 1 else ""
                logger.info(f"  - è§£æå‡ºçš„ rule_name: '{rule_name}'")
                logger.info(f"  - è§£æå‡ºçš„ query: '{query}'")

        logger.info(f"ğŸ¯ æœ€çµ‚è§£æçµæœ:")
        logger.info(f"  - rule_name: '{rule_name}'")
        logger.info(f"  - query: '{query}'")

        if rule_name:
            rule_event = {
                "type": "rule",
                "rule_name": rule_name,
                "message": f"ä½¿ç”¨è¦å‰‡: {rule_name}",
            }
            yield f"data: {json.dumps(rule_event, ensure_ascii=False)}\n\n"

        # ç™¼é€è™•ç†äº‹ä»¶
        processing_event = {"type": "processing", "message": "æ­£åœ¨åŸ·è¡Œä»»å‹™..."}
        yield f"data: {json.dumps(processing_event, ensure_ascii=False)}\n\n"

        # ğŸ¯ **é—œéµæ­¥é©Ÿï¼šå‚³éé è™•ç†å¾Œçš„ä¸Šä¸‹æ–‡çµ¦ SupervisorAgent**
        logger.info("ğŸ”„ æ­¥é©Ÿ2: æº–å‚™ä¸Šä¸‹æ–‡æ•¸æ“šå‚³éçµ¦ SupervisorAgent")

        context = (
            {
                "session_id": session_id,
                "context_data": final_context,  # é€™è£¡å·²ç¶“åŒ…å«äº† file_summary
                "current_time": __import__("datetime").datetime.now().isoformat(),
            }
            if final_context
            else {
                "session_id": session_id,
                "current_time": __import__("datetime").datetime.now().isoformat(),
            }
        )

        # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ summaryï¼ˆæ‡‰è©²å·²ç¶“åœ¨ final_context ä¸­ï¼‰
        if final_context and final_context.get("file_summary"):
            logger.info(f"ğŸ“‹ ç¢ºèªï¼šæ–‡ä»¶ Summary å·²åŒ…å«åœ¨ context_data ä¸­")
            logger.info(
                f"ğŸ“‹ Summary é¡å‹: {final_context['file_summary'].get('type', 'unknown')}"
            )
        elif file_summary:
            # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥æ·»åŠ åˆ° context
            context["file_summary"] = file_summary
            logger.info(f"ğŸ“‹ å‚™ç”¨ï¼šæ–‡ä»¶ Summary å·²ç›´æ¥æ·»åŠ åˆ° context")

        # å°‡å·¥å…·åç¨±åˆ—è¡¨å‚³éçµ¦ Agentï¼Œè€Œä¸æ˜¯å‡½æ•¸å°è±¡
        tool_names = [tool.name for tool in available_tools]
        context["available_tool_names"] = tool_names

        logger.info(
            f"ğŸ”„ æ­¥é©Ÿ3: æº–å‚™èª¿ç”¨ SupervisorAgentï¼Œå·¥å…·æ•¸é‡: {len(available_tools)}"
        )

        # ç²å–agentå¯¦ä¾‹ä¸¦è¨­ç½®streamå›èª¿
        agent = get_agent(session_id, stream_callback)

        # åŸ·è¡Œagentï¼Œstreamå›èª¿æœƒè‡ªå‹•è™•ç†å·¥å…·åŸ·è¡Œçµæœ
        result = await agent.run(
            query, rule_id=rule_name, context=context, available_tools=available_tools
        )

        # è½‰æ›numpyé¡å‹ä»¥é¿å…åºåˆ—åŒ–å•é¡Œ
        result = convert_numpy_types(result)

        # ç™¼é€æ‰€æœ‰å·¥å…·åŸ·è¡Œäº‹ä»¶ï¼ˆå£“ç¸®å¾Œï¼‰
        for event_data in stream_events:
            if event_data["type"] == "tool_result":
                # å£“ç¸®å·¥å…·çµæœ
                compressed_result = compress_tool_result(event_data["wrapped_result"])

                tool_event = {
                    "type": "tool_execution",
                    "tool_name": event_data["tool_name"],
                    "parameters": event_data["parameters"],
                    "execution_time": event_data["execution_time"],
                    "result": compressed_result,
                }
                tool_event = convert_numpy_types(tool_event)
                yield f"data: {json.dumps(tool_event, ensure_ascii=False)}\n\n"

        # ç™¼é€å·¥å…·ä½¿ç”¨äº‹ä»¶
        tools_used = result.get("tools_used", [])
        if tools_used:
            tools_event = {
                "type": "tools",
                "message": f'ä½¿ç”¨äº†å·¥å…·: {", ".join(tools_used)}',
            }
            tools_event = convert_numpy_types(tools_event)
            yield f"data: {json.dumps(tools_event, ensure_ascii=False)}\n\n"

        # ç™¼é€å…§å®¹äº‹ä»¶
        content_event = {
            "type": "content",
            "content": result.get("response", ""),
            "execution_time": result.get("execution_time", 0),
            "tools_used": tools_used,
        }
        content_event = convert_numpy_types(content_event)
        yield f"data: {json.dumps(content_event, ensure_ascii=False)}\n\n"

        # ç™¼é€å®Œæˆäº‹ä»¶
        complete_event = {
            "type": "complete",
            "message": "ä»»å‹™åŸ·è¡Œå®Œæˆ",
            "success": result.get("success", True),
        }
        complete_event = convert_numpy_types(complete_event)
        yield f"data: {json.dumps(complete_event, ensure_ascii=False)}\n\n"

    except Exception as e:
        logger.error(f"æµå¼éŸ¿æ‡‰ç”Ÿæˆå¤±æ•—: {e}")
        error_event = {"type": "error", "message": f"è™•ç†å¤±æ•—: {str(e)}"}
        json_str = json.dumps(error_event, ensure_ascii=False)
        yield f"data: {json_str}\n\n"

    finally:
        # ç™¼é€çµæŸæ¨™è¨˜
        yield "data: [DONE]\n\n"


@router.post("/stream")
async def stream_chat(request: StreamRequest):
    """æµå¼èŠå¤©æ¥å£"""
    try:
        print(f"ğŸš€ æ”¶åˆ°æµå¼èŠå¤©è«‹æ±‚: {request.message[:100]}...")
        print(f"  - user_id: {request.user_id}")
        print(f"  - session_id: {request.session_id}")

        logger.info(f"æ”¶åˆ°æµå¼èŠå¤©è«‹æ±‚: {request.message[:100]}...")
        logger.info(f"  - user_id: {request.user_id}")

        # å®Œæ•´è¼¸å‡º context_data - ä½¿ç”¨ logger è€Œä¸æ˜¯ print
        logger.info(f"ğŸ“‹ å®Œæ•´ context_data:")
        logger.info(f"  - type: {type(request.context_data)}")
        logger.info(f"  - content: {request.context_data}")
        print(f"ğŸ“‹ å®Œæ•´ context_data:")
        print(f"  - type: {type(request.context_data)}")
        print(f"  - content: {request.context_data}")

        if request.context_data:
            print(f"  - keys: {list(request.context_data.keys())}")
            for key, value in request.context_data.items():
                print(f"    {key}: {type(value)} = {value}")

                # ğŸ” ç‰¹åˆ¥æª¢æŸ¥ files å’Œ file_paths
                if key in ["files", "file_paths", "file_summary"] and value:
                    print(f"    ğŸ“ {key} è©³ç´°å…§å®¹:")
                    if isinstance(value, list):
                        for i, item in enumerate(value):
                            print(f"      [{i}]: {type(item)} = {item}")
                            if isinstance(item, dict):
                                for sub_key, sub_value in item.items():
                                    print(f"        {sub_key}: {sub_value}")
                    elif isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            print(f"      {sub_key}: {type(sub_value)} = {sub_value}")
                            if sub_key == "data_schema" and isinstance(sub_value, dict):
                                print(f"        data_schema å…§å®¹:")
                                for schema_key, schema_value in sub_value.items():
                                    print(
                                        f"          {schema_key}: {type(schema_value)} = {schema_value}"
                                    )
        else:
            print("âš ï¸ context_data ç‚ºç©ºæˆ– None")

        # é™åˆ¶ context_data æ—¥èªŒè¼¸å‡ºé•·åº¦
        context_str = str(request.context_data)
        if len(context_str) > 300:
            context_str = context_str[:300] + "..."
        logger.info(f"  - context_data: {context_str}")

        # ğŸ” åˆ¤æ–·è«‹æ±‚é¡å‹ä¸¦é¸æ“‡å°æ‡‰çš„è™•ç†æ–¹å¼
        request_type = _determine_request_type(request.context_data)
        print(f"ğŸ¯ åˆ¤æ–·çš„è«‹æ±‚é¡å‹: {request_type}")
        logger.info(f"ğŸ¯ è«‹æ±‚é¡å‹: {request_type}")

        return StreamingResponse(
            generate_stream_response(
                request.message,
                None,
                request.session_id,
                request.context_data,
                request_type,
            ),
            media_type="text/event-stream; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream; charset=utf-8",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
                "Access-Control-Allow-Methods": "*",
            },
        )

    except Exception as e:
        logger.error(f"æµå¼èŠå¤©å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status")
async def get_agent_status():
    """ç²å– Agent ç‹€æ…‹"""
    try:
        agent = get_agent()
        rules = agent.parser.list_rules()

        return {
            "status": "running",
            "rules_count": len(rules),
            "available_rules": rules,
        }
    except Exception as e:
        logger.error(f"ç²å–ç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))
