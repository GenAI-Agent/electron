#!/usr/bin/env python3
"""
AIé¸æƒ…æ²™ç›’ API è·¯ç”±
è™•ç†Threadã€PTTã€é™³æƒ…ç³»çµ±ç­‰è³‡æ–™ä¾†æºçš„ç®¡ç†å’Œåˆ†æ
"""

import os
import csv
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
import asyncio
import aiofiles
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# è³‡æ–™ç›®éŒ„
SANDBOX_DATA_DIR = Path(__file__).parent.parent.parent.parent / "data" / "sandbox"
SANDBOX_DATA_DIR.mkdir(exist_ok=True)

class RefreshRequest(BaseModel):
    source: str  # 'thread' æˆ– 'ptt'

class DataFile(BaseModel):
    filename: str
    date: str
    time: str
    fullPath: str

@router.get("/files")
async def get_available_files(source: str = Query(..., description="è³‡æ–™ä¾†æº: thread, ptt, petition")):
    """ç²å–æŒ‡å®šè³‡æ–™ä¾†æºçš„å¯ç”¨æª”æ¡ˆåˆ—è¡¨"""
    try:
        files = []
        
        # æ ¹æ“šè³‡æ–™ä¾†æºéæ¿¾æª”æ¡ˆ
        if source == "thread":
            pattern = "thread_data_*.csv"
        elif source == "ptt":
            pattern = "ptt_data_*.csv"
        elif source == "petition":
            pattern = "petition_system_*.csv"
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æ´çš„è³‡æ–™ä¾†æº")
        
        # æƒææª”æ¡ˆ
        for file_path in SANDBOX_DATA_DIR.glob(pattern):
            # è§£ææª”åä¸­çš„æ—¥æœŸæ™‚é–“
            filename = file_path.name
            try:
                # å‡è¨­æª”åæ ¼å¼: {source}_data_YYYYMMDD_HHMMSS.csv
                parts = filename.replace('.csv', '').split('_')
                if len(parts) >= 4:
                    date_str = parts[-2]  # YYYYMMDD
                    time_str = parts[-1]  # HHMMSS
                    
                    # æ ¼å¼åŒ–æ—¥æœŸæ™‚é–“
                    date_formatted = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                    time_formatted = f"{time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}"
                    
                    files.append(DataFile(
                        filename=filename,
                        date=date_formatted,
                        time=time_formatted,
                        fullPath=str(file_path)
                    ))
            except Exception as e:
                logger.warning(f"è§£ææª”åå¤±æ•— {filename}: {e}")
                continue
        
        # æŒ‰æ—¥æœŸæ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        files.sort(key=lambda x: f"{x.date} {x.time}", reverse=True)
        
        return files
        
    except Exception as e:
        logger.error(f"ç²å–æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/data")
async def get_file_data(filename: str = Query(..., description="æª”æ¡ˆåç¨±")):
    """è®€å–æŒ‡å®šæª”æ¡ˆçš„è³‡æ–™"""
    try:
        file_path = SANDBOX_DATA_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æª”æ¡ˆä¸å­˜åœ¨")
        
        # è®€å–CSVæª”æ¡ˆ
        data = []
        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
            content = await f.read()
            
        # è§£æCSV
        lines = content.strip().split('\n')
        if len(lines) < 2:
            return []
        
        headers = lines[0].split(',')
        for line in lines[1:]:
            values = line.split(',')
            if len(values) == len(headers):
                row = dict(zip(headers, values))
                data.append(row)
        
        return data
        
    except Exception as e:
        logger.error(f"è®€å–æª”æ¡ˆè³‡æ–™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/refresh")
async def refresh_data(request: RefreshRequest, background_tasks: BackgroundTasks):
    """åˆ·æ–°æŒ‡å®šè³‡æ–™ä¾†æºçš„è³‡æ–™"""
    try:
        if request.source not in ["thread", "ptt"]:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æ´çš„è³‡æ–™ä¾†æº")
        
        # ç”Ÿæˆæ–°çš„æª”æ¡ˆåç¨±ï¼ˆåŒ…å«ç•¶å‰æ™‚é–“æˆ³ï¼‰
        now = datetime.now()
        timestamp = now.strftime("%Y%m%d_%H%M%S")
        filename = f"{request.source}_data_{timestamp}.csv"
        file_path = SANDBOX_DATA_DIR / filename
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œè³‡æ–™åˆ·æ–°
        background_tasks.add_task(perform_data_refresh, request.source, file_path)
        
        return {
            "message": "è³‡æ–™åˆ·æ–°å·²é–‹å§‹",
            "filename": filename,
            "status": "processing"
        }
        
    except Exception as e:
        logger.error(f"åˆ·æ–°è³‡æ–™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def perform_data_refresh(source: str, file_path: Path):
    """åŸ·è¡Œå¯¦éš›çš„è³‡æ–™åˆ·æ–°ä½œæ¥­"""
    try:
        logger.info(f"é–‹å§‹åˆ·æ–° {source} è³‡æ–™...")
        
        if source == "thread":
            await refresh_thread_data(file_path)
        elif source == "ptt":
            await refresh_ptt_data(file_path)
        
        logger.info(f"{source} è³‡æ–™åˆ·æ–°å®Œæˆ: {file_path}")
        
    except Exception as e:
        logger.error(f"è³‡æ–™åˆ·æ–°å¤±æ•— {source}: {e}")

async def refresh_thread_data(file_path: Path):
    """åˆ·æ–°Threadè³‡æ–™"""
    # æ¨¡æ“¬APIèª¿ç”¨å’Œè³‡æ–™è™•ç†
    await asyncio.sleep(2)  # æ¨¡æ“¬APIèª¿ç”¨å»¶é²

    # ç”Ÿæˆæ¨¡æ“¬è³‡æ–™ï¼ˆå¯¦éš›æ‡‰è©²èª¿ç”¨Thread APIï¼‰
    import random

    topics = [
        "æ”¿åºœæ–°æ”¿ç­–è¨è«–", "ç¶“æ¿Ÿæ°‘ç”Ÿå•é¡Œ", "æ•™è‚²æ”¹é©è­°é¡Œ", "äº¤é€šå»ºè¨­é€²åº¦",
        "ç’°ä¿æ”¿ç­–åŸ·è¡Œ", "é†«ç™‚è³‡æºåˆ†é…", "æˆ¿å±‹æ”¿ç­–è¨è«–", "å‹å·¥æ¬Šç›Šä¿éšœ",
        "è¾²æ¥­ç™¼å±•å•é¡Œ", "æ–‡åŒ–ä¿å­˜å·¥ä½œ", "ç§‘æŠ€ç™¼å±•æ”¿ç­–", "ç¤¾æœƒç¦åˆ©åˆ¶åº¦"
    ]

    regions = ["å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "æ¡ƒåœ’å¸‚", "å°ä¸­å¸‚", "é«˜é›„å¸‚", "å°å—å¸‚", "æ–°ç«¹å¸‚", "åŸºéš†å¸‚"]
    genders = ["ç”·", "å¥³"]

    sample_data = []
    now = datetime.now()

    for i in range(20):  # ç”Ÿæˆ20ç­†è³‡æ–™
        topic = random.choice(topics)
        region = random.choice(regions)
        age = random.randint(20, 60)
        gender = random.choice(genders)

        # éš¨æ©Ÿç”Ÿæˆæ™‚é–“ï¼ˆéå»24å°æ™‚å…§ï¼‰
        time_offset = random.randint(0, 1440)  # 0-1440åˆ†é˜
        from datetime import timedelta
        post_time = now - timedelta(minutes=time_offset)

        sample_data.append({
            "æ—¥æœŸæ™‚é–“": post_time.strftime("%Y-%m-%d %H:%M:%S"),
            "è²¼æ–‡ID": f"T{post_time.strftime('%H%M%S')}{i:03d}",
            "å…§å®¹": f"é—œæ–¼{topic}çš„è¨è«–ï¼Œå¤§å®¶è¦ºå¾—å¦‚ä½•ï¼Ÿ",
            "é—œæ³¨æ•¸": str(random.randint(50, 500)),
            "è¡¨æƒ…åæ‡‰": f"ğŸ‘{random.randint(20, 200)} ğŸ‘{random.randint(5, 50)} ğŸ˜¡{random.randint(0, 30)}",
            "ç€è¦½æ•¸": str(random.randint(1000, 10000)),
            "åœ°å€": region,
            "å¹´é½¡": str(age),
            "æ€§åˆ¥": gender,
            "ä¸»é¡Œæ¨™ç±¤": f"#{topic[:4]}"
        })
    
    # å¯«å…¥CSVæª”æ¡ˆ
    async with aiofiles.open(file_path, 'w', encoding='utf-8', newline='') as f:
        if sample_data:
            headers = list(sample_data[0].keys())
            await f.write(','.join(headers) + '\n')
            
            for row in sample_data:
                values = [str(row.get(header, '')) for header in headers]
                await f.write(','.join(values) + '\n')

async def refresh_ptt_data(file_path: Path):
    """åˆ·æ–°PTTè³‡æ–™"""
    # æ¨¡æ“¬çˆ¬èŸ²å’Œè³‡æ–™è™•ç†
    await asyncio.sleep(3)  # æ¨¡æ“¬çˆ¬èŸ²å»¶é²

    # ç”Ÿæˆæ¨¡æ“¬è³‡æ–™ï¼ˆå¯¦éš›æ‡‰è©²åŸ·è¡ŒPTTçˆ¬èŸ²ï¼‰
    import random

    titles = [
        "[å•å¦] æ”¿åºœæ”¿ç­–åˆ°åº•æœ‰æ²’æœ‰æ•ˆï¼Ÿ",
        "[æ–°è] æœ€æ–°ç¶“æ¿Ÿæ•¸æ“šå‡ºçˆ",
        "[è¨è«–] æ•™è‚²æ”¹é©çš„å½±éŸ¿",
        "[æŠ±æ€¨] äº¤é€šå»ºè¨­é€²åº¦å¤ªæ…¢",
        "[ç’°ä¿] ç©ºæ°£å“è³ªæ”¹å–„æ–¹æ¡ˆ",
        "[é†«ç™‚] å¥ä¿åˆ¶åº¦æª¢è¨",
        "[æˆ¿å±‹] é’å¹´è³¼å±‹å›°é›£",
        "[å‹å·¥] è–ªè³‡æˆé•·åœæ»¯"
    ]

    boards = ["Gossiping", "Politics", "HatePolitics", "PublicServan", "Salary", "home-sale"]
    regions = ["å°åŒ—å¸‚", "æ–°åŒ—å¸‚", "æ¡ƒåœ’å¸‚", "å°ä¸­å¸‚", "é«˜é›„å¸‚", "å°å—å¸‚", "æ–°ç«¹å¸‚", "åŸºéš†å¸‚"]
    genders = ["ç”·", "å¥³"]

    sample_data = []
    now = datetime.now()

    for i in range(25):  # ç”Ÿæˆ25ç­†è³‡æ–™
        title = random.choice(titles)
        board = random.choice(boards)
        region = random.choice(regions)
        age = random.randint(18, 55)
        gender = random.choice(genders)

        # éš¨æ©Ÿç”Ÿæˆæ™‚é–“ï¼ˆéå»48å°æ™‚å…§ï¼‰
        time_offset = random.randint(0, 2880)  # 0-2880åˆ†é˜
        from datetime import timedelta
        post_time = now - timedelta(minutes=time_offset)

        sample_data.append({
            "æ—¥æœŸæ™‚é–“": post_time.strftime("%Y-%m-%d %H:%M:%S"),
            "æ–‡ç« ID": f"P{post_time.strftime('%H%M%S')}{i:03d}",
            "æ¨™é¡Œ": title,
            "å…§å®¹æ‘˜è¦": f"é—œæ–¼{title[4:]}çš„è©³ç´°è¨è«–å…§å®¹...",
            "æ¨æ–‡æ•¸": str(random.randint(10, 200)),
            "å™“æ–‡æ•¸": str(random.randint(0, 50)),
            "ç€è¦½æ•¸": str(random.randint(500, 5000)),
            "åœ°å€": region,
            "å¹´é½¡": str(age),
            "æ€§åˆ¥": gender,
            "çœ‹æ¿": board
        })
    
    # å¯«å…¥CSVæª”æ¡ˆ
    async with aiofiles.open(file_path, 'w', encoding='utf-8', newline='') as f:
        if sample_data:
            headers = list(sample_data[0].keys())
            await f.write(','.join(headers) + '\n')
            
            for row in sample_data:
                values = [str(row.get(header, '')) for header in headers]
                await f.write(','.join(values) + '\n')

@router.get("/stats")
async def get_data_stats(source: str = Query(..., description="è³‡æ–™ä¾†æº")):
    """ç²å–è³‡æ–™çµ±è¨ˆè³‡è¨Š"""
    try:
        files = await get_available_files(source)
        
        total_files = len(files)
        latest_file = files[0] if files else None
        
        # å¦‚æœæœ‰æœ€æ–°æª”æ¡ˆï¼Œè®€å–è³‡æ–™é‡
        total_records = 0
        if latest_file:
            data = await get_file_data(latest_file.filename)
            total_records = len(data)
        
        return {
            "source": source,
            "total_files": total_files,
            "total_records": total_records,
            "latest_update": f"{latest_file.date} {latest_file.time}" if latest_file else None
        }
        
    except Exception as e:
        logger.error(f"ç²å–çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))
