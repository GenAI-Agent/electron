"""
Data Analysis Tools

æä¾›é€šç”¨çš„æ•¸æ“šåˆ†ææ–¹æ³•å·¥å…·ï¼Œç”± Agent æŒ‡å®šå…·é«”çš„åˆ—åå’Œåƒæ•¸ã€‚
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from scipy import stats
from sklearn.linear_model import LinearRegression
import logging

logger = logging.getLogger(__name__)


class DataAnalysisTools:
    """é€šç”¨æ•¸æ“šåˆ†æå·¥å…·é›†"""

    def __init__(self):
        pass
    
    async def load_data_file(self, file_path: str) -> Dict[str, Any]:
        """
        åŠ è¼‰æ•¸æ“šæ–‡ä»¶ï¼Œä»¥ JSON æ ¼å¼ç‚ºä¸»

        Args:
            file_path: æ–‡ä»¶è·¯å¾‘

        Returns:
            åŒ…å«æ•¸æ“šå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        file_ext = Path(file_path).suffix.lower()

        try:
            result = {
                "file_path": file_path,
                "file_type": file_ext,
                "data": None,
                "metadata": {}
            }

            if file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                result["data"] = data
                result["metadata"]["original_format"] = "json"

            elif file_ext == '.csv':
                # CSV è½‰æ›ç‚º JSON æ ¼å¼ï¼Œè™•ç†å¤šè¡Œæ¬„ä½å’Œç·¨ç¢¼å•é¡Œ
                try:
                    # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼å’Œåƒæ•¸çµ„åˆ
                    encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp1252']
                    df = None

                    for encoding in encodings:
                        try:
                            df = pd.read_csv(
                                file_path,
                                encoding=encoding,
                                quotechar='"',           # æŒ‡å®šå¼•è™Ÿå­—ç¬¦
                                quoting=1,               # QUOTE_ALL
                                skipinitialspace=True,   # è·³éåˆå§‹ç©ºæ ¼
                                on_bad_lines='skip',     # è·³éæœ‰å•é¡Œçš„è¡Œ
                                engine='python'          # ä½¿ç”¨Pythonå¼•æ“ï¼Œæ›´å¥½è™•ç†è¤‡é›œCSV
                            )
                            logger.info(f"âœ… æˆåŠŸä½¿ç”¨ç·¨ç¢¼ {encoding} è®€å–CSVæª”æ¡ˆ")
                            break
                        except (UnicodeDecodeError, pd.errors.ParserError) as e:
                            logger.warning(f"âš ï¸ ç·¨ç¢¼ {encoding} è®€å–å¤±æ•—: {e}")
                            continue

                    if df is None:
                        raise ValueError("ç„¡æ³•ä½¿ç”¨ä»»ä½•ç·¨ç¢¼æˆåŠŸè®€å–CSVæª”æ¡ˆ")

                    # æ¸…ç†æ•¸æ“šï¼šç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
                    df = df.dropna(how='all')

                    # è™•ç†å¤šè¡Œå…§å®¹ï¼šå°‡æ›è¡Œç¬¦è™Ÿè½‰æ›ç‚ºç©ºæ ¼
                    for col in df.columns:
                        if df[col].dtype == 'object':  # å­—ç¬¦ä¸²åˆ—
                            df[col] = df[col].astype(str).str.replace('\n', ' ').str.replace('\r', ' ')

                    result["data"] = df.to_dict('records')
                    result["metadata"]["original_format"] = "csv"
                    result["metadata"]["columns"] = list(df.columns)
                    result["metadata"]["shape"] = df.shape

                except Exception as csv_error:
                    logger.error(f"CSVè®€å–å¤±æ•—: {csv_error}")
                    raise ValueError(f"CSVæª”æ¡ˆè®€å–å¤±æ•—: {csv_error}")

            elif file_ext in ['.xlsx', '.xls']:
                # Excel è½‰æ›ç‚º JSON æ ¼å¼
                df = pd.read_excel(file_path)
                result["data"] = df.to_dict('records')
                result["metadata"]["original_format"] = "excel"
                result["metadata"]["columns"] = list(df.columns)
                result["metadata"]["shape"] = df.shape

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•¸æ“šæ–‡ä»¶æ ¼å¼: {file_ext}")

            return result

        except Exception as e:
            logger.error(f"åŠ è¼‰æ•¸æ“šæ–‡ä»¶å¤±æ•— {file_path}: {e}")
            raise
    
    async def group_by_analysis(self, file_path: str, group_column: str, value_column: str,
                               operation: str = "sum", session_id: str = "default") -> Dict[str, Any]:
        """
        é€šç”¨åˆ†çµ„åˆ†æå·¥å…·ï¼ˆåŸºæ–¼ JSON æ•¸æ“šï¼‰

        Args:
            file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
            group_column: åˆ†çµ„åˆ—å
            value_column: æ•¸å€¼åˆ—å
            operation: çµ±è¨ˆæ“ä½œ (sum, mean, count, max, min)
            session_id: æœƒè©±ID

        Returns:
            åˆ†çµ„åˆ†æçµæœ
        """
        try:
            data_result = await self.load_data_file(file_path)
            data_list = data_result["data"]

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼
            if not isinstance(data_list, list) or not data_list:
                return {
                    "success": False,
                    "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢ºï¼Œéœ€è¦æ˜¯åŒ…å«å°è±¡çš„æ•¸çµ„"
                }

            # æª¢æŸ¥å¿…è¦çš„åˆ—
            first_item = data_list[0]
            if group_column not in first_item or value_column not in first_item:
                available_keys = list(first_item.keys()) if isinstance(first_item, dict) else []
                return {
                    "success": False,
                    "error": f"ç¼ºå°‘å¿…è¦çš„åˆ—: {group_column} æˆ– {value_column}",
                    "available_columns": available_keys
                }

            # æ‰‹å‹•é€²è¡Œåˆ†çµ„åˆ†æ
            groups = {}
            total_value = 0

            for item in data_list:
                if not isinstance(item, dict):
                    continue

                group_val = str(item.get(group_column, "æœªçŸ¥"))
                value_val = item.get(value_column, 0)

                # å˜—è©¦è½‰æ›ç‚ºæ•¸å­—ï¼Œè™•ç†å„ç¨®æ ¼å¼
                try:
                    if value_val is None or value_val == "":
                        value_val = 0
                    else:
                        # è™•ç†å­—ç¬¦ä¸²æ ¼å¼çš„æ•¸å­—
                        if isinstance(value_val, str):
                            # ç§»é™¤é€—è™Ÿå’Œç©ºæ ¼
                            value_val = value_val.replace(",", "").replace(" ", "").strip()
                            if value_val == "":
                                value_val = 0
                            else:
                                value_val = float(value_val)
                        else:
                            value_val = float(value_val)
                except (ValueError, TypeError):
                    logger.warning(f"âš ï¸ ç„¡æ³•è½‰æ›æ•¸å€¼: {repr(item.get(value_column))} -> è¨­ç‚º 0")
                    value_val = 0

                if group_val not in groups:
                    groups[group_val] = []

                groups[group_val].append(value_val)
                total_value += value_val

            # è¨ˆç®—çµ±è¨ˆ
            result = {
                "success": True,
                "analysis_type": "group_by_analysis",
                "session_id": session_id,
                "group_column": group_column,
                "value_column": value_column,
                "operation": operation,
                "results": {}
            }

            for group_val, values in groups.items():
                if values:
                    values_array = np.array(values)
                    stats = {
                        "count": len(values),
                        "sum": float(np.sum(values_array)),
                        "mean": float(np.mean(values_array)),
                        "median": float(np.median(values_array)),
                        "std": float(np.std(values_array)),
                        "min": float(np.min(values_array)),
                        "max": float(np.max(values_array))
                    }

                    # æ ¹æ“šoperationè¿”å›ä¸»è¦çµæœ
                    if operation in stats:
                        result["results"][group_val] = {
                            "value": stats[operation],
                            "all_stats": stats
                        }
                    else:
                        result["results"][group_val] = stats

            # è¨ˆç®—ç¸½é«”çµ±è¨ˆå’Œä½”æ¯”
            result["summary"] = {
                "total_value": float(total_value),
                "group_percentages": {}
            }

            for group_val in result["results"]:
                if total_value > 0:
                    # å®‰å…¨åœ°ç²å– sum å€¼
                    group_result = result["results"][group_val]
                    if isinstance(group_result, dict):
                        if "all_stats" in group_result:
                            sum_value = group_result["all_stats"]["sum"]
                        elif "sum" in group_result:
                            sum_value = group_result["sum"]
                        else:
                            sum_value = 0
                    else:
                        sum_value = 0

                    percentage = (sum_value / total_value) * 100
                    result["summary"]["group_percentages"][group_val] = round(percentage, 2)

            return result

        except Exception as e:
            logger.error(f"åˆ†çµ„åˆ†æå¤±æ•— {file_path}: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def threshold_analysis(self, file_path: str, value_column: str, threshold: float,
                                comparison: str = "greater", session_id: str = "default") -> Dict[str, Any]:
        """
        é€šç”¨é–¾å€¼åˆ†æå·¥å…·

        Args:
            file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
            value_column: æ•¸å€¼åˆ—å
            threshold: é–¾å€¼
            comparison: æ¯”è¼ƒæ–¹å¼ ('greater', 'less', 'equal')
            session_id: æœƒè©±ID

        Returns:
            é–¾å€¼åˆ†æçµæœ
        """
        try:
            data_result = await self.load_data_file(file_path)
            data_list = data_result["data"]

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼
            if not isinstance(data_list, list) or not data_list:
                return {
                    "success": False,
                    "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢ºï¼Œéœ€è¦æ˜¯åŒ…å«å°è±¡çš„æ•¸çµ„"
                }

            # æª¢æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            first_item = data_list[0] if data_list else {}
            if value_column not in first_item:
                available_keys = list(first_item.keys()) if isinstance(first_item, dict) else []
                return {
                    "success": False,
                    "error": f"ç¼ºå°‘åˆ— '{value_column}'",
                    "available_columns": available_keys
                }

            # æ‰‹å‹•ç¯©é¸æ•¸æ“š
            filtered_items = []
            total_value = 0
            filtered_value = 0

            for item in data_list:
                if not isinstance(item, dict) or value_column not in item:
                    continue

                try:
                    value = float(item[value_column])
                    total_value += value

                    # æ ¹æ“šæ¯”è¼ƒæ–¹å¼åˆ¤æ–·
                    meets_condition = False
                    if comparison == "greater":
                        meets_condition = value > threshold
                        condition_desc = f"å¤§æ–¼ {threshold}"
                    elif comparison == "less":
                        meets_condition = value < threshold
                        condition_desc = f"å°æ–¼ {threshold}"
                    elif comparison == "equal":
                        meets_condition = value == threshold
                        condition_desc = f"ç­‰æ–¼ {threshold}"
                    else:
                        return {
                            "success": False,
                            "error": f"ä¸æ”¯æŒçš„æ¯”è¼ƒæ–¹å¼: {comparison}"
                        }

                    if meets_condition:
                        filtered_items.append(item)
                        filtered_value += value

                except (ValueError, TypeError):
                    continue

            # è¨ˆç®—çµ±è¨ˆ
            total_records = len(data_list)
            filtered_count = len(filtered_items)

            # è¨ˆç®—ä½”æ¯”
            count_percentage = (filtered_count / total_records) * 100 if total_records > 0 else 0
            value_percentage = (filtered_value / total_value) * 100 if total_value > 0 else 0

            return {
                "success": True,
                "analysis_type": "threshold_analysis",
                "session_id": session_id,
                "value_column": value_column,
                "threshold": threshold,
                "comparison": comparison,
                "condition": condition_desc,
                "results": {
                    "total_records": total_records,
                    "total_value": float(total_value),
                    "filtered_records": filtered_count,
                    "filtered_value": float(filtered_value),
                    "count_percentage": round(count_percentage, 2),
                    "value_percentage": round(value_percentage, 2)
                },
                "filtered_data": filtered_items[:20]  # åªè¿”å›å‰20å€‹çµæœ
            }

        except Exception as e:
            logger.error(f"é–¾å€¼åˆ†æå¤±æ•— {file_path}: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def correlation_analysis(self, file_path: str, x_column: str, y_column: str,
                                  session_id: str = "default") -> Dict[str, Any]:
        """
        é€šç”¨ç›¸é—œæ€§åˆ†æå·¥å…·

        Args:
            file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
            x_column: Xè»¸è®Šé‡åˆ—å
            y_column: Yè»¸è®Šé‡åˆ—å
            session_id: æœƒè©±ID

        Returns:
            ç›¸é—œæ€§åˆ†æçµæœ
        """
        try:
            data_result = await self.load_data_file(file_path)
            data_list = data_result["data"]

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼
            if not isinstance(data_list, list) or not data_list:
                return {
                    "success": False,
                    "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢ºï¼Œéœ€è¦æ˜¯åŒ…å«å°è±¡çš„æ•¸çµ„"
                }

            # æª¢æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            first_item = data_list[0] if data_list else {}
            required_cols = [x_column, y_column]
            missing_cols = [col for col in required_cols if col not in first_item]
            if missing_cols:
                available_keys = list(first_item.keys()) if isinstance(first_item, dict) else []
                return {
                    "success": False,
                    "error": f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}",
                    "available_columns": available_keys
                }

            # æå–æ•¸å€¼æ•¸æ“š
            x_values = []
            y_values = []

            for item in data_list:
                if not isinstance(item, dict):
                    continue

                try:
                    x_val = float(item.get(x_column, 0))
                    y_val = float(item.get(y_column, 0))
                    x_values.append(x_val)
                    y_values.append(y_val)
                except (ValueError, TypeError):
                    continue

            if len(x_values) < 2 or len(y_values) < 2:
                return {
                    "success": False,
                    "error": "æœ‰æ•ˆæ•¸æ“šé»ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œç›¸é—œæ€§åˆ†æ"
                }

            # è½‰æ›ç‚º numpy æ•¸çµ„
            x_array = np.array(x_values)
            y_array = np.array(y_values)

            # è¨ˆç®—ç›¸é—œä¿‚æ•¸
            correlation = np.corrcoef(x_array, y_array)[0, 1]

            # é€²è¡Œç·šæ€§å›æ­¸
            X = x_array.reshape(-1, 1)
            y = y_array

            model = LinearRegression()
            model.fit(X, y)

            # è¨ˆç®— RÂ²
            r_squared = model.score(X, y)

            # çµ±è¨ˆæª¢é©—
            correlation_test = stats.pearsonr(x_array, y_array)

            return {
                "success": True,
                "analysis_type": "correlation_analysis",
                "session_id": session_id,
                "x_column": x_column,
                "y_column": y_column,
                "results": {
                    "correlation": round(correlation, 4),
                    "p_value": round(correlation_test[1], 4),
                    "r_squared": round(r_squared, 4),
                    "slope": round(model.coef_[0], 4),
                    "intercept": round(model.intercept_, 4),
                    "regression_equation": f"{y_column} = {round(model.coef_[0], 4)} Ã— {x_column} + {round(model.intercept_, 4)}"
                },
                "interpretation": self._interpret_correlation(correlation),
                "model_params": {
                    "slope": float(model.coef_[0]),
                    "intercept": float(model.intercept_)
                },
                "data_points": len(x_values)
            }

        except Exception as e:
            logger.error(f"ç›¸é—œæ€§åˆ†æå¤±æ•— {file_path}: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def linear_prediction(self, file_path: str, x_column: str, y_column: str,
                               target_x_value: float, session_id: str = "default") -> Dict[str, Any]:
        """
        é€šç”¨ç·šæ€§é æ¸¬å·¥å…·

        Args:
            file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
            x_column: è‡ªè®Šé‡åˆ—å
            y_column: å› è®Šé‡åˆ—å
            target_x_value: ç›®æ¨™è‡ªè®Šé‡å€¼
            session_id: æœƒè©±ID

        Returns:
            é æ¸¬çµæœ
        """
        try:
            df = await self.load_data_file(file_path)

            required_cols = [x_column, y_column]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                return {
                    "success": False,
                    "error": f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}",
                    "available_columns": list(df.columns)
                }

            # ç¢ºä¿å…©åˆ—éƒ½æ˜¯æ•¸å­—é¡å‹
            try:
                df[x_column] = pd.to_numeric(df[x_column], errors='coerce')
                df[y_column] = pd.to_numeric(df[y_column], errors='coerce')
            except:
                return {
                    "success": False,
                    "error": f"åˆ— '{x_column}' æˆ– '{y_column}' ç„¡æ³•è½‰æ›ç‚ºæ•¸å­—é¡å‹"
                }

            # ç§»é™¤ç¼ºå¤±å€¼
            clean_df = df[[x_column, y_column]].dropna()

            if len(clean_df) < 2:
                return {
                    "success": False,
                    "error": "æœ‰æ•ˆæ•¸æ“šé»ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œé æ¸¬"
                }

            # è¨“ç·´æ¨¡å‹
            X = clean_df[[x_column]]
            y = clean_df[y_column]

            model = LinearRegression()
            model.fit(X, y)

            # é æ¸¬
            predicted_value = model.predict([[target_x_value]])[0]

            # è¨ˆç®—é æ¸¬å€é–“ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            residuals = y - model.predict(X)
            mse = np.mean(residuals ** 2)
            std_error = np.sqrt(mse)

            # 95% é æ¸¬å€é–“
            confidence_interval = 1.96 * std_error
            lower_bound = predicted_value - confidence_interval
            upper_bound = predicted_value + confidence_interval

            # æ‰¾åˆ°ç›¸ä¼¼å€¼çš„å¯¦éš›æ•¸æ“š
            value_range = abs(target_x_value * 0.1)  # Â±10%
            similar_data = clean_df[
                (clean_df[x_column] >= target_x_value - value_range) &
                (clean_df[x_column] <= target_x_value + value_range)
            ]

            return {
                "success": True,
                "analysis_type": "linear_prediction",
                "session_id": session_id,
                "x_column": x_column,
                "y_column": y_column,
                "target_x_value": target_x_value,
                "prediction": {
                    "predicted_value": round(predicted_value, 4),
                    "confidence_interval_lower": round(lower_bound, 4),
                    "confidence_interval_upper": round(upper_bound, 4),
                    "confidence_interval": f"{round(lower_bound, 4)} - {round(upper_bound, 4)}"
                },
                "model_info": {
                    "regression_equation": f"{y_column} = {round(model.coef_[0], 4)} Ã— {x_column} + {round(model.intercept_, 4)}",
                    "r_squared": round(model.score(X, y), 4),
                    "standard_error": round(std_error, 4)
                },
                "reference_data": {
                    "similar_range": f"{target_x_value - value_range:.2f} - {target_x_value + value_range:.2f}",
                    "similar_count": len(similar_data),
                    "similar_mean": round(similar_data[y_column].mean(), 4) if len(similar_data) > 0 else None,
                    "similar_range_values": f"{similar_data[y_column].min():.2f} - {similar_data[y_column].max():.2f}" if len(similar_data) > 0 else None
                },
                "data_points": len(clean_df)
            }

        except Exception as e:
            logger.error(f"ç·šæ€§é æ¸¬å¤±æ•— {file_path}: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _interpret_correlation(self, correlation: float) -> str:
        """è§£é‡‹ç›¸é—œä¿‚æ•¸"""
        abs_corr = abs(correlation)
        
        if abs_corr >= 0.8:
            strength = "éå¸¸å¼·"
        elif abs_corr >= 0.6:
            strength = "å¼·"
        elif abs_corr >= 0.4:
            strength = "ä¸­ç­‰"
        elif abs_corr >= 0.2:
            strength = "å¼±"
        else:
            strength = "éå¸¸å¼±"
        
        direction = "æ­£" if correlation > 0 else "è² "
        
        return f"{direction}ç›¸é—œï¼Œç›¸é—œå¼·åº¦ï¼š{strength}"
    
    async def get_data_info(self, file_path: str, session_id: str = "default") -> Dict[str, Any]:
        """
        ç²å–æ•¸æ“šæ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼ˆåŸºæ–¼ JSON æ•¸æ“šï¼‰

        Args:
            file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
            session_id: æœƒè©±ID

        Returns:
            æ•¸æ“šæ–‡ä»¶ä¿¡æ¯
        """
        try:
            data_result = await self.load_data_file(file_path)
            data_list = data_result["data"]

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼
            if not isinstance(data_list, list) or not data_list:
                return {
                    "success": False,
                    "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢ºï¼Œéœ€è¦æ˜¯åŒ…å«å°è±¡çš„æ•¸çµ„"
                }

            # åˆ†ææ•¸æ“šçµæ§‹
            first_item = data_list[0] if data_list else {}
            if not isinstance(first_item, dict):
                return {
                    "success": False,
                    "error": "æ•¸æ“šé …ç›®æ ¼å¼ä¸æ­£ç¢ºï¼Œéœ€è¦æ˜¯å°è±¡"
                }

            # åŸºæœ¬ä¿¡æ¯
            columns = list(first_item.keys())
            row_count = len(data_list)

            # é¸æ“‡æœ€å°‘NaNå€¼çš„è³‡æ–™ç­†ä½œç‚ºæ¨£æœ¬
            def count_nan_values(row_dict):
                """è¨ˆç®—ä¸€ç­†è³‡æ–™ä¸­çš„NaNå€¼æ•¸é‡"""
                nan_count = 0
                for value in row_dict.values():
                    if value is None or value == "" or \
                       (isinstance(value, float) and np.isnan(value)) or \
                       (isinstance(value, str) and value.lower() in ['nan', 'null', 'none', 'na', '']):
                        nan_count += 1
                return nan_count

            # å°æ‰€æœ‰è³‡æ–™æŒ‰NaNæ•¸é‡æ’åºï¼Œé¸æ“‡æœ€å°‘NaNçš„1ç­†ä½œç‚ºæ¨£æœ¬
            sorted_data = sorted(data_list, key=count_nan_values)
            best_sample_data = sorted_data[:1]  # ç¢ºä¿åªè¿”å›1ç­†æœ€å®Œæ•´çš„è³‡æ–™

            info = {
                "session_id": session_id,
                "total_rows": row_count,
                "columns": columns,
                "sample_data": best_sample_data,  # åªåŒ…å«1ç­†æœ€å®Œæ•´çš„æ¨£æœ¬æ•¸æ“š
                "data_shape": [row_count, len(columns)]  # æ·»åŠ æ•¸æ“šå½¢ç‹€ä¿¡æ¯
            }

            # åˆ†æåˆ—é¡å‹å’Œçµ±è¨ˆ
            numeric_columns = []
            categorical_columns = []
            id_columns = []
            column_stats = {}

            def _is_id_column(col_name: str, numeric_values: list, non_null_values: list) -> bool:
                """æª¢æ¸¬æ˜¯å¦ç‚ºIDé¡å‹çš„åˆ—"""
                if not numeric_values:
                    return False

                # æª¢æŸ¥åˆ—åæ˜¯å¦åŒ…å«IDç›¸é—œé—œéµå­—
                id_keywords = ['id', 'no', 'code', 'pk', 'key', 'ref']
                col_lower = col_name.lower()
                has_id_keyword = any(keyword in col_lower for keyword in id_keywords)

                # æª¢æŸ¥å”¯ä¸€æ€§æ¯”ä¾‹
                unique_ratio = len(set(str(v) for v in non_null_values)) / len(non_null_values) if non_null_values else 0

                # æª¢æŸ¥æ˜¯å¦éƒ½æ˜¯æ­£æ•´æ•¸
                all_positive_integers = all(isinstance(v, (int, float)) and v > 0 and v == int(v) for v in numeric_values)

                # æª¢æŸ¥æ•¸å€¼ç¯„åœï¼ˆIDé€šå¸¸æ˜¯è¼ƒå¤§çš„æ•¸å­—ï¼‰
                if numeric_values:
                    avg_value = sum(numeric_values) / len(numeric_values)
                    has_large_values = avg_value > 1000  # IDé€šå¸¸æ˜¯è¼ƒå¤§çš„æ•¸å­—
                else:
                    has_large_values = False

                # æª¢æŸ¥é•·åº¦ä¸€è‡´æ€§ï¼ˆè½‰ç‚ºå­—ç¬¦ä¸²å¾Œï¼‰
                str_lengths = [len(str(int(v))) for v in numeric_values if v == int(v)]
                length_consistency = len(set(str_lengths)) <= 2 if str_lengths else False  # å…è¨±1-2ç¨®é•·åº¦

                # IDåˆ¤æ–·æ¢ä»¶ï¼š
                # 1. æœ‰IDé—œéµå­— + é«˜å”¯ä¸€æ€§
                # 2. æˆ–è€…ï¼šé«˜å”¯ä¸€æ€§ + æ­£æ•´æ•¸ + å¤§æ•¸å€¼ + é•·åº¦ä¸€è‡´
                is_id = (has_id_keyword and unique_ratio > 0.8) or \
                       (unique_ratio > 0.95 and all_positive_integers and has_large_values and length_consistency)

                return is_id

            def _filter_valid_values(values: list) -> list:
                """éæ¿¾æœ‰æ•ˆçš„æ•¸å€¼ï¼Œæ’é™¤NaNå’Œç„¡æ•ˆå€¼"""
                valid_values = []
                for v in values:
                    if v is not None and v != "" and not (isinstance(v, float) and np.isnan(v)):
                        # æ’é™¤å­—ç¬¦ä¸²å½¢å¼çš„nan
                        if isinstance(v, str) and v.lower() in ['nan', 'null', 'none', '']:
                            continue
                        valid_values.append(v)
                return valid_values

            for col in columns:
                values = [item.get(col) for item in data_list if col in item]
                non_null_values = _filter_valid_values(values)

                # å˜—è©¦åˆ¤æ–·æ˜¯å¦ç‚ºæ•¸å€¼åˆ—
                numeric_values = []
                for v in non_null_values:
                    try:
                        num_val = float(v)
                        # æ’é™¤NaNå€¼
                        if not np.isnan(num_val):
                            numeric_values.append(num_val)
                    except (ValueError, TypeError):
                        break

                if len(numeric_values) == len(non_null_values) and numeric_values:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºIDé¡å‹
                    if _is_id_column(col, numeric_values, non_null_values):
                        # IDåˆ—
                        id_columns.append(col)
                        column_stats[col] = {
                            "type": "id",
                            "count": len(numeric_values),
                            "valid_count": len(numeric_values),
                            "unique_count": len(set(numeric_values)),
                            "min": float(np.min(numeric_values)),
                            "max": float(np.max(numeric_values)),
                            "sample_values": [float(v) for v in numeric_values[:5]]
                        }
                    else:
                        # æ•¸å€¼åˆ—
                        numeric_columns.append(col)
                        # ä½¿ç”¨æœ‰æ•ˆæ•¸å€¼è¨ˆç®—çµ±è¨ˆé‡
                        valid_numeric = [v for v in numeric_values if not np.isnan(v)]
                        if valid_numeric:
                            column_stats[col] = {
                                "type": "numeric",
                                "count": len(values),  # ç¸½æ•¸é‡
                                "valid_count": len(valid_numeric),  # æœ‰æ•ˆæ•¸å­—æ•¸é‡
                                "mean": float(np.mean(valid_numeric)),
                                "std": float(np.std(valid_numeric)),
                                "min": float(np.min(valid_numeric)),
                                "max": float(np.max(valid_numeric))
                            }
                        else:
                            column_stats[col] = {
                                "type": "numeric",
                                "count": len(values),
                                "valid_count": 0,
                                "mean": None,
                                "std": None,
                                "min": None,
                                "max": None
                            }
                else:
                    # åˆ†é¡åˆ—
                    categorical_columns.append(col)
                    value_counts = {}
                    # åªçµ±è¨ˆæœ‰æ•ˆå€¼ï¼Œæ’é™¤å„ç¨®å½¢å¼çš„ç©ºå€¼
                    for v in non_null_values:
                        str_v = str(v)
                        # æ’é™¤å„ç¨®å½¢å¼çš„ç©ºå€¼
                        if str_v.lower() not in ['nan', 'null', 'none', '', 'na']:
                            value_counts[str_v] = value_counts.get(str_v, 0) + 1

                    # å–å‰5å€‹æœ€å¸¸è¦‹çš„å€¼
                    top_values = dict(sorted(value_counts.items(), key=lambda x: x[1], reverse=True)[:5])

                    column_stats[col] = {
                        "type": "categorical",
                        "count": len(values),  # ç¸½æ•¸é‡
                        "valid_count": len(non_null_values),  # æœ‰æ•ˆæ•¸æ“šæ•¸é‡
                        "unique_count": len(value_counts),
                        "top_values": top_values
                    }

            info["numeric_columns"] = numeric_columns
            info["categorical_columns"] = categorical_columns
            info["id_columns"] = id_columns
            info["column_stats"] = column_stats

            return info

        except Exception as e:
            logger.error(f"ç²å–æ•¸æ“šä¿¡æ¯å¤±æ•— {file_path}: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def group_by_analysis_multi_file(self, multi_file_data: Dict[str, Any],
                                         group_column: str, value_column: str,
                                         operation: str = "sum", session_id: str = "default") -> Dict[str, Any]:
        """
        å°å¤šæª”æ¡ˆè³‡æ–™é€²è¡Œåˆ†çµ„åˆ†æ

        Args:
            multi_file_data: å¤šæª”æ¡ˆè³‡æ–™
            group_column: åˆ†çµ„åˆ—å
            value_column: æ•¸å€¼åˆ—å
            operation: æ“ä½œé¡å‹
            session_id: æœƒè©±ID

        Returns:
            åˆ†çµ„åˆ†æçµæœ
        """
        try:
            logger.info(f"ğŸ”„ é–‹å§‹å¤šæª”æ¡ˆåˆ†çµ„åˆ†æ: {group_column} by {value_column} ({operation})")

            datasets = multi_file_data.get("datasets", [])
            all_data = []

            # åˆä½µæ‰€æœ‰è³‡æ–™é›†çš„è³‡æ–™
            for dataset in datasets:
                data = dataset.get("data", [])
                for row in data:
                    # æ·»åŠ è³‡æ–™ä¾†æºæ¨™è­˜
                    row_with_source = row.copy()
                    row_with_source["_source"] = dataset.get("source", "unknown")
                    row_with_source["_dataset_date"] = dataset.get("date", "")
                    all_data.append(row_with_source)

            if not all_data:
                return {
                    "success": False,
                    "error": "æ²’æœ‰å¯åˆ†æçš„è³‡æ–™"
                }

            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(all_data)

            # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
            if group_column not in df.columns:
                return {
                    "success": False,
                    "error": f"åˆ†çµ„æ¬„ä½ '{group_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¬„ä½: {list(df.columns)}"
                }

            if value_column not in df.columns:
                return {
                    "success": False,
                    "error": f"æ•¸å€¼æ¬„ä½ '{value_column}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¬„ä½: {list(df.columns)}"
                }

            # åŸ·è¡Œåˆ†çµ„åˆ†æ
            if operation == "count":
                grouped = df.groupby(group_column).size().reset_index(name='count')
                result_data = grouped.to_dict('records')
            else:
                # å˜—è©¦è½‰æ›æ•¸å€¼æ¬„ä½
                try:
                    df[value_column] = pd.to_numeric(df[value_column], errors='coerce')
                except:
                    pass

                if operation == "sum":
                    grouped = df.groupby(group_column)[value_column].sum().reset_index()
                elif operation == "mean":
                    grouped = df.groupby(group_column)[value_column].mean().reset_index()
                elif operation == "max":
                    grouped = df.groupby(group_column)[value_column].max().reset_index()
                elif operation == "min":
                    grouped = df.groupby(group_column)[value_column].min().reset_index()
                else:
                    return {
                        "success": False,
                        "error": f"ä¸æ”¯æ´çš„æ“ä½œ: {operation}"
                    }

                result_data = grouped.to_dict('records')

            # æŒ‰ä¾†æºåˆ†çµ„çš„çµ±è¨ˆ
            source_stats = df.groupby(['_source', group_column]).size().reset_index(name='count')
            source_breakdown = {}
            for _, row in source_stats.iterrows():
                source = row['_source']
                if source not in source_breakdown:
                    source_breakdown[source] = {}
                source_breakdown[source][row[group_column]] = row['count']

            return {
                "success": True,
                "session_id": session_id,
                "analysis_type": f"multi_file_group_by_{operation}",
                "group_column": group_column,
                "value_column": value_column,
                "operation": operation,
                "total_records": len(all_data),
                "total_groups": len(result_data),
                "results": result_data,
                "source_breakdown": source_breakdown,
                "datasets_analyzed": len(datasets)
            }

        except Exception as e:
            logger.error(f"å¤šæª”æ¡ˆåˆ†çµ„åˆ†æå¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }


# å…¨å±€å·¥å…·å¯¦ä¾‹
data_analysis_tools = DataAnalysisTools()


# é€šç”¨åˆ†æå·¥å…·å‡½æ•¸ï¼Œä¾› Agent èª¿ç”¨
async def get_data_info_tool(file_path: str, session_id: str = "default") -> Dict[str, Any]:
    """ç²å–æ•¸æ“šæ–‡ä»¶ä¿¡æ¯çš„å·¥å…·å‡½æ•¸"""
    return await data_analysis_tools.get_data_info(file_path, session_id)


# æ³¨æ„ï¼šgroup_by_analysis_tool å·²åœ¨ langchain_local_file_tools.py ä¸­å®šç¾©
# é€™è£¡ç§»é™¤é‡è¤‡å®šç¾©ä»¥é¿å…è¡çª


async def threshold_analysis_tool(file_path: str, value_column: str, threshold: float,
                                 comparison: str = "greater", session_id: str = "default") -> Dict[str, Any]:
    """é–¾å€¼åˆ†æå·¥å…·å‡½æ•¸"""
    return await data_analysis_tools.threshold_analysis(file_path, value_column, threshold, comparison, session_id)


async def correlation_analysis_tool(file_path: str, x_column: str, y_column: str,
                                   session_id: str = "default") -> Dict[str, Any]:
    """ç›¸é—œæ€§åˆ†æå·¥å…·å‡½æ•¸"""
    return await data_analysis_tools.correlation_analysis(file_path, x_column, y_column, session_id)


async def linear_prediction_tool(file_path: str, x_column: str, y_column: str,
                                target_x_value: float, session_id: str = "default") -> Dict[str, Any]:
    """ç·šæ€§é æ¸¬å·¥å…·å‡½æ•¸"""
    return await data_analysis_tools.linear_prediction(file_path, x_column, y_column, target_x_value, session_id)
