"""
Gmail æ‰¹é‡æŠ“å–å·¥å…·
å¯¦ç¾ç•°æ­¥æ‰¹é‡æŠ“å– Gmail éƒµä»¶æ•¸æ“š
"""

import asyncio
import aiohttp
import json
import pandas as pd
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class GmailBatchFetcher:
    """Gmail æ‰¹é‡æŠ“å–å™¨"""
    
    def __init__(self):
        self.session = None
    
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def fetch_gmail_batch(
        self, 
        access_token: str, 
        email_address: str,
        total_emails: int = 1000,
        batch_size: int = 50,
        concurrent_batches: int = 20,
        session_id: str = "default"
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡æŠ“å– Gmail éƒµä»¶
        
        Args:
            access_token: OAuth access token
            email_address: ç”¨æˆ¶ email åœ°å€
            total_emails: ç¸½å…±è¦æŠ“å–çš„éƒµä»¶æ•¸é‡
            batch_size: æ¯æ‰¹æŠ“å–çš„éƒµä»¶æ•¸é‡
            concurrent_batches: ä¸¦ç™¼æ‰¹æ¬¡æ•¸é‡
            session_id: æœƒè©± ID
            
        Returns:
            åŒ…å«æŠ“å–çµæœå’Œ CSV æ–‡ä»¶è·¯å¾‘çš„å­—å…¸
        """
        try:
            logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡æŠ“å– Gmail éƒµä»¶: {email_address}")
            logger.info(f"ğŸ“Š åƒæ•¸: total={total_emails}, batch_size={batch_size}, concurrent={concurrent_batches}")
            
            # æ­¥é©Ÿ1: ç²å–éƒµä»¶ ID åˆ—è¡¨
            message_ids = await self._get_message_ids(access_token, total_emails)
            if not message_ids:
                return {
                    "success": False,
                    "error": "ç„¡æ³•ç²å–éƒµä»¶ ID åˆ—è¡¨"
                }
            
            actual_total = min(len(message_ids), total_emails)
            logger.info(f"ğŸ“§ ç²å–åˆ° {len(message_ids)} å€‹éƒµä»¶ IDï¼Œå°‡æŠ“å– {actual_total} å°")
            
            # æ­¥é©Ÿ2: åˆ†æ‰¹è™•ç†
            batches = []
            for i in range(0, actual_total, batch_size):
                batch_ids = message_ids[i:i + batch_size]
                batches.append(batch_ids)
            
            logger.info(f"ğŸ“¦ åˆ†æˆ {len(batches)} æ‰¹ï¼Œæ¯æ‰¹æœ€å¤š {batch_size} å°")
            
            # æ­¥é©Ÿ3: ç•°æ­¥ä¸¦ç™¼æŠ“å–
            semaphore = asyncio.Semaphore(concurrent_batches)
            tasks = []
            
            for batch_index, batch_ids in enumerate(batches):
                task = self._fetch_batch_with_semaphore(
                    semaphore, access_token, batch_ids, batch_index
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ­¥é©Ÿ4: æ•´åˆçµæœ
            all_emails = []
            successful_batches = 0
            failed_batches = 0
            
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ æ‰¹æ¬¡ {i} å¤±æ•—: {result}")
                    failed_batches += 1
                elif result and result.get("success"):
                    all_emails.extend(result.get("emails", []))
                    successful_batches += 1
                else:
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i} è¿”å›ç©ºçµæœ")
                    failed_batches += 1
            
            logger.info(f"âœ… æ‰¹é‡æŠ“å–å®Œæˆ: æˆåŠŸ {successful_batches} æ‰¹ï¼Œå¤±æ•— {failed_batches} æ‰¹")
            logger.info(f"ğŸ“§ ç¸½å…±ç²å– {len(all_emails)} å°éƒµä»¶")
            
            # æ­¥é©Ÿ5: ä¿å­˜ç‚º CSV
            csv_path = await self._save_emails_to_csv(all_emails, email_address, session_id)
            
            # æ­¥é©Ÿ6: ç”Ÿæˆæ–‡ä»¶æ‘˜è¦
            file_summary = await self._generate_file_summary(all_emails, csv_path)
            
            return {
                "success": True,
                "total_emails": len(all_emails),
                "csv_path": csv_path,
                "file_summary": file_summary,
                "email_address": email_address,
                "successful_batches": successful_batches,
                "failed_batches": failed_batches
            }
            
        except Exception as e:
            logger.error(f"âŒ Gmail æ‰¹é‡æŠ“å–å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _get_message_ids(self, access_token: str, max_results: int) -> List[str]:
        """ç²å–éƒµä»¶ ID åˆ—è¡¨"""
        try:
            url = "https://gmail.googleapis.com/gmail/v1/users/me/messages"
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            params = {
                "maxResults": min(max_results, 500),  # Gmail API å–®æ¬¡æœ€å¤š 500
                "q": "category:primary"  # åªç²å–ä¸»è¦å€åŸŸ
            }
            
            message_ids = []
            next_page_token = None
            
            while len(message_ids) < max_results:
                if next_page_token:
                    params["pageToken"] = next_page_token
                
                async with self.session.get(url, headers=headers, params=params) as response:
                    if response.status != 200:
                        logger.error(f"âŒ ç²å–éƒµä»¶åˆ—è¡¨å¤±æ•—: {response.status}")
                        break
                    
                    data = await response.json()
                    messages = data.get("messages", [])
                    
                    for msg in messages:
                        if len(message_ids) >= max_results:
                            break
                        message_ids.append(msg["id"])
                    
                    next_page_token = data.get("nextPageToken")
                    if not next_page_token:
                        break
            
            return message_ids
            
        except Exception as e:
            logger.error(f"âŒ ç²å–éƒµä»¶ ID åˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    async def _fetch_batch_with_semaphore(
        self, 
        semaphore: asyncio.Semaphore, 
        access_token: str, 
        batch_ids: List[str], 
        batch_index: int
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼çš„æ‰¹æ¬¡æŠ“å–"""
        async with semaphore:
            return await self._fetch_email_batch(access_token, batch_ids, batch_index)
    
    async def _fetch_email_batch(
        self, 
        access_token: str, 
        batch_ids: List[str], 
        batch_index: int
    ) -> Dict[str, Any]:
        """æŠ“å–ä¸€æ‰¹éƒµä»¶çš„è©³ç´°ä¿¡æ¯"""
        try:
            logger.info(f"ğŸ“¦ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_index}: {len(batch_ids)} å°éƒµä»¶")
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            emails = []
            
            # ä¸¦ç™¼ç²å–æ¯å°éƒµä»¶çš„è©³ç´°ä¿¡æ¯
            tasks = []
            for msg_id in batch_ids:
                task = self._fetch_single_email(headers, msg_id)
                tasks.append(task)
            
            email_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in email_results:
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸ ç²å–å–®å°éƒµä»¶å¤±æ•—: {result}")
                elif result:
                    emails.append(result)
            
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} å®Œæˆ: æˆåŠŸç²å– {len(emails)} å°éƒµä»¶")
            
            return {
                "success": True,
                "emails": emails,
                "batch_index": batch_index
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_index} è™•ç†å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "batch_index": batch_index
            }
    
    async def _fetch_single_email(self, headers: Dict[str, str], msg_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–å–®å°éƒµä»¶çš„è©³ç´°ä¿¡æ¯"""
        try:
            url = f"https://gmail.googleapis.com/gmail/v1/users/me/messages/{msg_id}"
            params = {
                "format": "metadata",
                "metadataHeaders": ["Subject", "From", "Date", "To"]
            }
            
            async with self.session.get(url, headers=headers, params=params) as response:
                if response.status != 200:
                    return None
                
                data = await response.json()
                headers_list = data.get("payload", {}).get("headers", [])
                
                # æå–éƒµä»¶ä¿¡æ¯
                subject = ""
                sender = ""
                date = ""
                recipient = ""
                
                for header in headers_list:
                    name = header.get("name", "").lower()
                    value = header.get("value", "")
                    
                    if name == "subject":
                        subject = value
                    elif name == "from":
                        sender = value
                    elif name == "date":
                        date = value
                    elif name == "to":
                        recipient = value
                
                return {
                    "id": data.get("id"),
                    "thread_id": data.get("threadId"),
                    "subject": subject[:200] if subject else "ç„¡ä¸»æ—¨",
                    "sender": sender,
                    "recipient": recipient,
                    "date": date,
                    "snippet": data.get("snippet", ""),
                    "is_read": "UNREAD" not in data.get("labelIds", []),
                    "labels": ",".join(data.get("labelIds", [])),
                    "url": f"https://mail.google.com/mail/u/0/#all/{data.get('id')}"
                }
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç²å–éƒµä»¶ {msg_id} å¤±æ•—: {e}")
            return None
    
    async def _save_emails_to_csv(
        self, 
        emails: List[Dict[str, Any]], 
        email_address: str, 
        session_id: str
    ) -> str:
        """å°‡éƒµä»¶æ•¸æ“šä¿å­˜ç‚º CSV æ–‡ä»¶"""
        try:
            # å‰µå»ºæœƒè©±ç´šæš«å­˜ç›®éŒ„
            temp_dir = Path(tempfile.gettempdir()) / "agent_sessions" / session_id
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆ CSV æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"gmail_emails_{email_address.replace('@', '_')}_{timestamp}.csv"
            csv_path = temp_dir / csv_filename
            
            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(emails)
            
            # ä¿å­˜ç‚º CSV
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"ğŸ’¾ Gmail æ•¸æ“šå·²ä¿å­˜åˆ°: {csv_path}")
            logger.info(f"ğŸ“Š CSV æ–‡ä»¶åŒ…å« {len(emails)} è¡Œæ•¸æ“š")
            
            return str(csv_path)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ CSV æ–‡ä»¶å¤±æ•—: {e}")
            raise
    
    async def _generate_file_summary(
        self, 
        emails: List[Dict[str, Any]], 
        csv_path: str
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡ä»¶æ‘˜è¦"""
        try:
            if not emails:
                return {
                    "file_type": "gmail_csv",
                    "total_emails": 0,
                    "summary": "ç©ºçš„ Gmail æ•¸æ“šæ–‡ä»¶"
                }
            
            # çµ±è¨ˆä¿¡æ¯
            total_emails = len(emails)
            unread_count = sum(1 for email in emails if not email.get("is_read", True))
            
            # çµ±è¨ˆç™¼ä»¶äºº
            senders = {}
            for email in emails:
                sender = email.get("sender", "æœªçŸ¥ç™¼ä»¶äºº")
                senders[sender] = senders.get(sender, 0) + 1
            
            top_senders = sorted(senders.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # æ™‚é–“ç¯„åœ
            dates = [email.get("date", "") for email in emails if email.get("date")]
            date_range = f"æœ€æ–° {total_emails} å°éƒµä»¶" if dates else "ç„¡æ—¥æœŸä¿¡æ¯"
            
            summary = {
                "file_type": "gmail_csv",
                "file_path": csv_path,
                "total_emails": total_emails,
                "unread_emails": unread_count,
                "read_emails": total_emails - unread_count,
                "top_senders": top_senders,
                "date_range": date_range,
                "columns": ["id", "thread_id", "subject", "sender", "recipient", "date", "snippet", "is_read", "labels", "url"],
                "summary": f"Gmail éƒµä»¶æ•¸æ“šï¼Œå…± {total_emails} å°éƒµä»¶ï¼Œå…¶ä¸­ {unread_count} å°æœªè®€"
            }
            
            logger.info(f"ğŸ“‹ æ–‡ä»¶æ‘˜è¦å·²ç”Ÿæˆ: {summary['summary']}")
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ–‡ä»¶æ‘˜è¦å¤±æ•—: {e}")
            return {
                "file_type": "gmail_csv",
                "total_emails": len(emails),
                "summary": f"Gmail æ•¸æ“šæ–‡ä»¶ï¼ŒåŒ…å« {len(emails)} å°éƒµä»¶",
                "error": str(e)
            }


# å…¨å±€å¯¦ä¾‹
gmail_fetcher = GmailBatchFetcher()


async def fetch_gmail_emails_batch(
    access_token: str,
    email_address: str,
    total_emails: int = 1000,
    session_id: str = "default"
) -> Dict[str, Any]:
    """
    æ‰¹é‡æŠ“å– Gmail éƒµä»¶çš„ä¾¿åˆ©å‡½æ•¸
    
    Args:
        access_token: OAuth access token
        email_address: ç”¨æˆ¶ email åœ°å€
        total_emails: è¦æŠ“å–çš„éƒµä»¶ç¸½æ•¸
        session_id: æœƒè©± ID
        
    Returns:
        æŠ“å–çµæœå­—å…¸
    """
    async with GmailBatchFetcher() as fetcher:
        return await fetcher.fetch_gmail_batch(
            access_token=access_token,
            email_address=email_address,
            total_emails=total_emails,
            batch_size=50,
            concurrent_batches=20,
            session_id=session_id
        )
