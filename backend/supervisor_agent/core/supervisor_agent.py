"""
Supervisor Agent - Gmail è‡ªå‹•åŒ–è™•ç†ç›£ç£è€…
ä½¿ç”¨ LangGraph æ¶æ§‹ï¼Œæ•´åˆå¤šå€‹å°ˆæ¥­å·¥å…·ï¼Œæä¾› Gmail è‡ªå‹•åŒ–è™•ç†åŠŸèƒ½
åƒè€ƒ example/gov_agent.py çš„æ¶æ§‹å¯¦ç¾
"""

from __future__ import annotations
import logging
import asyncio
import json
import time
import uuid
import os
import sys
from typing import List, Optional, Dict, Any, Annotated, Literal
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.prebuilt.tool_node import ToolNode as BaseToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import AzureChatOpenAI
import tiktoken

# å·¥å…·å°‡åœ¨æŸ¥è©¢æ™‚å‹•æ…‹å°å…¥

from ..utils.logger import get_logger

logger = get_logger(__name__)

# ----------------------- State Definition ----------------------- #

class SupervisorAgentState(TypedDict):
    """Supervisor Agent çš„ç‹€æ…‹å®šç¾©"""
    messages: Annotated[list, add_messages]
    query: str
    rule_id: Optional[str]
    context: Optional[Dict[str, Any]]

# ----------------------- å¹³è¡Œå·¥å…·åŸ·è¡Œç¯€é» ----------------------- #

class ParallelToolNode(BaseToolNode):
    """å¹³è¡ŒåŸ·è¡Œå·¥å…·çš„è‡ªå®šç¾© ToolNode"""

    def __init__(self, tools: List, stream_callback=None):
        super().__init__(tools)
        self.tools_by_name = {tool.name: tool for tool in tools}
        self.stream_callback = stream_callback  # æ·»åŠ streamå›èª¿å‡½æ•¸

    async def _execute_single_tool_with_message(self, tool, tool_args, tool_call_id, tool_name):
        """åŸ·è¡Œå–®å€‹å·¥å…·ä¸¦è¿”å› ToolMessage"""
        try:
            # è¨˜éŒ„å·¥å…·èª¿ç”¨åƒæ•¸
            logger.info(f"ğŸ”§ åŸ·è¡Œå·¥å…·: {tool_name}")
            logger.info(f"ğŸ“‹ å·¥å…·åƒæ•¸: {tool_args}")
            start_time = time.time()

            # åŸ·è¡Œå·¥å…·
            if asyncio.iscoroutinefunction(tool.func):
                result = await tool.func(**tool_args)
            else:
                result = tool.func(**tool_args)

            execution_time = time.time() - start_time
            logger.info(f"âœ… å·¥å…· {tool_name} åŸ·è¡Œå®Œæˆï¼Œè€—æ™‚ {execution_time:.2f}ç§’")

            # è¨˜éŒ„å·¥å…·åŸ·è¡Œçµæœï¼ˆå‰300å­—ç¬¦ï¼‰
            result_str = str(result)
            logger.info(f"ğŸ“¤ å·¥å…· {tool_name} åŸ·è¡Œçµæœå‰300å­—ç¬¦: {result_str[:300]}")

            # åŒ…è£å·¥å…·çµæœï¼Œæ·»åŠ  tool æ¨™ç±¤
            wrapped_result = f"<tool name='{tool_name}' execution_time='{execution_time:.2f}s'>\n{result_str}\n</tool>"

            # å¦‚æœæœ‰streamå›èª¿ï¼Œå¯¦æ™‚ç™¼é€å·¥å…·åŸ·è¡Œçµæœ
            if self.stream_callback:
                await self.stream_callback({
                    'type': 'tool_result',
                    'tool_name': tool_name,
                    'parameters': tool_args,
                    'result': result_str,
                    'execution_time': execution_time,
                    'wrapped_result': wrapped_result
                })

            # å‰µå»º ToolMessage
            return ToolMessage(
                content=wrapped_result,
                tool_call_id=tool_call_id,
                name=tool_name
            )

        except Exception as e:
            logger.error(f"âŒ å·¥å…· {tool_name} åŸ·è¡Œå¤±æ•—: {e}")
            error_result = f"<tool name='{tool_name}' status='error'>\nå·¥å…·åŸ·è¡Œå¤±æ•—: {str(e)}\n</tool>"
            return ToolMessage(
                content=error_result,
                tool_call_id=tool_call_id,
                name=tool_name
            )

    async def __call__(self, state: SupervisorAgentState) -> Dict[str, Any]:
        """å¹³è¡ŒåŸ·è¡Œæ‰€æœ‰å·¥å…·èª¿ç”¨"""
        messages = state.get("messages", [])

        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ AI æ¶ˆæ¯ä¸­çš„å·¥å…·èª¿ç”¨
        tool_calls = []
        for message in reversed(messages):
            if isinstance(message, AIMessage) and hasattr(message, "tool_calls") and message.tool_calls:
                tool_calls = message.tool_calls
                break

        if not tool_calls:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°å·¥å…·èª¿ç”¨")
            return {"messages": []}

        logger.info(f"ğŸš€ å¹³è¡ŒåŸ·è¡Œ {len(tool_calls)} å€‹å·¥å…·")

        # æº–å‚™å¹³è¡ŒåŸ·è¡Œçš„ä»»å‹™
        tasks = []
        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call.get("args", {})
            tool_call_id = tool_call.get("id", "")

            if tool_name in self.tools_by_name:
                tool = self.tools_by_name[tool_name]

                # å‰µå»ºç•°æ­¥ä»»å‹™
                task = self._execute_single_tool_with_message(
                    tool, tool_args, tool_call_id, tool_name
                )
                tasks.append(task)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥å·¥å…·: {tool_name}")

        if not tasks:
            return {"messages": []}

        # å¹³è¡ŒåŸ·è¡Œæ‰€æœ‰å·¥å…·
        try:
            tool_messages = await asyncio.gather(*tasks, return_exceptions=True)

            # è™•ç†çµæœ
            valid_messages = []
            for msg in tool_messages:
                if isinstance(msg, ToolMessage):
                    valid_messages.append(msg)
                elif isinstance(msg, Exception):
                    logger.error(f"âŒ å·¥å…·åŸ·è¡Œç•°å¸¸: {msg}")

            logger.info(f"âœ… å¹³è¡Œå·¥å…·åŸ·è¡Œå®Œæˆï¼ŒæˆåŠŸ {len(valid_messages)} å€‹")
            return {"messages": valid_messages}

        except Exception as e:
            logger.error(f"âŒ å¹³è¡Œå·¥å…·åŸ·è¡Œå¤±æ•—: {e}")
            return {"messages": []}

# ----------------------- Supervisor Agent ----------------------- #

class SupervisorAgent:
    """Gmail è‡ªå‹•åŒ–è™•ç†ç›£ç£è€… Agent"""

    def __init__(self, rules_dir: str = "data/rules", stream_callback=None):
        logger.info("ğŸ”„ é–‹å§‹åˆå§‹åŒ– Supervisor Agent...")
        init_start = time.time()

        # è¨­ç½®è¦å‰‡ç›®éŒ„
        self.rules_dir = rules_dir
        # è¨­ç½®streamå›èª¿å‡½æ•¸
        self.stream_callback = stream_callback

        # åˆå§‹åŒ– LLM
        self.llm = AzureChatOpenAI(
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment="gpt-4o",
            api_version="2025-01-01-preview",
            temperature=0.7,
        )

        logger.info("âœ… LLM åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–Tokenè¨ˆç®—å™¨
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4")
        except:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # ç•¶å‰æœƒè©±çš„å·¥å…·ï¼ˆå‹•æ…‹è¨­ç½®ï¼‰
        self.current_tools = []
        self.current_llm_with_tools = None
        self.current_graph = None

        init_time = time.time() - init_start
        logger.info(f"âœ… Supervisor Agent åˆå§‹åŒ–å®Œæˆï¼Œè€—æ™‚ {init_time:.2f}ç§’")

    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®—æ–‡æœ¬çš„tokenæ•¸é‡"""
        try:
            return len(self.tokenizer.encode(text))
        except Exception as e:
            logger.warning(f"Tokenè¨ˆç®—å¤±æ•—: {e}")
            # ç°¡å–®ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦
            return len(text) // 4

    def calculate_messages_tokens(self, messages: List) -> int:
        """è¨ˆç®—æ¶ˆæ¯åˆ—è¡¨çš„ç¸½tokenæ•¸"""
        total_tokens = 0
        for msg in messages:
            if hasattr(msg, 'content'):
                total_tokens += self.calculate_tokens(str(msg.content))
            else:
                total_tokens += self.calculate_tokens(str(msg))
        return total_tokens

    def manage_context_for_batch_processing(self, messages: List, context: Dict[str, Any]) -> List:
        """ç‚ºbatch processingç®¡ç†ä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™é€²åº¦ä¿¡æ¯"""
        is_batch_mode = context.get("is_batch_processing", False)

        if not is_batch_mode:
            return messages  # ébatchæ¨¡å¼ï¼Œä¿æŒåŸæœ‰é‚è¼¯

        # Batchæ¨¡å¼ï¼šåªä¿ç•™æœ€è¿‘çš„é‡è¦æ¶ˆæ¯å’Œé€²åº¦ä¿¡æ¯
        important_messages = []
        tool_call_count = 0

        for msg in messages:
            if isinstance(msg, (HumanMessage, SystemMessage)):
                # ä¿ç•™ç”¨æˆ¶æ¶ˆæ¯å’Œç³»çµ±æ¶ˆæ¯
                important_messages.append(msg)
            elif isinstance(msg, AIMessage):
                # ä¿ç•™AIæ¶ˆæ¯ï¼Œä½†ç°¡åŒ–å…§å®¹
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    tool_call_count += len(msg.tool_calls)
                important_messages.append(msg)
            elif isinstance(msg, ToolMessage):
                # å·¥å…·æ¶ˆæ¯åªä¿ç•™é€²åº¦ä¿¡æ¯ï¼Œä¸ä¿ç•™è©³ç´°çµæœ
                tool_call_count += 1

                # æª¢æŸ¥æ˜¯å¦æ˜¯é€²åº¦ç›¸é—œçš„å·¥å…·çµæœ
                content = str(msg.content)
                if any(keyword in content.lower() for keyword in ['é€²åº¦', 'progress', 'å®Œæˆ', 'ä»»å‹™', 'task']):
                    # ä¿ç•™é€²åº¦ä¿¡æ¯
                    important_messages.append(msg)
                else:
                    # ç°¡åŒ–å·¥å…·çµæœ
                    simplified_content = f"å·¥å…· {msg.name} åŸ·è¡Œå®Œæˆ (ç¬¬{tool_call_count}æ¬¡èª¿ç”¨)"
                    simplified_msg = ToolMessage(
                        content=simplified_content,
                        tool_call_id=msg.tool_call_id,
                        name=msg.name
                    )
                    important_messages.append(simplified_msg)

        # è¨˜éŒ„tokenç¯€çœæƒ…æ³
        original_tokens = self.calculate_messages_tokens(messages)
        managed_tokens = self.calculate_messages_tokens(important_messages)
        logger.info(f"ğŸ§  Batchæ¨¡å¼Tokenç®¡ç†: {original_tokens} â†’ {managed_tokens} (ç¯€çœ {original_tokens - managed_tokens})")

        return important_messages

    def compress_tool_messages(self, messages: List, max_tool_results: int = 3) -> List:
        """
        å£“ç¸®å·¥å…·æ¶ˆæ¯ï¼Œåªä¿ç•™æœ€è¿‘çš„å¹¾å€‹å·¥å…·çµæœ

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tool_results: æœ€å¤§ä¿ç•™çš„å·¥å…·çµæœæ•¸é‡

        Returns:
            å£“ç¸®å¾Œçš„æ¶ˆæ¯åˆ—è¡¨
        """
        compressed_messages = []
        tool_message_count = 0

        # å¾å¾Œå¾€å‰éæ­·ï¼Œä¿ç•™æœ€è¿‘çš„å·¥å…·çµæœ
        for msg in reversed(messages):
            if isinstance(msg, ToolMessage):
                if tool_message_count < max_tool_results:
                    # å£“ç¸®å·¥å…·çµæœå…§å®¹
                    content = str(msg.content)
                    if len(content) > 500:  # å¦‚æœå…§å®¹å¤ªé•·ï¼Œæˆªæ–·
                        try:
                            import json
                            parsed = json.loads(content)
                            if isinstance(parsed, dict):
                                # ä¿ç•™é—œéµä¿¡æ¯ï¼Œç§»é™¤å¤§æ•¸æ“šå­—æ®µ
                                compressed_parsed = {
                                    "success": parsed.get("success", True),
                                    "message": parsed.get("message", ""),
                                    "summary": f"å·¥å…·åŸ·è¡Œçµæœå·²å£“ç¸® (åŸé•·åº¦: {len(content)} å­—ç¬¦)"
                                }
                                # ä¿ç•™é—œéµçµ±è¨ˆä¿¡æ¯
                                for key in ["total_rows", "filtered_rows", "analysis_type", "results_count"]:
                                    if key in parsed:
                                        compressed_parsed[key] = parsed[key]

                                # ä¿ç•™é‡è¦çš„å·¥ä½œé€²åº¦ä¿¡æ¯
                                important_keys = [
                                    "temp_file_path", "temp_file_created", "current_data_updated",
                                    "operation", "file_path", "columns", "results"
                                ]
                                for key in important_keys:
                                    if key in parsed:
                                        if key == "results" and isinstance(parsed[key], dict):
                                            # ä¿ç•™çµæœæ‘˜è¦ï¼Œä¸ä¿ç•™è©³ç´°æ•¸æ“š
                                            compressed_parsed[key + "_summary"] = {
                                                k: v for k, v in parsed[key].items()
                                                if not isinstance(v, (list, dict)) or k in ["count", "mean", "sum"]
                                            }
                                        else:
                                            compressed_parsed[key] = parsed[key]

                                content = json.dumps(compressed_parsed, ensure_ascii=False)
                        except:
                            # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æˆªæ–·
                            content = content[:500] + "... (å…§å®¹å·²æˆªæ–·)"

                    compressed_msg = ToolMessage(
                        content=content,
                        tool_call_id=msg.tool_call_id,
                        name=msg.name
                    )
                    compressed_messages.insert(0, compressed_msg)
                    tool_message_count += 1
                else:
                    # è¶…éé™åˆ¶çš„å·¥å…·æ¶ˆæ¯ç”¨æ‘˜è¦æ›¿ä»£
                    summary_msg = ToolMessage(
                        content=f"å·¥å…· {msg.name} åŸ·è¡Œå®Œæˆ (çµæœå·²çœç•¥)",
                        tool_call_id=msg.tool_call_id,
                        name=msg.name
                    )
                    compressed_messages.insert(0, summary_msg)
            else:
                compressed_messages.insert(0, msg)

        return compressed_messages

    def setup_tools_for_query(self, tool_names: List[str] = None, available_tools: List = None):
        """ç‚ºç•¶å‰æŸ¥è©¢å‹•æ…‹è¨­ç½®å·¥å…·"""
        logger.info(f"ğŸ”§ é–‹å§‹å‹•æ…‹è¨­ç½®å·¥å…·ï¼Œè¦å‰‡å·¥å…·: {tool_names}")

        # é–‹å§‹è¨­ç½®å·¥å…·
        self.current_tools = []

        # å¦‚æœæœ‰å¤–éƒ¨æä¾›çš„å·¥å…·åˆ—è¡¨ï¼Œå„ªå…ˆä½¿ç”¨
        if available_tools:
            self.current_tools = available_tools
            logger.info(f"ğŸ“ ä½¿ç”¨å¤–éƒ¨æä¾›çš„å·¥å…·ï¼Œå…± {len(available_tools)} å€‹")
            for tool in available_tools:
                tool_name = getattr(tool, 'name', str(tool))
                logger.info(f"ğŸ”§ æ·»åŠ å·¥å…·: {tool_name}")
        else:
            # å¦å‰‡ä½¿ç”¨é»˜èªç€è¦½å™¨å·¥å…·ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            try:
                from ..tools.langchain_browser_tools import get_langchain_browser_tools
                browser_tools = get_langchain_browser_tools()

                for tool in browser_tools:
                    self.current_tools.append(tool)
                    logger.info(f"ğŸŒ æ·»åŠ é»˜èªç€è¦½å™¨å·¥å…·: {tool.name}")

            except Exception as e:
                logger.warning(f"âš ï¸ ç€è¦½å™¨å·¥å…·å°å…¥å¤±æ•—: {e}")

        # æ ¹æ“šè¦å‰‡æ·»åŠ é¡å¤–å·¥å…·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if tool_names:
            logger.info(f"ğŸ“‹ è¦å‰‡æŒ‡å®šçš„å·¥å…·: {tool_names}")
            # é€™è£¡å¯ä»¥æ ¹æ“š tool_names æ·»åŠ é¡å¤–çš„å·¥å…·

        # ç¶å®šå·¥å…·åˆ° LLM
        if self.current_tools:
            self.current_llm_with_tools = self.llm.bind_tools(self.current_tools)
            logger.info(f"ğŸ”§ å·¥å…·ç¶å®šå®Œæˆï¼Œå…± {len(self.current_tools)} å€‹å·¥å…·")
        else:
            self.current_llm_with_tools = self.llm
            logger.info("ğŸ”§ ç„¡å·¥å…·æ¨¡å¼ï¼Œä½¿ç”¨ç´”LLM")

        # é‡æ–°å»ºç«‹ graph
        self._build_graph()

    def _build_graph(self):
        """å»ºç«‹ LangGraph workflow - å¾ªç’°æ±ºç­–æ¶æ§‹"""
        # åˆå§‹åŒ– StateGraph
        workflow = StateGraph(SupervisorAgentState)

        # å‰µå»ºè‡ªå®šç¾©çš„å¹³è¡Œ ToolNode ä¾†è™•ç†å·¥å…·èª¿ç”¨
        if self.current_tools:
            tool_node = ParallelToolNode(self.current_tools, self.stream_callback)
        else:
            # æ²’æœ‰å·¥å…·æ™‚å‰µå»ºä¸€å€‹ç©ºçš„å·¥å…·ç¯€é»
            tool_node = ParallelToolNode([], self.stream_callback)

        # æ·»åŠ ç¯€é»
        workflow.add_node("supervisor", self.supervisor_node)  # ä¸­å¤®æ±ºç­–ç¯€é»
        workflow.add_node("tools", tool_node)  # å·¥å…·åŸ·è¡Œç¯€é»ï¼ˆä½¿ç”¨å¹³è¡ŒåŸ·è¡Œï¼‰
        workflow.add_node("response_generator", self.response_generator_node)  # æœ€çµ‚å›ç­”ç”Ÿæˆç¯€é»

        # è¨­å®šæµç¨‹ - å¾supervisorç¯€é»é–‹å§‹
        workflow.add_edge(START, "supervisor")

        # supervisorç¯€é»å¾Œçš„æ¢ä»¶åˆ†æ”¯
        workflow.add_conditional_edges(
            "supervisor",
            self.should_continue,  # è‡ªå®šç¾©æ¢ä»¶å‡½æ•¸
            {
                "tools": "tools",           # éœ€è¦èª¿ç”¨å·¥å…·
                "respond": "response_generator",  # ç›´æ¥å›ç­”
                "__end__": END,             # çµæŸ
            },
        )

        # å·¥å…·åŸ·è¡Œå¾Œå›åˆ°supervisorç¯€é»é‡æ–°è©•ä¼°
        workflow.add_edge("tools", "supervisor")

        # å›ç­”ç”Ÿæˆå¾ŒçµæŸ
        workflow.add_edge("response_generator", END)

        # ç·¨è­¯ graph with çŸ­æœŸè¨˜æ†¶
        self.current_graph = workflow.compile(checkpointer=MemorySaver())

        logger.info(f"âœ… Supervisor Agent Graph å»ºç«‹å®Œæˆï¼Œå·¥å…·æ•¸é‡: {len(self.current_tools)}")

    def should_continue(self, state: SupervisorAgentState) -> str:
        """æ±ºå®šä¸‹ä¸€æ­¥å‹•ä½œçš„æ¢ä»¶å‡½æ•¸"""
        messages = state.get("messages", [])
        context = state.get("context", {})

        if not messages:
            return "respond"

        last_message = messages[-1]

        # å¦‚æœæœ€å¾Œä¸€å€‹æ¶ˆæ¯æ˜¯AIæ¶ˆæ¯ä¸”æœ‰å·¥å…·èª¿ç”¨ï¼ŒåŸ·è¡Œå·¥å…·
        if isinstance(last_message, AIMessage) and hasattr(last_message, "tool_calls") and last_message.tool_calls:
            logger.info(f"ğŸ”§ supervisoræ±ºå®šèª¿ç”¨å·¥å…·: {len(last_message.tool_calls)} å€‹")
            return "tools"

        # æª¢æŸ¥æ˜¯å¦æ˜¯batch processingæ¨¡å¼
        is_batch_mode = context.get("is_batch_processing", False)

        # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§å·¥å…·èª¿ç”¨æ¬¡æ•¸ï¼ˆé˜²æ­¢ç„¡é™å¾ªç’°ï¼‰
        tool_messages = [msg for msg in messages if isinstance(msg, ToolMessage)]
        max_tools = 50 if is_batch_mode else 10  # batchæ¨¡å¼å…è¨±æ›´å¤šå·¥å…·èª¿ç”¨

        if len(tool_messages) >= max_tools:
            logger.info(f"ğŸ›‘ é”åˆ°æœ€å¤§å·¥å…·èª¿ç”¨æ¬¡æ•¸({max_tools})ï¼Œå¼·åˆ¶ç”Ÿæˆå›ç­”")
            return "respond"

        # å¦‚æœæœ€å¾Œä¸€å€‹æ¶ˆæ¯æ˜¯AIæ¶ˆæ¯ä½†æ²’æœ‰å·¥å…·èª¿ç”¨ï¼Œç”Ÿæˆæœ€çµ‚å›ç­”
        if isinstance(last_message, AIMessage):
            logger.info("ï¿½ supervisoræ±ºå®šç”Ÿæˆæœ€çµ‚å›ç­”")
            return "respond"

        # å…¶ä»–æƒ…æ³ï¼ˆå¦‚ToolMessageï¼‰ï¼Œèªªæ˜éœ€è¦å›åˆ°supervisoré‡æ–°è©•ä¼°
        # ä½†é€™å€‹é‚è¼¯å·²ç¶“åœ¨graphä¸­è™•ç†äº†ï¼ˆtools -> supervisorï¼‰
        logger.info("ğŸ”„ å…¶ä»–æƒ…æ³ï¼Œç”Ÿæˆå›ç­”")
        return "respond"

    async def supervisor_node(self, state: SupervisorAgentState) -> Dict[str, Any]:
        """ä¸­å¤®æ±ºç­–ç¯€é» - åˆ†æç•¶å‰ç‹€æ…‹ä¸¦æ±ºå®šä¸‹ä¸€æ­¥å‹•ä½œ"""
        query = state.get("query", "")
        messages = state.get("messages", [])
        rule_id = state.get("rule_id")
        context = state.get("context", {})

        # è¨ˆç®—ç•¶å‰tokenä½¿ç”¨é‡
        current_tokens = self.calculate_messages_tokens(messages)
        logger.info(f"ğŸ“Š ç•¶å‰ä¸Šä¸‹æ–‡Tokenæ•¸: {current_tokens}")

        # æ™ºèƒ½è¨˜æ†¶ç®¡ç†
        if current_tokens > 8000:  # å¦‚æœtokenæ•¸é‡éå¤šï¼Œé€²è¡Œå£“ç¸®
            logger.info(f"ğŸ§  Tokenæ•¸é‡éå¤š ({current_tokens})ï¼Œé–‹å§‹è¨˜æ†¶å£“ç¸®")
            messages = self.compress_tool_messages(messages, max_tool_results=3)
            compressed_tokens = self.calculate_messages_tokens(messages)
            logger.info(f"ğŸ§  è¨˜æ†¶å£“ç¸®å®Œæˆ: {current_tokens} â†’ {compressed_tokens} (ç¯€çœ {current_tokens - compressed_tokens})")
            state["messages"] = messages

            # å£“ç¸®å¾Œï¼Œå°‡æœƒè©±ç‹€æ…‹ä¿¡æ¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œç¢ºä¿ä¸ä¸Ÿå¤±é‡è¦ä¿¡æ¯
            try:
                from ..core.session_data_manager import session_data_manager
                session_summary = session_data_manager.get_session_summary(context.get("session_id", "default"))
                if session_summary.get("has_current_data"):
                    context["session_data_info"] = {
                        "current_data_file": session_summary.get("current_data_file"),
                        "operations_count": session_summary.get("operations_count"),
                        "last_operation": session_summary.get("last_operation"),
                        "note": "è¨˜æ†¶å£“ç¸®å¾Œä¿ç•™çš„æœƒè©±æ•¸æ“šç‹€æ…‹ä¿¡æ¯"
                    }
                    logger.info(f"ğŸ”„ æœƒè©±ç‹€æ…‹ä¿¡æ¯å·²æ³¨å…¥ä¸Šä¸‹æ–‡: {session_summary.get('current_data_file')}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•æ³¨å…¥æœƒè©±ç‹€æ…‹ä¿¡æ¯: {e}")

        # å¦‚æœæ˜¯batch processingæ¨¡å¼ï¼Œé¡å¤–ç®¡ç†ä¸Šä¸‹æ–‡
        if context.get("is_batch_processing", False):
            messages = self.manage_context_for_batch_processing(messages, context)
            managed_tokens = self.calculate_messages_tokens(messages)
            logger.info(f"ğŸ§  Batchæ¨¡å¼Tokenç®¡ç†å¾Œ: {managed_tokens}")
            # æ›´æ–°stateä¸­çš„messages
            state["messages"] = messages

        # æª¢æŸ¥æ˜¯å¦æ˜¯åˆå§‹æŸ¥è©¢
        is_initial_query = not any(isinstance(msg, (AIMessage, ToolMessage)) for msg in messages)

        if is_initial_query:
            logger.info(f"ğŸ¤– è™•ç†åˆå§‹ç”¨æˆ¶æŸ¥è©¢: {query}")

            # æ§‹å»ºç³»çµ±æç¤º
            system_prompt = self._get_system_prompt(rule_id, context)

            # æ§‹å»ºæ¶ˆæ¯
            llm_messages = [SystemMessage(content=system_prompt)]

            # æ·»åŠ ç”¨æˆ¶æŸ¥è©¢
            if query:
                llm_messages.append(HumanMessage(content=query))

        else:
            # é€™æ˜¯å·¥å…·åŸ·è¡Œå¾Œçš„é‡æ–°è©•ä¼°
            logger.info("ğŸ”„ å·¥å…·åŸ·è¡Œå¾Œé‡æ–°è©•ä¼°ï¼Œæ±ºå®šä¸‹ä¸€æ­¥å‹•ä½œ")

            # æª¢æŸ¥æœ€è¿‘çš„å·¥å…·åŸ·è¡Œçµæœ
            recent_tool_messages = [msg for msg in messages[-5:] if isinstance(msg, ToolMessage)]

            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åŸ·è¡Œäº†å¤ªå¤šå·¥å…·ï¼ˆé˜²æ­¢ç„¡é™å¾ªç’°ï¼‰
            tool_count = len([msg for msg in messages if isinstance(msg, ToolMessage)])
            if tool_count >= 8:
                logger.info(f"ğŸ›‘ å·²åŸ·è¡Œ {tool_count} å€‹å·¥å…·ï¼Œåœæ­¢ä¸¦ç”Ÿæˆå›ç­”")
                # ç›´æ¥ç”Ÿæˆå›ç­”ï¼Œä¸å†èª¿ç”¨å·¥å…·
                final_prompt = f"""åŸºæ–¼å·²åŸ·è¡Œçš„å·¥å…·çµæœï¼Œè«‹ç›´æ¥å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š

ç”¨æˆ¶è«‹æ±‚: {query}

å·²åŸ·è¡Œçš„å·¥å…·çµæœ:
{chr(10).join([f"- {msg.name}: {msg.content[:300]}..." for msg in recent_tool_messages])}

è«‹åŸºæ–¼é€™äº›ä¿¡æ¯æä¾›å®Œæ•´çš„å›ç­”ï¼Œä¸è¦å†èª¿ç”¨ä»»ä½•å·¥å…·ã€‚"""

                llm_messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„ä¿¡æ¯ç›´æ¥å›ç­”ç”¨æˆ¶å•é¡Œï¼Œä¸è¦èª¿ç”¨ä»»ä½•å·¥å…·ã€‚"),
                    HumanMessage(content=final_prompt)
                ]
            elif recent_tool_messages:
                # æª¢æŸ¥æ˜¯å¦ç²å¾—äº†æœ‰æ•ˆçš„é é¢å…§å®¹
                page_content_found = False
                for msg in recent_tool_messages:
                    if msg.name == "browser_get_page_data_tool" and len(msg.content) > 100:
                        page_content_found = True
                        break

                if page_content_found:
                    logger.info("âœ… å·²ç²å¾—æœ‰æ•ˆçš„é é¢å…§å®¹ï¼Œæº–å‚™ç”Ÿæˆå›ç­”")
                    # æœ‰äº†é é¢å…§å®¹ï¼Œæ‡‰è©²å¯ä»¥å›ç­”äº†
                    evaluation_prompt = f"""ä½ å·²ç¶“æˆåŠŸç²å–äº†é é¢å…§å®¹ã€‚è«‹åŸºæ–¼ä»¥ä¸‹ä¿¡æ¯ç›´æ¥å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š

ç”¨æˆ¶è«‹æ±‚: {query}

é é¢å…§å®¹:
{chr(10).join([f"- {msg.name}: {msg.content[:500]}..." for msg in recent_tool_messages if msg.name == "browser_get_page_data_tool"])}

è«‹æä¾›å®Œæ•´çš„å›ç­”ï¼Œä¸éœ€è¦å†èª¿ç”¨å…¶ä»–å·¥å…·ã€‚"""
                else:
                    # æ²’æœ‰ç²å¾—æœ‰æ•ˆå…§å®¹ï¼Œå¯èƒ½éœ€è¦é‡è©¦
                    evaluation_prompt = f"""åŸºæ–¼ä»¥ä¸‹å·¥å…·åŸ·è¡Œçµæœï¼Œè«‹åˆ†ææ˜¯å¦éœ€è¦èª¿ç”¨æ›´å¤šå·¥å…·ä¾†å®Œæˆç”¨æˆ¶çš„è«‹æ±‚ï¼š

ç”¨æˆ¶åŸå§‹è«‹æ±‚: {query}

æœ€è¿‘çš„å·¥å…·åŸ·è¡Œçµæœ:
{chr(10).join([f"- {msg.name}: {msg.content[:200]}..." for msg in recent_tool_messages])}

è«‹æ±ºå®šï¼š
1. å¦‚æœéœ€è¦æ›´å¤šå·¥å…·ä¾†å®Œæˆä»»å‹™ï¼Œè«‹èª¿ç”¨ç›¸æ‡‰çš„å·¥å…·
2. å¦‚æœå·²ç¶“æœ‰è¶³å¤ çš„ä¿¡æ¯ï¼Œè«‹ç›´æ¥å›ç­”ç”¨æˆ¶

æ³¨æ„ï¼šé¿å…é‡è¤‡èª¿ç”¨ç›¸åŒçš„å·¥å…·ï¼Œé™¤éæœ‰æ–°çš„åƒæ•¸æˆ–éœ€æ±‚ã€‚"""

                llm_messages = [
                    SystemMessage(content=self._get_system_prompt(rule_id, context)),
                    HumanMessage(content=evaluation_prompt)
                ]

                # æ·»åŠ å°è©±æ­·å²ï¼ˆæœ€è¿‘çš„æ¶ˆæ¯ï¼‰
                llm_messages.extend(messages[-10:])  # åªä¿ç•™æœ€è¿‘10æ¢æ¶ˆæ¯é¿å…tokenéå¤š
            else:
                # æ²’æœ‰å·¥å…·æ¶ˆæ¯ï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰æ¶ˆæ¯
                llm_messages = [SystemMessage(content=self._get_system_prompt(rule_id, context))]
                llm_messages.extend(messages)

        # èª¿ç”¨ LLM é€²è¡Œæ±ºç­–
        response = await self.current_llm_with_tools.ainvoke(llm_messages)

        # è¨˜éŒ„æ±ºç­–çµæœ
        if hasattr(response, "tool_calls") and response.tool_calls:
            tool_names = [call["name"] for call in response.tool_calls]
            logger.info(f"ğŸ”§ æ±ºå®šèª¿ç”¨å·¥å…·: {tool_names}")
        else:
            logger.info("ğŸ’¬ æ±ºå®šç›´æ¥å›æ‡‰ç”¨æˆ¶")

        return {"messages": [response]}

    def _get_system_prompt(self, rule_id: Optional[str], context: Dict[str, Any]) -> str:
        """ç²å–ç³»çµ±æç¤º"""

        # æª¢æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶è™•ç†æ¨¡å¼
        context_data = context.get('context_data', {}) if context else {}
        is_file_mode = context_data.get('type') == 'file'

        # æª¢æŸ¥æ˜¯å¦æœ‰è‡ªå®šç¾©çš„system_prompt
        custom_system_prompt = context_data.get('system_prompt')
        if custom_system_prompt:
            logger.info("ğŸ“‹ ä½¿ç”¨è‡ªå®šç¾©system_prompt")
            return custom_system_prompt

        if is_file_mode:
            # æ–‡ä»¶è™•ç†æ¨¡å¼çš„ç³»çµ±æç¤º
            file_path = context_data.get('file_path', 'æœªçŸ¥æ–‡ä»¶')
            current_time = context.get('current_time', 'æœªçŸ¥æ™‚é–“')

            # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ summaryï¼ˆå„ªå…ˆå¾ context_data ä¸­ç²å–ï¼‰
            file_summary_info = ""
            summary = None

            # å„ªå…ˆå¾ context_data ä¸­ç²å– file_summary
            if context_data and context_data.get('file_summary'):
                summary = context_data['file_summary']
                logger.info("ğŸ“‹ å¾ context_data ä¸­ç²å–åˆ°æ–‡ä»¶ summary")
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¾ context ä¸­ç²å–
            elif context.get('file_summary'):
                summary = context['file_summary']
                logger.info("ğŸ“‹ å¾ context ä¸­ç²å–åˆ°æ–‡ä»¶ summary")

            if summary:
                file_type = summary.get('type', 'unknown')

                if file_type == 'data':
                    data_info = summary.get('data_info', {})
                    data_shape = data_info.get('data_shape', [0, 0])
                    file_summary_info = f"""
ğŸ“Š **æ–‡ä»¶ Summary å·²è¼‰å…¥**:
- æ–‡ä»¶é¡å‹: æ•¸æ“šæ–‡ä»¶ ({summary.get('file_extension', 'unknown')})
- æ•¸æ“šå½¢ç‹€: {data_shape[0]} è¡Œ Ã— {data_shape[1]} åˆ—
- è™•ç†æ™‚é–“: {summary.get('processed_at', 'unknown')}
- æ•¸å€¼åˆ—: {data_info.get('numeric_columns', [])}
- åˆ†é¡åˆ—: {data_info.get('categorical_columns', [])}
- Session ç›®éŒ„: temp/{summary.get('session_id', 'unknown')}/
"""
                elif file_type == 'text':
                    # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°çš„ç°¡æ½”æ‘˜è¦æ ¼å¼
                    if 'content_sections' in summary and 'file_path' in summary:
                        # æ–°çš„ç°¡æ½”æ‘˜è¦æ ¼å¼
                        file_path = summary.get('file_path', 'unknown')
                        content_sections = summary.get('content_sections', [])

                        file_summary_info = f"""
ğŸ“„ **æ–‡ä»¶æ‘˜è¦**:
æ–‡ä»¶è·¯å¾‘: {file_path}

ğŸ“‹ **å…§å®¹æ®µè½**:
"""
                        # æ·»åŠ æ®µè½æ‘˜è¦ - ä½¿ç”¨ä½ è¦çš„æ ¼å¼
                        for section in content_sections:
                            start_line = section.get('start_line', 0)
                            end_line = section.get('end_line', 0)
                            summary_text = section.get('summary', 'ç„¡æ‘˜è¦')

                            if start_line == end_line:
                                file_summary_info += f"\n**ç¬¬{start_line}è¡Œ**: {summary_text}"
                            else:
                                file_summary_info += f"\n**ç¬¬{start_line}-{end_line}è¡Œ**: {summary_text}"

                    # æª¢æŸ¥æ˜¯å¦æ˜¯æ™ºèƒ½æ‘˜è¦æ ¼å¼
                    elif 'file_info' in summary and 'content_sections' in summary:
                        # æ™ºèƒ½æ‘˜è¦æ ¼å¼
                        file_info = summary.get('file_info', {})
                        content_sections = summary.get('content_sections', [])

                        file_summary_info = f"""
ğŸ“„ **æ–‡ä»¶æ‘˜è¦**:
æ–‡ä»¶è·¯å¾‘: {file_info.get('path', 'unknown')}

ğŸ“‹ **å…§å®¹æ®µè½**:
"""
                        # æ·»åŠ æ®µè½æ‘˜è¦ - ä½¿ç”¨ä½ è¦çš„æ ¼å¼
                        for section in content_sections:
                            section_number = section.get('section_number', 0)
                            line_range = section.get('line_range', '')
                            title = section.get('title', 'ç„¡æ¨™é¡Œ')

                            file_summary_info += f"\n**ç¬¬{line_range}è¡Œ**: {title}"

                    else:
                        # èˆŠçš„æ‘˜è¦æ ¼å¼
                        text_summary = summary.get('text_summary', {})
                        if text_summary and text_summary.get('success'):
                            summary_data = text_summary.get('summary', {})
                            file_info = summary_data.get('file_info', {})
                            segments = summary_data.get('segments', [])
                            overall_stats = summary_data.get('overall_stats', {})

                            file_summary_info = f"""
ğŸ“„ **æ–‡ä»¶ Summary å·²è¼‰å…¥**:
- æ–‡ä»¶é¡å‹: æ–‡æœ¬æ–‡ä»¶ ({summary.get('file_extension', 'unknown')})
- æ–‡ä»¶å¤§å°: {file_info.get('size', 0)} bytes
- è¡Œæ•¸: {file_info.get('lines', 0)}
- ç·¨ç¢¼: {file_info.get('encoding', 'unknown')}
- æ‘˜è¦æ®µè½æ•¸: {len(segments)}
- é—œéµè©: {overall_stats.get('unique_keywords', [])}
- é ä¼°é–±è®€æ™‚é–“: {overall_stats.get('estimated_reading_time', {}).get('reading_time_minutes', 0):.1f} åˆ†é˜

ğŸ“‹ **æ–‡ä»¶å…§å®¹æ‘˜è¦**:
"""
                            # æ·»åŠ æ®µè½æ‘˜è¦
                            for i, segment in enumerate(segments[:5], 1):  # åªé¡¯ç¤ºå‰5å€‹æ®µè½
                                file_summary_info += f"\n{i}. ç¬¬{segment.get('start_line', 0)}-{segment.get('end_line', 0)}è¡Œ: {segment.get('summary', 'ç„¡æ‘˜è¦')}"

                            if len(segments) > 5:
                                file_summary_info += f"\n... é‚„æœ‰ {len(segments) - 5} å€‹æ®µè½"
                        else:
                            file_summary_info = f"""
ğŸ“„ **æ–‡ä»¶ Summary å·²è¼‰å…¥**:
- æ–‡ä»¶é¡å‹: æ–‡æœ¬æ–‡ä»¶ ({summary.get('file_extension', 'unknown')})
- è™•ç†ç‹€æ…‹: æ‘˜è¦ç”Ÿæˆå¤±æ•—
- Session ç›®éŒ„: temp/{summary.get('session_id', 'unknown')}/
"""
                else:
                    file_summary_info = f"""
ğŸ“„ **æ–‡ä»¶ Summary å·²è¼‰å…¥**:
- æ–‡ä»¶é¡å‹: åŸå§‹æ–‡æœ¬ ({summary.get('file_extension', 'unknown')})
- å­—ç¬¦æ•¸: {summary.get('char_count', 0)}
- è¡Œæ•¸: {summary.get('line_count', 0)}
- è™•ç†æ™‚é–“: {summary.get('processed_at', 'unknown')}
- Session ç›®éŒ„: temp/{summary.get('session_id', 'unknown')}/
"""

            base_instructions = f"""
ğŸ“ **æ–‡ä»¶è™•ç†æ¨¡å¼ - Session è¨˜æ†¶ç³»çµ±** (ç•¶å‰æ™‚é–“: {current_time}):
ä½ æ­£åœ¨è™•ç†æ–‡ä»¶: {file_path}

ğŸ§  **Session è¨˜æ†¶ç³»çµ±**:
- é€™æ˜¯ä¸€å€‹æŒçºŒçš„ sessionï¼Œæ–‡ä»¶çš„æ‰€æœ‰ä¿®æ”¹éƒ½æœƒç´¯ç©åœ¨è¨˜æ†¶ä¸­
- ä»¥ä¸‹ Summary æ˜¯ç•¶å‰ session ä¸­æ–‡ä»¶çš„æœ€æ–°ç‹€æ…‹
- æ¯æ¬¡æ–‡ä»¶ä¿®æ”¹å¾Œï¼Œä½ å¿…é ˆæ›´æ–°é€™å€‹ Summary
- é€™å€‹ Summary æ˜¯ä½ å°æ–‡ä»¶çš„å®Œæ•´è¨˜æ†¶ï¼ŒåŒ…å«æ‰€æœ‰æ­·å²ä¿®æ”¹

{file_summary_info}
ğŸ”§ **å¯ç”¨å·¥å…·** (å…±15å€‹)ï¼š

**æ–‡ä»¶æ“ä½œå·¥å…·**:
1. **read_file_with_summary_tool**: é‡æ–°è®€å–æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦
2. **edit_file_by_lines_tool**: æŒ‰è¡Œç·¨è¼¯æ–‡ä»¶
3. **highlight_file_sections_tool**: é«˜äº®æ–‡ä»¶å€åŸŸ
4. **save_file_tool**: ä¿å­˜æ–‡ä»¶
5. **create_file_tool**: å‰µå»ºæ–°æ–‡ä»¶
6. **delete_file_tool**: åˆªé™¤æ–‡ä»¶

**æ•¸æ“šæ–‡ä»¶å·¥å…·**:
7. **read_data_file_tool**: è®€å–æ•¸æ“šæ–‡ä»¶
8. **edit_data_file_tool**: ç·¨è¼¯æ•¸æ“šæ–‡ä»¶ (æ·»åŠ /åˆªé™¤/ä¿®æ”¹è¡Œ)

**æ•¸æ“šåˆ†æå·¥å…·**:
9. **get_data_info_tool**: ç²å–æ•¸æ“šåŸºæœ¬ä¿¡æ¯
10. **group_by_analysis_tool**: åˆ†çµ„åˆ†æ
11. **threshold_analysis_tool**: é–¾å€¼åˆ†æ
12. **correlation_analysis_tool**: ç›¸é—œæ€§åˆ†æ
13. **linear_prediction_tool**: ç·šæ€§é æ¸¬

ğŸ’¡ **åŸ·è¡Œç­–ç•¥ - Session è¨˜æ†¶ç®¡ç†**ï¼š
- **å„ªå…ˆä½¿ç”¨ Session è¨˜æ†¶**: å§‹çµ‚åŸºæ–¼ç•¶å‰ Summary (Session è¨˜æ†¶) å›ç­”å•é¡Œ
- **æ‘˜è¦è«‹æ±‚**: ç›´æ¥ä½¿ç”¨ Summary ä¸­çš„æœ€æ–°ä¿¡æ¯ï¼Œç„¡éœ€èª¿ç”¨å·¥å…·
- **æ•¸æ“šåˆ†æ**: åŸºæ–¼ Summary é€²è¡Œåˆ†æï¼Œå¿…è¦æ™‚ä½¿ç”¨åˆ†æå·¥å…·
- **æ–‡ä»¶ç·¨è¼¯**:
  1. åŸ·è¡Œç·¨è¼¯æ“ä½œ
  2. **ç«‹å³æ›´æ–° Summary** (é€™æ˜¯é—œéµï¼)
  3. ç¢ºä¿ Session è¨˜æ†¶ä¿æŒæœ€æ–°ç‹€æ…‹
- **Session æŒçºŒæ€§**: åŒä¸€ session å…§çš„æ‰€æœ‰æ“ä½œéƒ½åŸºæ–¼ç´¯ç©çš„è¨˜æ†¶

âš ï¸ **Session è¨˜æ†¶ç³»çµ±é‡è¦è¦å‰‡**:
1. **æ°¸é åŸºæ–¼ Summary å›ç­”** - é€™æ˜¯ä½ å°æ–‡ä»¶çš„å®Œæ•´è¨˜æ†¶
2. **ä»»ä½•æ–‡ä»¶ä¿®æ”¹éƒ½å¿…é ˆæ›´æ–° Summary** - ä¿æŒè¨˜æ†¶åŒæ­¥
3. **Summary æ˜¯æŒçºŒç´¯ç©çš„** - åŒ…å«æ‰€æœ‰æ­·å²ä¿®æ”¹ä¿¡æ¯
4. **æ¯æ¬¡æ“ä½œå¾Œæª¢æŸ¥ Summary æ˜¯å¦éœ€è¦æ›´æ–°** - ç¢ºä¿è¨˜æ†¶æº–ç¢ºæ€§

ğŸ“Š **æœƒè©±æ•¸æ“šç®¡ç†**:
- ç•¶ä½¿ç”¨ filter_data_tool æ™‚ï¼Œè¨­ç½® save_filtered_data=True ä¾†ä¿å­˜éæ¿¾å¾Œçš„æ•¸æ“š
- å¾ŒçºŒåˆ†æå·¥å…·å¯ä»¥ä½¿ç”¨ "@current" ä½œç‚º file_path ä¾†è‡ªå‹•ä½¿ç”¨æœ€æ–°çš„éæ¿¾æ•¸æ“š
- ä½¿ç”¨ get_session_data_status_tool() æŸ¥çœ‹ç•¶å‰æœƒè©±çš„æ•¸æ“šç‹€æ…‹

ğŸ”¢ **åˆ†ææ“ä½œé¸æ“‡æŒ‡å—**:
- group_by_analysis_tool æ”¯æŒå¤šç¨®æ“ä½œé¡å‹ï¼Œæ ¹æ“šåˆ†æéœ€æ±‚é¸æ“‡ï¼š
  * "mean" - å¹³å‡å€¼ï¼ˆè–ªè³‡åˆ†æã€ç¸¾æ•ˆè©•ä¼°ç­‰ï¼‰
  * "sum" - ç¸½å’Œï¼ˆéŠ·å”®é¡ã€æ•¸é‡çµ±è¨ˆç­‰ï¼‰
  * "count" - è¨ˆæ•¸ï¼ˆäººå“¡çµ±è¨ˆã€é »æ¬¡åˆ†æç­‰ï¼‰
  * "max" - æœ€å¤§å€¼ï¼ˆæœ€é«˜è–ªè³‡ã€å³°å€¼åˆ†æç­‰ï¼‰
  * "min" - æœ€å°å€¼ï¼ˆæœ€ä½è–ªè³‡ã€åŸºæº–åˆ†æç­‰ï¼‰
- ä¾‹å¦‚ï¼šgroup_by_analysis_tool("@current", "department", "salary", "mean", session_id)

"""
        else:
            # ç€è¦½å™¨æ¨¡å¼çš„ç³»çµ±æç¤º
            base_instructions = """
ğŸŒ **ç€è¦½å™¨æ“ä½œæŒ‡å—**:
ä½ å·²é€£æ¥åˆ°å‰ç«¯ Puppeteer ç€è¦½å™¨ï¼Œå¯ä»¥åŸ·è¡Œä»¥ä¸‹æ“ä½œï¼š

1. **è®€å–ç¶²é å…§å®¹**: ä½¿ç”¨ `read_page_content_tool()` ç²å–ç•¶å‰é é¢çš„æ–‡å­—å…§å®¹å’Œæ‰€æœ‰é€£çµ
2. **ç²å–å¯é»æ“Šå…ƒç´ **: ä½¿ç”¨ `get_clickable_elements_tool()` æ‰¾åˆ°é é¢ä¸Šæ‰€æœ‰å¯é»æ“Šçš„å…ƒç´ 
3. **é»æ“Šé€£çµ**: ä½¿ç”¨ `click_link_tool("é€£çµæ–‡å­—")` é»æ“Šç‰¹å®šé€£çµ
4. **å°èˆªåˆ°URL**: ä½¿ç”¨ `navigate_to_url_tool("https://...")` ç›´æ¥å°èˆªåˆ°æŒ‡å®šç¶²å€
5. **å…¶ä»–ç€è¦½å™¨æ“ä½œ**: é»æ“Šã€è¼¸å…¥ã€æ»¾å‹•ç­‰

âš ï¸ **é‡è¦**: åœ¨åˆ†æä»»ä½•ç¶²é å…§å®¹ä¹‹å‰ï¼Œå¿…é ˆå…ˆä½¿ç”¨ `read_page_content_tool()` è®€å–ç•¶å‰é é¢å…§å®¹ï¼

"""

        # æ·»åŠ ä¸Šä¸‹æ–‡è³‡æ–™
        context_info = ""
        if not is_file_mode and context and "page_data" in context:
            page_data = context["page_data"]
            if page_data:
                context_info = f"""
ğŸ“„ **ç•¶å‰é é¢è³‡è¨Š**:
- URL: {page_data.get('url', 'N/A')}
- æ¨™é¡Œ: {page_data.get('title', 'N/A')}
- å…§å®¹é è¦½: {page_data.get('content', '')[:500]}...
- äº’å‹•å…ƒç´ æ•¸é‡: {len(page_data.get('interactiveElements', []))}
- è¼‰å…¥ç‹€æ…‹: {page_data.get('metadata', {}).get('loadState', 'unknown')}

"""

        if rule_id:
            # å˜—è©¦è¼‰å…¥è¦å‰‡çš„æç¤º
            rule_data = self._load_rule(rule_id)
            if rule_data and rule_data.get("prompt"):
                logger.info(f"ğŸ“‹ ä½¿ç”¨è¦å‰‡æç¤º: {rule_data.get('name', rule_id)}")

                # å‹•æ…‹æ›¿æ› prompt ä¸­çš„å ä½ç¬¦
                rule_prompt = rule_data["prompt"]

                # å¾ context ä¸­ç²å– file_path
                file_path = "æœªæä¾›"
                if context and isinstance(context, dict):
                    context_data = context.get('context_data', {})
                    if isinstance(context_data, dict):
                        file_path = context_data.get('file_path', 'æœªæä¾›')

                # ç²å–ç•¶å‰å°ç£æ™‚é–“
                from datetime import datetime
                import pytz
                taiwan_tz = pytz.timezone('Asia/Taipei')
                current_time = datetime.now(taiwan_tz).strftime('%Y-%m-%d %H:%M:%S (å°ç£æ™‚é–“)')

                # æ›¿æ›å ä½ç¬¦
                rule_prompt = rule_prompt.replace('{file_path}', str(file_path))
                rule_prompt = rule_prompt.replace('{current_time}', current_time)

                logger.info(f"ğŸ“‹ å·²æ›¿æ›å ä½ç¬¦: file_path={file_path}, current_time={current_time}")

                return base_instructions + context_info + "\n" + rule_prompt

        # é è¨­ç³»çµ±æç¤º
        return base_instructions + context_info + """ä½ æ˜¯ä¸€å€‹æ™ºèƒ½çš„ä»»å‹™åŸ·è¡ŒåŠ©æ‰‹ï¼Œå…·å‚™ä»¥ä¸‹èƒ½åŠ›ï¼š

ğŸ¯ **æ ¸å¿ƒè·è²¬**ï¼š
- åˆ†æç”¨æˆ¶éœ€æ±‚ï¼Œåˆ¶å®šåŸ·è¡Œè¨ˆåŠƒ
- æ™ºèƒ½é¸æ“‡å’Œèª¿ç”¨å·¥å…·
- æ ¹æ“šå·¥å…·åŸ·è¡Œçµæœæ±ºå®šä¸‹ä¸€æ­¥å‹•ä½œ
- æä¾›æº–ç¢ºã€æœ‰ç”¨çš„æœ€çµ‚å›ç­”

ğŸ”§ **å¯ç”¨å·¥å…·**ï¼š
1. ğŸ“§ Gmailç®¡ç†ï¼šgmail_summary_tool, mark_important_emails_tool, download_invoices_tool, compose_email_tool, financial_management_tool
2. ğŸŒ ç€è¦½å™¨è‡ªå‹•åŒ–ï¼šbrowser_navigate_tool, browser_click_tool, browser_type_tool, browser_scroll_tool, browser_screenshot_tool, browser_execute_script_tool
3. ğŸ“š Taaze.aiæ¸¬è©¦ï¼štaaze_navigate_to_bestsellers_tool, taaze_click_first_book_tool, taaze_find_qa_section_tool, taaze_ask_question_tool, taaze_get_ai_response_tool, taaze_complete_workflow_tool
4. ğŸ§ª æ¸¬è©¦å·¥å…·ï¼štest_tool
5. ğŸ§  ä»»å‹™è¨˜æ†¶ç®¡ç†ï¼šcreate_batch_task_tool, get_task_status_tool, save_temp_data_tool, load_temp_data_tool, list_session_tasks_tool, pause_task_tool, resume_task_tool, generate_task_report_tool
6. ğŸš€ æ™ºèƒ½æ‰¹æ¬¡è™•ç†ï¼šsmart_batch_processor_tool, get_batch_processing_status_tool
7. ğŸ“Š ç¹ªåœ–å¯è¦–åŒ–ï¼šcreate_line_chart_tool, create_bar_chart_tool, create_scatter_plot_tool, create_pie_chart_tool, list_session_plots_tool
8. ğŸ“ æ–‡ä»¶æ“ä½œï¼šread_file_with_summary_tool, edit_file_by_lines_tool, save_file_tool, create_file_tool, delete_file_tool
9. ğŸ“ˆ æ•¸æ“šåˆ†æï¼šread_data_file_tool, get_data_info_tool, group_by_analysis_tool, threshold_analysis_tool, correlation_analysis_tool, linear_prediction_tool

ğŸ’¡ **åŸ·è¡Œç­–ç•¥**ï¼š
- å¦‚æœä»»å‹™éœ€è¦å¤šå€‹æ­¥é©Ÿï¼Œè«‹é€æ­¥åŸ·è¡Œï¼Œæ¯æ¬¡èª¿ç”¨å¿…è¦çš„å·¥å…·
- æ ¹æ“šå·¥å…·åŸ·è¡Œçµæœè©•ä¼°æ˜¯å¦éœ€è¦èª¿ç”¨æ›´å¤šå·¥å…·
- é¿å…é‡è¤‡èª¿ç”¨ç›¸åŒå·¥å…·ï¼ˆé™¤éåƒæ•¸ä¸åŒï¼‰
- ç•¶æ”¶é›†åˆ°è¶³å¤ ä¿¡æ¯æ™‚ï¼Œæä¾›å®Œæ•´çš„æœ€çµ‚å›ç­”

ğŸ”„ **å¤§é‡æ•¸æ“šè™•ç†ç­–ç•¥**ï¼š
- ç•¶é‡åˆ°éœ€è¦è™•ç†å¤§é‡æ•¸æ“šï¼ˆ>100é …ï¼‰æ™‚ï¼Œå„ªå…ˆä½¿ç”¨ smart_batch_processor_tool
- è©²å·¥å…·æœƒè‡ªå‹•å‰µå»ºæ‰¹æ¬¡ä»»å‹™ã€å¾ªç’°è™•ç†ã€ä¿å­˜ä¸­é–“çµæœåˆ° tmp ç©ºé–“
- æ‰€æœ‰ tool results æœƒè‡ªå‹•ç´¯ç©ï¼Œç„¡éœ€æ‰‹å‹•ç®¡ç†
- ä½¿ç”¨ get_batch_processing_status_tool æŸ¥è©¢è™•ç†é€²åº¦

ğŸ“Š **å¯è¦–åŒ–ç­–ç•¥**ï¼š
- æ•¸æ“šåˆ†æå®Œæˆå¾Œï¼Œä½¿ç”¨ç¹ªåœ–å·¥å…·å‰µå»ºç›¸æ‡‰çš„åœ–è¡¨
- åœ–è¡¨æœƒè‡ªå‹•ä¿å­˜åˆ°æœƒè©±ç›®éŒ„ï¼Œç”¨æˆ¶å¯ä»¥æŸ¥çœ‹å’Œä¸‹è¼‰

ğŸ¯ **æ±ºç­–åŸå‰‡**ï¼š
- å„ªå…ˆä½¿ç”¨æœ€ç›¸é—œçš„å·¥å…·
- å¦‚æœä¸€å€‹å·¥å…·å¤±æ•—ï¼Œè€ƒæ…®æ›¿ä»£æ–¹æ¡ˆ
- ä¿æŒä»»å‹™åŸ·è¡Œçš„é‚è¼¯æ€§å’Œæ•ˆç‡
- å§‹çµ‚ä»¥å®Œæˆç”¨æˆ¶ç›®æ¨™ç‚ºå°å‘

è«‹æ ¹æ“šç”¨æˆ¶éœ€æ±‚æ™ºèƒ½åœ°é¸æ“‡å·¥å…·ä¸¦åŸ·è¡Œä»»å‹™ã€‚"""

    def _parse_query(self, query: str, rule_id: Optional[str] = None) -> tuple[str, Optional[Dict[str, Any]]]:
        """è§£ææŸ¥è©¢ï¼Œæå–è¦å‰‡ä¿¡æ¯"""
        # å¦‚æœç›´æ¥æä¾›äº†rule_idï¼Œè¼‰å…¥è¦å‰‡
        if rule_id:
            rule_data = self._load_rule(rule_id)
            return query, rule_data

        # æª¢æŸ¥æŸ¥è©¢æ˜¯å¦ä»¥ /rule_name æ ¼å¼é–‹å§‹
        if query.startswith("/"):
            parts = query.split(" ", 1)
            if len(parts) >= 1:
                rule_name = parts[0][1:]  # ç§»é™¤ /
                user_input = parts[1] if len(parts) > 1 else ""

                # æ ¹æ“š rule_name æŸ¥æ‰¾è¦å‰‡
                rule_data = self._find_rule_by_name(rule_name)

                if rule_data:
                    logger.info(f"ğŸ¯ æ‰¾åˆ°è¦å‰‡: {rule_name} -> {rule_data['id']}")
                    return user_input, rule_data
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è¦å‰‡: {rule_name}")
                    return query, None

        # æ²’æœ‰è¦å‰‡èª¿ç”¨
        return query, None

    def _load_rule(self, rule_id: str) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥è¦å‰‡"""
        try:
            from pathlib import Path
            import json

            rules_dir = Path(self.rules_dir)
            logger.info(f"ğŸ” å˜—è©¦è¼‰å…¥è¦å‰‡: {rule_id}")
            logger.info(f"ğŸ” è¦å‰‡ç›®éŒ„: {rules_dir}")

            # å˜—è©¦å¤šç¨®æ–‡ä»¶åæ ¼å¼
            possible_files = [
                rules_dir / f"{rule_id}.json",           # hr_analysis.json
                rules_dir / f"{rule_id.replace('_', '-')}.json",  # hr-analysis.json
                rules_dir / f"{rule_id}-rule.json",      # hr_analysis-rule.json
            ]

            logger.info(f"ğŸ” å˜—è©¦çš„æ–‡ä»¶å: {[f.name for f in possible_files]}")

            # å˜—è©¦ç›´æ¥æ–‡ä»¶ååŒ¹é…
            for rule_file in possible_files:
                logger.info(f"ğŸ” æª¢æŸ¥æ–‡ä»¶: {rule_file}")
                logger.info(f"ğŸ” æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {rule_file.exists()}")
                if rule_file.exists():
                    logger.info(f"âœ… æ‰¾åˆ°è¦å‰‡æ–‡ä»¶: {rule_file.name}")
                    with open(rule_file, 'r', encoding='utf-8') as f:
                        rule_data = json.load(f)
                        logger.info(f"âœ… è¦å‰‡è¼‰å…¥æˆåŠŸ: {rule_data.get('name', 'unknown')}")
                        return rule_data

            # å¦‚æœç›´æ¥åŒ¹é…å¤±æ•—ï¼Œéæ­·æ‰€æœ‰æ–‡ä»¶æŸ¥æ‰¾ name åŒ¹é…
            logger.info(f"ğŸ” ç›´æ¥åŒ¹é…å¤±æ•—ï¼Œéæ­·æ‰€æœ‰æ–‡ä»¶æŸ¥æ‰¾ name åŒ¹é…...")
            all_files = list(rules_dir.glob("*.json"))
            logger.info(f"ğŸ” æ‰¾åˆ°çš„æ‰€æœ‰ JSON æ–‡ä»¶: {[f.name for f in all_files]}")

            for rule_file in all_files:
                try:
                    logger.info(f"ğŸ” æª¢æŸ¥æ–‡ä»¶: {rule_file.name}")
                    with open(rule_file, 'r', encoding='utf-8') as f:
                        rule_data = json.load(f)
                        file_name = rule_data.get("name", "unknown")
                        logger.info(f"ğŸ” æ–‡ä»¶ {rule_file.name} çš„ name: '{file_name}', å°‹æ‰¾: '{rule_id}'")
                        if file_name == rule_id:
                            logger.info(f"âœ… é€šé name æ‰¾åˆ°è¦å‰‡æ–‡ä»¶: {rule_file.name}")
                            return rule_data
                except Exception as e:
                    logger.warning(f"âš ï¸ è®€å–æ–‡ä»¶å¤±æ•— {rule_file.name}: {e}")
                    continue

            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è¦å‰‡æ–‡ä»¶: {rule_id}")
            return None
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥è¦å‰‡å¤±æ•— {rule_id}: {e}")
            return None

    def _find_rule_by_name(self, rule_name: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ“šåç¨±æŸ¥æ‰¾è¦å‰‡"""
        try:
            from pathlib import Path
            import json

            rules_dir = Path(self.rules_dir)
            for rule_file in rules_dir.glob("*.json"):
                try:
                    with open(rule_file, 'r', encoding='utf-8') as f:
                        rule_data = json.load(f)
                        if rule_data.get("name", "").replace(" ", "_").lower() == rule_name.lower():
                            return rule_data
                except:
                    continue
            return None
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾è¦å‰‡å¤±æ•— {rule_name}: {e}")
            return None

    async def response_generator_node(self, state: SupervisorAgentState) -> Dict[str, Any]:
        """å›ç­”ç”Ÿæˆç¯€é»"""
        messages = state.get("messages", [])
        query = state.get("query", "")

        logger.info("ğŸ¤– ç”Ÿæˆæœ€çµ‚å›ç­”...")

        # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨çµæœ
        has_tool_results = any(isinstance(msg, ToolMessage) for msg in messages)

        if has_tool_results:
            # æœ‰å·¥å…·èª¿ç”¨çµæœï¼Œç”ŸæˆåŸºæ–¼çµæœçš„å›ç­”
            system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šå·¥å…·åŸ·è¡Œçµæœç‚ºç”¨æˆ¶ç”Ÿæˆç°¡æ½”æ˜ç­çš„å›ç­”ã€‚

è¦æ±‚ï¼š
1. å›ç­”è¦å…·é«”ä¸”æœ‰ç”¨
2. å¦‚æœæœ‰æ•¸æ“šï¼Œè«‹æä¾›å…·é«”æ•¸å­—
3. å¦‚æœæœ‰éŒ¯èª¤ï¼Œè«‹èªªæ˜åŸå› ä¸¦æä¾›è§£æ±ºå»ºè­°
4. ä¿æŒå°ˆæ¥­ä¸”å‹å¥½çš„èªèª¿
5. ç”¨ç¹é«”ä¸­æ–‡å›ç­”"""

            response_messages = [SystemMessage(content=system_prompt)]
            response_messages.extend(messages)

            final_instruction = f"""ç”¨æˆ¶å•é¡Œï¼š{query}

è«‹æ ¹æ“šä¸Šè¿°å·¥å…·åŸ·è¡Œçµæœç”Ÿæˆæœ€çµ‚å›ç­”ã€‚"""

            response_messages.append(HumanMessage(content=final_instruction))
            final_response = await self.llm.ainvoke(response_messages)

            response_content = (
                final_response.content
                if hasattr(final_response, "content")
                else "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†é€™å€‹è«‹æ±‚ã€‚"
            )
        else:
            # æ²’æœ‰å·¥å…·èª¿ç”¨ï¼Œä½¿ç”¨ç›´æ¥å›æ‡‰
            last_message = messages[-1] if messages else None
            if last_message and hasattr(last_message, "content"):
                response_content = last_message.content
            else:
                response_content = "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†é€™å€‹è«‹æ±‚ã€‚"

        logger.info(f"âœ… æœ€çµ‚å›ç­”ç”Ÿæˆå®Œæˆ: {response_content[:100]}...")

        return {
            "messages": [AIMessage(content=response_content)]
        }

    async def run(self, query: str, rule_id: Optional[str] = None, context: Optional[Dict[str, Any]] = None, available_tools: List = None) -> Dict[str, Any]:
        """åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›å›æ‡‰"""
        logger.info(f"ğŸš€ é–‹å§‹è™•ç†æŸ¥è©¢: {query}")
        logger.info(f"ğŸ” è©³ç´°åƒæ•¸:")
        logger.info(f"  - query: {query}")
        logger.info(f"  - rule_id: {rule_id}")
        logger.info(f"  - context: {context}")

        # æ ¹æ“š rule_id è¼‰å…¥è¦å‰‡
        rule_data = None
        if rule_id:
            rule_data = self._load_rule(rule_id)
            logger.info(f"ğŸ“‹ è¼‰å…¥è¦å‰‡: {rule_id}")
            logger.info(f"ğŸ“‹ è¦å‰‡å…§å®¹: {rule_data}")

        # æ ¹æ“šè¦å‰‡è¨­ç½®å·¥å…·
        if rule_data:
            tool_names = rule_data.get("tools", [])
            logger.info(f"ğŸ”§ è¦å‰‡ä¸­çš„å·¥å…·: {tool_names}")
            self.setup_tools_for_query(tool_names, available_tools)
            logger.info(f"ğŸ“‹ ä½¿ç”¨è¦å‰‡: {rule_data.get('name', rule_id)}ï¼Œè¦å‰‡å·¥å…·: {tool_names}")
        else:
            # æ²’æœ‰è¦å‰‡ï¼Œä½¿ç”¨å¤–éƒ¨æä¾›çš„å·¥å…·æˆ–é»˜èªå·¥å…·
            self.setup_tools_for_query([], available_tools)
            if available_tools:
                logger.info("ğŸ“ ä½¿ç”¨å¤–éƒ¨æä¾›çš„ Local File Use Tools")
            else:
                logger.info("ğŸ’¬ ä½¿ç”¨é è¨­æ¨¡å¼ï¼ˆç€è¦½å™¨å·¥å…·ï¼‰")

        # ä½¿ç”¨åŸå§‹æŸ¥è©¢ä½œç‚ºè™•ç†å…§å®¹
        parsed_query = query

        initial_state = {
            "messages": [],
            "query": parsed_query,
            "rule_id": rule_id,
            "context": context or {},
        }

        config = {
            "configurable": {"thread_id": str(uuid.uuid4())},
            "recursion_limit": 50  # å¢åŠ éæ­¸é™åˆ¶åˆ° 50
        }

        # åŸ·è¡Œ graph
        start_time = time.time()
        result = await self.current_graph.ainvoke(initial_state, config=config)
        execution_time = time.time() - start_time

        logger.info(f"â±ï¸ æŸ¥è©¢åŸ·è¡Œå®Œæˆï¼Œè€—æ™‚ {execution_time:.2f}ç§’")

        # æå–æœ€çµ‚å›æ‡‰
        final_message = result["messages"][-1]

        if isinstance(final_message, AIMessage):
            response_content = final_message.content
        else:
            response_content = "æŠ±æ­‰ï¼Œç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚"

        # æå–ä½¿ç”¨çš„å·¥å…·
        tools_used = []
        for msg in result["messages"]:
            if isinstance(msg, ToolMessage):
                tools_used.append(msg.name)

        return {
            "response": response_content,
            "rule_id": rule_id,
            "tools_used": tools_used,
            "execution_time": execution_time,
            "context": context or {}
        }

    # å…·é«”çš„æ¥­å‹™æ–¹æ³•
    async def gmail_summary(self, days: int = 7, keywords: List[str] = None) -> Dict[str, Any]:
        """Gmail éƒµä»¶æ‘˜è¦"""
        query = f"å¹«æˆ‘ç¸½çµæœ€è¿‘ {days} å¤©çš„æœªè®€éƒµä»¶"
        if keywords:
            query += f"ï¼Œç‰¹åˆ¥é—œæ³¨åŒ…å« {', '.join(keywords)} çš„éƒµä»¶"

        return await self.run(query, context={"days": days, "keywords": keywords})

    async def mark_important_emails(self, days: int = 7) -> Dict[str, Any]:
        """æ¨™è¨˜é‡è¦éƒµä»¶"""
        query = f"å¹«æˆ‘æª¢æŸ¥æœ€è¿‘ {days} å¤©çš„éƒµä»¶ï¼Œä¸¦æ¨™è¨˜é‡è¦çš„éƒµä»¶"
        return await self.run(query, context={"days": days})

    async def download_invoices(self, sender: str = "google colab") -> Dict[str, Any]:
        """ä¸‹è¼‰ç™¼ç¥¨"""
        query = f"å¹«æˆ‘ä¸‹è¼‰æ‰€æœ‰ä¾†è‡ª {sender} çš„ç™¼ç¥¨"
        return await self.run(query, context={"sender": sender})

    async def browser_automation(self, action: str, **kwargs) -> Dict[str, Any]:
        """ç€è¦½å™¨è‡ªå‹•åŒ–"""
        query = f"åŸ·è¡Œç€è¦½å™¨æ“ä½œ: {action}"
        return await self.run(query, context={"action": action, **kwargs})

    async def taaze_test(self, question: str = "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ") -> Dict[str, Any]:
        """Taaze.ai æ¸¬è©¦"""
        query = f"åœ¨ Taaze.ai ä¸Šæ¸¬è©¦å•ç­”åŠŸèƒ½ï¼Œå•é¡Œ: {question}"
        return await self.run(query, context={"question": question})

    async def get_status(self) -> Dict[str, Any]:
        """ç²å– Agent ç‹€æ…‹"""
        return {
            "status": "running",
            "tools_count": len(self.tools),
            "uptime": time.time()
        }
