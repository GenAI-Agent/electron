"""
LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·
æä¾›æ¨™æº–çš„ LangChain tool æ ¼å¼
"""

import logging
import sys
from pathlib import Path
from typing import Dict, Any, List
from langchain_core.tools import tool

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å·¥å…·
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

# å°å…¥åŸå§‹å·¥å…·å‡½æ•¸
from tools.local_file_tools import local_file_tools
from tools.data_file_tools import data_file_tools
from tools.data_analysis_tools import data_analysis_tools

logger = logging.getLogger(__name__)

# å°å…¥æ–°çš„å·¥å…·æ¨¡çµ„
try:
    from .langchain_task_memory_tools import get_langchain_task_memory_tools
    from .langchain_plotting_tools import get_langchain_plotting_tools
    from .langchain_batch_processor_tool import get_batch_processor_tools
    EXTENDED_TOOLS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"æ“´å±•å·¥å…·ä¸å¯ç”¨: {e}")
    EXTENDED_TOOLS_AVAILABLE = False

@tool
async def read_file_with_summary_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åŒ…å«æ–‡ä»¶å…§å®¹å’Œæ‘˜è¦çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [read_file_with_summary_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: èª¿ç”¨åº•å±¤å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: èª¿ç”¨ local_file_tools.read_file_with_summary")
        result = await local_file_tools.read_file_with_summary(file_path, session_id)

        # æ­¥é©Ÿ2: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è™•ç†å·¥å…·è¿”å›çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [read_file_with_summary_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [read_file_with_summary_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def edit_file_by_lines_tool(file_path: str, start_line: int, end_line: int, 
                                 new_content: str, session_id: str = "default") -> str:
    """
    æŒ‰è¡Œç·¨è¼¯æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        start_line: é–‹å§‹è¡Œè™Ÿ
        end_line: çµæŸè¡Œè™Ÿ
        new_content: æ–°å…§å®¹
        session_id: æœƒè©±ID
        
    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.edit_file_by_lines(file_path, start_line, end_line, new_content, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·¨è¼¯æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def get_data_info_tool(file_path: str, session_id: str = "default") -> str:
    """
    æ™ºèƒ½ç²å–æ–‡ä»¶ä¿¡æ¯ - è‡ªå‹•åˆ¤æ–·æ–‡ä»¶é¡å‹ä¸¦ä½¿ç”¨ç›¸æ‡‰çš„è™•ç†æ–¹å¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ–‡ä»¶ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [get_data_info_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        import os
        from pathlib import Path
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_size = os.path.getsize(file_path)
        file_ext = Path(file_path).suffix.lower()
        logger.info(f"âœ“ æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} bytesï¼Œå‰¯æª”å: {file_ext}")

        # æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹")
        data_file_extensions = ['.csv', '.json', '.xlsx', '.xls', '.parquet']
        text_file_extensions = ['.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.yaml', '.yml']

        if file_ext in data_file_extensions:
            # æ•¸æ“šæ–‡ä»¶ - ä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·
            logger.info(f"ğŸ“Š è­˜åˆ¥ç‚ºæ•¸æ“šæ–‡ä»¶ï¼Œä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·")
            result = await data_analysis_tools.get_data_info(file_path, session_id)

        elif file_ext in text_file_extensions:
            # æ–‡æœ¬æ–‡ä»¶ - ä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·ç”Ÿæˆæ‘˜è¦
            logger.info(f"ğŸ“„ è­˜åˆ¥ç‚ºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·")
            file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)

            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            result = {
                "success": True,
                "file_type": "text_file",
                "file_path": file_path,
                "file_size": file_size,
                "file_extension": file_ext,
                "summary": file_summary,
                "analysis_type": "text_file_summary"
            }

        else:
            # æœªçŸ¥æ–‡ä»¶é¡å‹ - å˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†
            logger.info(f"âš ï¸ æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†")
            try:
                file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)
                result = {
                    "success": True,
                    "file_type": "unknown_text_file",
                    "file_path": file_path,
                    "file_size": file_size,
                    "file_extension": file_ext,
                    "summary": file_summary,
                    "analysis_type": "text_file_summary",
                    "warning": f"æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå·²ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†"
                }
            except Exception as text_error:
                # å®Œå…¨ç„¡æ³•è™•ç†
                result = {
                    "success": False,
                    "error": f"ç„¡æ³•è™•ç†æ–‡ä»¶é¡å‹ {file_ext}ï¼Œæ—¢ä¸æ˜¯æ”¯æŒçš„æ•¸æ“šæ ¼å¼ï¼Œä¹Ÿç„¡æ³•ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–: {str(text_error)}",
                    "file_path": file_path,
                    "file_extension": file_ext,
                    "supported_data_formats": data_file_extensions,
                    "supported_text_formats": text_file_extensions
                }

        # æ­¥é©Ÿ3: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: è™•ç†åˆ†æçµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            if result.get('file_type') == 'text_file':
                logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æ‘˜è¦å·²ç”Ÿæˆ")
            else:
                data_shape = result.get('data_shape', [0, 0])
                logger.info(f"ğŸ“Š æ•¸æ“šå½¢ç‹€: {data_shape[0]} è¡Œ Ã— {data_shape[1]} åˆ—")
                logger.info(f"ğŸ“Š æ•¸å€¼åˆ—: {result.get('numeric_columns', [])}")
                logger.info(f"ğŸ“Š åˆ†é¡åˆ—: {result.get('categorical_columns', [])}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [get_data_info_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [get_data_info_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def group_by_analysis_tool(file_path: str, group_column: str, value_column: str, 
                                operation: str = "mean", session_id: str = "default") -> str:
    """
    é€šç”¨åˆ†çµ„åˆ†æå·¥å…·
    
    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ (mean, sum, count, min, max)
        session_id: æœƒè©±ID
        
    Returns:
        åˆ†çµ„åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.group_by_analysis(file_path, group_column, value_column, operation, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆ†çµ„åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def threshold_analysis_tool(file_path: str, value_column: str, threshold: float, 
                                 comparison: str = "greater", session_id: str = "default") -> str:
    """
    é€šç”¨é–¾å€¼åˆ†æå·¥å…·
    
    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        value_column: æ•¸å€¼åˆ—å
        threshold: é–¾å€¼
        comparison: æ¯”è¼ƒæ–¹å¼ (greater, less, equal)
        session_id: æœƒè©±ID
        
    Returns:
        é–¾å€¼åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.threshold_analysis(file_path, value_column, threshold, comparison, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é–¾å€¼åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def read_data_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ•¸æ“šæ–‡ä»¶
    
    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID
        
    Returns:
        æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_file_tools.read_data_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ è®€å–æ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# æ·»åŠ æ›´å¤šå·¥å…·
@tool
async def highlight_file_sections_tool(file_path: str, ranges: str, session_id: str = "default") -> str:
    """
    é«˜äº®æ–‡ä»¶å€åŸŸ

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        ranges: ç¯„åœåˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        é«˜äº®çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        ranges_list = json.loads(ranges)
        result = await local_file_tools.highlight_file_sections(file_path, ranges_list, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é«˜äº®æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def save_file_tool(file_path: str, content: str, encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    ä¿å­˜æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        ä¿å­˜çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.save_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def create_file_tool(file_path: str, content: str = "", encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    å‰µå»ºæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.create_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def correlation_analysis_tool(file_path: str, x_column: str, y_column: str, session_id: str = "default") -> str:
    """
    ç›¸é—œæ€§åˆ†æ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        session_id: æœƒè©±ID

    Returns:
        ç›¸é—œæ€§åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.correlation_analysis(file_path, x_column, y_column, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç›¸é—œæ€§åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def linear_prediction_tool(file_path: str, x_column: str, y_column: str, target_x_value: float, session_id: str = "default") -> str:
    """
    ç·šæ€§é æ¸¬

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        target_x_value: ç›®æ¨™Xå€¼
        session_id: æœƒè©±ID

    Returns:
        é æ¸¬çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.linear_prediction(file_path, x_column, y_column, target_x_value, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·šæ€§é æ¸¬å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def edit_data_file_tool(file_path: str, operation: str, data: str, session_id: str = "default") -> str:
    """
    ç·¨è¼¯æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        operation: æ“ä½œé¡å‹ (add_row, delete_row, update_cell, update_row)
        data: æ“ä½œæ•¸æ“šçš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [edit_data_file_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸:")
    logger.info(f"   - file_path: '{file_path}'")
    logger.info(f"   - operation: '{operation}'")
    logger.info(f"   - data: '{data}'")
    logger.info(f"   - session_id: '{session_id}'")

    try:
        # æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹")
        valid_operations = ['add_row', 'delete_row', 'update_cell', 'update_row']
        if operation not in valid_operations:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œé¡å‹: {operation}ï¼Œæ”¯æŒçš„æ“ä½œ: {valid_operations}")
        logger.info(f"âœ“ æ“ä½œé¡å‹æœ‰æ•ˆ: {operation}")

        # æ­¥é©Ÿ2: è§£ææ•¸æ“š
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è§£ææ“ä½œæ•¸æ“š")
        import json
        try:
            parsed_data = json.loads(data)
            logger.info(f"âœ“ æ•¸æ“šè§£ææˆåŠŸ: {type(parsed_data)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}")

        # æ­¥é©Ÿ3: èª¿ç”¨ç·¨è¼¯å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: èª¿ç”¨ data_file_tools.edit_data_file")
        result = await data_file_tools.edit_data_file(file_path, operation, data, session_id)

        # æ­¥é©Ÿ4: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ4: è™•ç†ç·¨è¼¯çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            logger.info(f"âœ… ç·¨è¼¯æ“ä½œæˆåŠŸå®Œæˆ")
            if 'affected_rows' in result:
                logger.info(f"ğŸ“Š å½±éŸ¿è¡Œæ•¸: {result['affected_rows']}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [edit_data_file_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [edit_data_file_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def delete_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.delete_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# ç²å–æ‰€æœ‰æœ¬åœ°æ–‡ä»¶å·¥å…·
def get_langchain_local_file_tools() -> List:
    """ç²å–æ‰€æœ‰ LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·"""
    tools = [
        # åŸºæœ¬æ–‡ä»¶æ“ä½œå·¥å…·
        read_file_with_summary_tool,
        edit_file_by_lines_tool,
        highlight_file_sections_tool,
        save_file_tool,
        create_file_tool,
        delete_file_tool,
        # æ•¸æ“šæ–‡ä»¶å·¥å…·
        read_data_file_tool,
        edit_data_file_tool,
        # æ•¸æ“šåˆ†æå·¥å…·
        get_data_info_tool,
        group_by_analysis_tool,
        threshold_analysis_tool,
        correlation_analysis_tool,
        linear_prediction_tool,
    ]

    # æ·»åŠ æ“´å±•å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if EXTENDED_TOOLS_AVAILABLE:
        try:
            # æ·»åŠ  Task Memory å·¥å…·
            tools.extend(get_langchain_task_memory_tools())
            logger.info("âœ… Task Memory å·¥å…·å·²æ·»åŠ ")

            # æ·»åŠ ç¹ªåœ–å·¥å…·
            tools.extend(get_langchain_plotting_tools())
            logger.info("âœ… ç¹ªåœ–å·¥å…·å·²æ·»åŠ ")

            # æ·»åŠ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·
            tools.extend(get_batch_processor_tools())
            logger.info("âœ… æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·å·²æ·»åŠ ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ æ“´å±•å·¥å…·å¤±æ•—: {e}")

    return tools
