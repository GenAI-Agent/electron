"""
LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·
æä¾›æ¨™æº–çš„ LangChain tool æ ¼å¼
"""

import logging
import sys
from pathlib import Path
from typing import Dict, Any, List
from langchain_core.tools import tool

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å·¥å…·
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

# å°å…¥åŸå§‹å·¥å…·å‡½æ•¸
from tools.local_file_tools import local_file_tools
from tools.data_file_tools import data_file_tools
from tools.data_analysis_tools import data_analysis_tools

# å°å…¥æœƒè©±æ•¸æ“šç®¡ç†å™¨
import sys
from pathlib import Path
core_dir = Path(__file__).parent.parent / "core"
sys.path.insert(0, str(core_dir))
from session_data_manager import session_data_manager

logger = logging.getLogger(__name__)

# å°å…¥æ–°çš„å·¥å…·æ¨¡çµ„
# åˆå§‹åŒ–æ“´å±•å·¥å…·å‡½æ•¸
get_langchain_task_memory_tools = lambda: []
get_langchain_plotting_tools = lambda: []
get_batch_processor_tools = lambda: []

EXTENDED_TOOLS_AVAILABLE = False

try:
    # ä½¿ç”¨çµ•å°å°å…¥é¿å…ç›¸å°å°å…¥å•é¡Œ
    import importlib.util

    # è·³étask_memory_toolsçš„å°å…¥ï¼Œå› ç‚ºå®ƒä¾è³´è¤‡é›œçš„å¤–éƒ¨å­˜å„²ç³»çµ±
    # åœ¨ç°¡åŒ–ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å€‘å°ˆæ³¨æ–¼æ ¸å¿ƒçš„æ–‡ä»¶æ“ä½œå’Œæ•¸æ“šè™•ç†åŠŸèƒ½
    logger.info("âš ï¸ è·³éTask Memoryå·¥å…·å°å…¥ï¼ˆé¿å…è¤‡é›œä¾è³´ï¼‰")
    logger.info("âš ï¸ è·³éPlottingå·¥å…·å°å…¥ï¼ˆå°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½ï¼‰")

    # å‹•æ…‹å°å…¥batch_processor_tool
    batch_path = current_dir / "langchain_batch_processor_tool.py"
    if batch_path.exists():
        try:
            spec = importlib.util.spec_from_file_location("batch_processor_tool", batch_path)
            batch_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(batch_module)
            get_batch_processor_tools = batch_module.get_batch_processor_tools
            logger.info("âœ… Batch Processorå·¥å…·å°å…¥æˆåŠŸ")
            EXTENDED_TOOLS_AVAILABLE = True  # è‡³å°‘batch processoræˆåŠŸäº†
        except Exception as e:
            logger.warning(f"âš ï¸ Batch Processorå·¥å…·å°å…¥å¤±æ•—: {e}")

except Exception as e:
    logger.warning(f"æ“´å±•å·¥å…·å°å…¥éç¨‹å¤±æ•—: {e}")

@tool
async def read_file_with_summary_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åŒ…å«æ–‡ä»¶å…§å®¹å’Œæ‘˜è¦çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [read_file_with_summary_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: èª¿ç”¨åº•å±¤å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: èª¿ç”¨ local_file_tools.read_file_with_summary")
        result = await local_file_tools.read_file_with_summary(file_path, session_id)

        # æ­¥é©Ÿ2: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è™•ç†å·¥å…·è¿”å›çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [read_file_with_summary_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [read_file_with_summary_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def edit_file_by_lines_tool(file_path: str, start_line: int, end_line: int, 
                                 new_content: str, session_id: str = "default") -> str:
    """
    æŒ‰è¡Œç·¨è¼¯æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        start_line: é–‹å§‹è¡Œè™Ÿ
        end_line: çµæŸè¡Œè™Ÿ
        new_content: æ–°å…§å®¹
        session_id: æœƒè©±ID
        
    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.edit_file_by_lines(file_path, start_line, end_line, new_content, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·¨è¼¯æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def get_data_info_tool(file_path: str, session_id: str = "default") -> str:
    """
    æ™ºèƒ½ç²å–æ–‡ä»¶ä¿¡æ¯ - è‡ªå‹•åˆ¤æ–·æ–‡ä»¶é¡å‹ä¸¦ä½¿ç”¨ç›¸æ‡‰çš„è™•ç†æ–¹å¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ–‡ä»¶ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [get_data_info_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        import os
        from pathlib import Path
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_size = os.path.getsize(file_path)
        file_ext = Path(file_path).suffix.lower()
        logger.info(f"âœ“ æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} bytesï¼Œå‰¯æª”å: {file_ext}")

        # æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹")
        data_file_extensions = ['.csv', '.json', '.xlsx', '.xls', '.parquet']
        text_file_extensions = ['.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.yaml', '.yml']

        if file_ext in data_file_extensions:
            # æ•¸æ“šæ–‡ä»¶ - ä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·
            logger.info(f"ğŸ“Š è­˜åˆ¥ç‚ºæ•¸æ“šæ–‡ä»¶ï¼Œä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·")
            result = await data_analysis_tools.get_data_info(file_path, session_id)

        elif file_ext in text_file_extensions:
            # æ–‡æœ¬æ–‡ä»¶ - ä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·ç”Ÿæˆæ‘˜è¦
            logger.info(f"ğŸ“„ è­˜åˆ¥ç‚ºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·")
            file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)

            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            result = {
                "success": True,
                "file_type": "text_file",
                "file_path": file_path,
                "file_size": file_size,
                "file_extension": file_ext,
                "summary": file_summary,
                "analysis_type": "text_file_summary"
            }

        else:
            # æœªçŸ¥æ–‡ä»¶é¡å‹ - å˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†
            logger.info(f"âš ï¸ æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†")
            try:
                file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)
                result = {
                    "success": True,
                    "file_type": "unknown_text_file",
                    "file_path": file_path,
                    "file_size": file_size,
                    "file_extension": file_ext,
                    "summary": file_summary,
                    "analysis_type": "text_file_summary",
                    "warning": f"æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå·²ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†"
                }
            except Exception as text_error:
                # å®Œå…¨ç„¡æ³•è™•ç†
                result = {
                    "success": False,
                    "error": f"ç„¡æ³•è™•ç†æ–‡ä»¶é¡å‹ {file_ext}ï¼Œæ—¢ä¸æ˜¯æ”¯æŒçš„æ•¸æ“šæ ¼å¼ï¼Œä¹Ÿç„¡æ³•ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–: {str(text_error)}",
                    "file_path": file_path,
                    "file_extension": file_ext,
                    "supported_data_formats": data_file_extensions,
                    "supported_text_formats": text_file_extensions
                }

        # æ­¥é©Ÿ3: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: è™•ç†åˆ†æçµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            if result.get('file_type') == 'text_file':
                logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æ‘˜è¦å·²ç”Ÿæˆ")
            else:
                data_shape = result.get('data_shape', [0, 0])
                logger.info(f"ğŸ“Š æ•¸æ“šå½¢ç‹€: {data_shape[0]} è¡Œ Ã— {data_shape[1]} åˆ—")
                logger.info(f"ğŸ“Š æ•¸å€¼åˆ—: {result.get('numeric_columns', [])}")
                logger.info(f"ğŸ“Š åˆ†é¡åˆ—: {result.get('categorical_columns', [])}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [get_data_info_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [get_data_info_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def group_by_analysis_tool(file_path: str, group_column: str, value_column: str,
                                operation: str = "sum", session_id: str = "default",
                                data_source: str = "file") -> str:
    """
    é€šç”¨åˆ†çµ„åˆ†æå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘ï¼Œæ”¯æŒç‰¹æ®Šå€¼ "@current" ä½¿ç”¨ç•¶å‰æœƒè©±çš„æœ€æ–°æ•¸æ“š
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ï¼Œæ ¹æ“šåˆ†æéœ€æ±‚é¸æ“‡ï¼š
                  - "mean": å¹³å‡å€¼ï¼ˆè–ªè³‡åˆ†æã€ç¸¾æ•ˆè©•ä¼°ï¼‰
                  - "sum": ç¸½å’Œï¼ˆéŠ·å”®é¡ã€æ•¸é‡çµ±è¨ˆï¼‰
                  - "count": è¨ˆæ•¸ï¼ˆäººå“¡çµ±è¨ˆã€é »æ¬¡åˆ†æï¼‰
                  - "max": æœ€å¤§å€¼ï¼ˆæœ€é«˜è–ªè³‡ã€å³°å€¼åˆ†æï¼‰
                  - "min": æœ€å°å€¼ï¼ˆæœ€ä½è–ªè³‡ã€åŸºæº–åˆ†æï¼‰
        session_id: æœƒè©±ID
        data_source: æ•¸æ“šæºé¡å‹ ("file": å¾æ–‡ä»¶åŠ è¼‰, "current": ä½¿ç”¨ç•¶å‰æœƒè©±æ•¸æ“š)

    Returns:
        åˆ†çµ„åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        logger.info(f"ğŸ”„ group_by_analysis_tool é–‹å§‹åŸ·è¡Œ:")
        logger.info(f"  - åŸå§‹ file_path: {file_path}")
        logger.info(f"  - group_column: {group_column}")
        logger.info(f"  - value_column: {value_column}")
        logger.info(f"  - operation: {operation}")
        logger.info(f"  - session_id: {session_id}")
        logger.info(f"  - data_source: {data_source}")

        # æ ¹æ“š data_source åƒæ•¸æ±ºå®šæ•¸æ“šä¾†æº
        if data_source == "current" or file_path in ["@current", "current", "latest"]:
            # ä½¿ç”¨æœƒè©±æ•¸æ“šç®¡ç†å™¨è§£æè·¯å¾‘
            resolved_file_path = session_data_manager.resolve_file_path(session_id, file_path)
            logger.info(f"ğŸ”„ ä½¿ç”¨æœƒè©±æ•¸æ“š: {file_path} -> {resolved_file_path}")
        else:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„æ–‡ä»¶è·¯å¾‘
            resolved_file_path = file_path
            logger.info(f"ğŸ”„ ä½¿ç”¨æŒ‡å®šæ–‡ä»¶: {resolved_file_path}")

        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        from pathlib import Path
        if not Path(resolved_file_path).exists():
            error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {resolved_file_path}"
            logger.error(f"âŒ {error_msg}")
            return f'{{"success": false, "error": "{error_msg}"}}'

        result = await data_analysis_tools.group_by_analysis(resolved_file_path, group_column, value_column, operation, session_id)
        logger.info(f"âœ… group_by_analysis_tool åŸ·è¡Œå®Œæˆ")
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆ†çµ„åˆ†æå¤±æ•—: {e}")
        import traceback
        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def threshold_analysis_tool(file_path: str, value_column: str, threshold: float,
                                 comparison: str = "greater", session_id: str = "default") -> str:
    """
    é€šç”¨é–¾å€¼åˆ†æå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘ï¼Œæ”¯æŒç‰¹æ®Šå€¼ "@current" ä½¿ç”¨ç•¶å‰æœƒè©±çš„æœ€æ–°æ•¸æ“š
        value_column: æ•¸å€¼åˆ—å
        threshold: é–¾å€¼
        comparison: æ¯”è¼ƒæ–¹å¼ (greater, less, equal)
        session_id: æœƒè©±ID

    Returns:
        é–¾å€¼åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # è§£ææ–‡ä»¶è·¯å¾‘
        resolved_file_path = session_data_manager.resolve_file_path(session_id, file_path)
        logger.info(f"ğŸ”„ threshold_analysis_tool: {file_path} -> {resolved_file_path}")

        result = await data_analysis_tools.threshold_analysis(resolved_file_path, value_column, threshold, comparison, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é–¾å€¼åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def read_data_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_file_tools.read_data_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ è®€å–æ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def filter_data_tool(file_path: str, filter_conditions: str, session_id: str = "default",
                          save_filtered_data: bool = False, selected_columns: str = None) -> str:
    """
    æ ¹æ“šæ¢ä»¶éæ¿¾æ•¸æ“šæ–‡ä»¶ï¼Œæ”¯æŒåˆ—é¸æ“‡

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        filter_conditions: éæ¿¾æ¢ä»¶çš„JSONå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: {"column": "age", "operator": ">", "value": 25}
        session_id: æœƒè©±ID
        save_filtered_data: æ˜¯å¦å°‡éæ¿¾å¾Œçš„æ•¸æ“šä¿å­˜ç‚ºè‡¨æ™‚æ–‡ä»¶ï¼Œä¾›å…¶ä»–å·¥å…·ä½¿ç”¨
        selected_columns: è¦ä¿ç•™çš„åˆ—åJSONæ•¸çµ„ï¼Œä¾‹å¦‚: ["å§“å", "éƒ¨é–€", "åŸºæœ¬è–ªè³‡"]ï¼Œå¦‚æœç‚ºNoneå‰‡ä¿ç•™æ‰€æœ‰åˆ—

    Returns:
        éæ¿¾å¾Œçš„æ•¸æ“šJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœsave_filtered_data=Trueï¼Œé‚„æœƒåŒ…å«è‡¨æ™‚æ–‡ä»¶è·¯å¾‘
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£æéæ¿¾æ¢ä»¶
        conditions = json.loads(filter_conditions)

        # è®€å–æ•¸æ“šæ–‡ä»¶
        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹è®€å–
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        if isinstance(conditions, dict):
            conditions = [conditions]  # è½‰æ›ç‚ºåˆ—è¡¨

        filtered_df = df.copy()

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in filtered_df.columns:
                continue

            if operator == '>':
                filtered_df = filtered_df[filtered_df[column] > value]
            elif operator == '<':
                filtered_df = filtered_df[filtered_df[column] < value]
            elif operator == '>=':
                filtered_df = filtered_df[filtered_df[column] >= value]
            elif operator == '<=':
                filtered_df = filtered_df[filtered_df[column] <= value]
            elif operator == '==':
                filtered_df = filtered_df[filtered_df[column] == value]
            elif operator == '!=':
                filtered_df = filtered_df[filtered_df[column] != value]
            elif operator == 'contains':
                filtered_df = filtered_df[filtered_df[column].str.contains(str(value), na=False)]
            elif operator == 'in':
                filtered_df = filtered_df[filtered_df[column].isin(value)]

        # è™•ç†åˆ—é¸æ“‡
        if selected_columns:
            try:
                columns_list = json.loads(selected_columns) if isinstance(selected_columns, str) else selected_columns
                if isinstance(columns_list, list):
                    # æª¢æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                    available_columns = [col for col in columns_list if col in filtered_df.columns]
                    missing_columns = [col for col in columns_list if col not in filtered_df.columns]

                    if missing_columns:
                        logger.warning(f"âš ï¸ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {missing_columns}")

                    if available_columns:
                        filtered_df = filtered_df[available_columns]
                        logger.info(f"âœ… å·²é¸æ“‡åˆ—: {available_columns}")
                    else:
                        logger.warning(f"âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„åˆ—å¯é¸æ“‡ï¼Œä¿ç•™æ‰€æœ‰åˆ—")
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"âš ï¸ åˆ—é¸æ“‡åƒæ•¸æ ¼å¼éŒ¯èª¤: {e}ï¼Œä¿ç•™æ‰€æœ‰åˆ—")

        # æº–å‚™åŸºæœ¬çµæœ
        result = {
            "success": True,
            "original_rows": len(df),
            "filtered_rows": len(filtered_df),
            "columns": list(filtered_df.columns),
            "filter_conditions": conditions
        }

        # å¦‚æœéœ€è¦ä¿å­˜è‡¨æ™‚æ–‡ä»¶
        if save_filtered_data and len(filtered_df) > 0:
            import tempfile
            import os
            from pathlib import Path

            # å‰µå»ºæœƒè©±ç´šè‡¨æ™‚ç›®éŒ„
            temp_dir = Path(tempfile.gettempdir()) / "agent_sessions" / session_id
            temp_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆè‡¨æ™‚æ–‡ä»¶å
            original_name = Path(file_path).stem
            timestamp = __import__('datetime').datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_filename = f"{original_name}_filtered_{timestamp}.json"
            temp_file_path = temp_dir / temp_filename

            # ä¿å­˜éæ¿¾å¾Œçš„æ•¸æ“šç‚ºJSONæ ¼å¼
            filtered_df.to_json(temp_file_path, orient='records', ensure_ascii=False, indent=2)

            # æ›´æ–°æœƒè©±æ•¸æ“šç‹€æ…‹
            session_data_manager.update_data_state(
                session_id=session_id,
                original_file=file_path,
                current_file=str(temp_file_path),
                operation="filter",
                metadata={
                    "original_rows": len(df),
                    "filtered_rows": len(filtered_df),
                    "filter_conditions": conditions
                },
                description=f"éæ¿¾æ¢ä»¶: {conditions}"
            )

            result.update({
                "temp_file_path": str(temp_file_path),
                "temp_file_created": True,
                "current_data_updated": True,
                "message": f"éæ¿¾å¾Œçš„æ•¸æ“šå·²ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶ä¸¦è¨­ç‚ºç•¶å‰æ•¸æ“šæº: {temp_file_path}"
            })

            # åªè¿”å›å‰10è¡Œæ•¸æ“šé è¦½ï¼Œé¿å…éŸ¿æ‡‰éå¤§
            result["data_preview"] = filtered_df.head(10).to_dict('records')
            logger.info(f"âœ… éæ¿¾å¾Œæ•¸æ“šå·²ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶: {temp_file_path}")
        else:
            # ä¸ä¿å­˜æ–‡ä»¶æ™‚ï¼Œè¿”å›å®Œæ•´æ•¸æ“šï¼ˆä½†é™åˆ¶åœ¨100è¡Œä»¥å…§ï¼‰
            max_rows = 100
            if len(filtered_df) > max_rows:
                result["data"] = filtered_df.head(max_rows).to_dict('records')
                result["data_truncated"] = True
                result["message"] = f"æ•¸æ“šå·²æˆªæ–·ï¼Œåªé¡¯ç¤ºå‰{max_rows}è¡Œã€‚å¦‚éœ€å®Œæ•´æ•¸æ“šï¼Œè«‹è¨­ç½®save_filtered_data=True"
            else:
                result["data"] = filtered_df.to_dict('records')

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šéæ¿¾å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def cleanup_temp_files_tool(session_id: str = "default") -> str:
    """
    æ¸…ç†æœƒè©±çš„è‡¨æ™‚æ–‡ä»¶

    Args:
        session_id: æœƒè©±ID

    Returns:
        æ¸…ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import tempfile
        import shutil
        from pathlib import Path

        temp_dir = Path(tempfile.gettempdir()) / "agent_sessions" / session_id

        if temp_dir.exists():
            # è¨ˆç®—æ–‡ä»¶æ•¸é‡å’Œå¤§å°
            files = list(temp_dir.glob("*"))
            file_count = len(files)
            total_size = sum(f.stat().st_size for f in files if f.is_file())

            # åˆªé™¤æ•´å€‹æœƒè©±ç›®éŒ„
            shutil.rmtree(temp_dir)

            result = {
                "success": True,
                "cleaned_files": file_count,
                "freed_space_bytes": total_size,
                "message": f"å·²æ¸…ç† {file_count} å€‹è‡¨æ™‚æ–‡ä»¶ï¼Œé‡‹æ”¾ {total_size} å­—ç¯€ç©ºé–“"
            }
        else:
            result = {
                "success": True,
                "cleaned_files": 0,
                "message": "æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è‡¨æ™‚æ–‡ä»¶"
            }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def get_session_data_status_tool(session_id: str = "default") -> str:
    """
    ç²å–æœƒè©±çš„æ•¸æ“šç‹€æ…‹ä¿¡æ¯

    Args:
        session_id: æœƒè©±ID

    Returns:
        æœƒè©±æ•¸æ“šç‹€æ…‹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        summary = session_data_manager.get_session_summary(session_id)
        history = session_data_manager.get_data_history(session_id)

        result = {
            "success": True,
            "session_summary": summary,
            "data_history": history,
            "message": f"æœƒè©± {session_id} æ•¸æ“šç‹€æ…‹ä¿¡æ¯"
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ ç²å–æœƒè©±æ•¸æ“šç‹€æ…‹å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def clear_session_data_tool(session_id: str = "default") -> str:
    """
    æ¸…ç†æœƒè©±çš„æ•¸æ“šç‹€æ…‹ï¼ˆä¸åˆªé™¤å¯¦éš›æ–‡ä»¶ï¼‰

    Args:
        session_id: æœƒè©±ID

    Returns:
        æ¸…ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = session_data_manager.clear_session_data(session_id)
        result["success"] = True

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†æœƒè©±æ•¸æ“šç‹€æ…‹å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def suggest_analysis_operation_tool(analysis_purpose: str) -> str:
    """
    æ ¹æ“šåˆ†æç›®çš„å»ºè­°åˆé©çš„æ“ä½œé¡å‹

    Args:
        analysis_purpose: åˆ†æç›®çš„æè¿°ï¼Œä¾‹å¦‚ "è¨ˆç®—éƒ¨é–€å¹³å‡è–ªè³‡"ã€"çµ±è¨ˆå„éƒ¨é–€äººæ•¸"

    Returns:
        å»ºè­°çš„æ“ä½œé¡å‹å’Œèªªæ˜
    """
    try:
        purpose_lower = analysis_purpose.lower()

        suggestions = {
            "mean": {
                "keywords": ["å¹³å‡", "å‡å€¼", "average", "mean", "è–ªè³‡åˆ†æ", "ç¸¾æ•ˆ", "è©•åˆ†"],
                "description": "è¨ˆç®—å¹³å‡å€¼ï¼Œé©ç”¨æ–¼è–ªè³‡åˆ†æã€ç¸¾æ•ˆè©•ä¼°ã€è©•åˆ†çµ±è¨ˆç­‰"
            },
            "sum": {
                "keywords": ["ç¸½å’Œ", "ç¸½è¨ˆ", "åˆè¨ˆ", "sum", "total", "éŠ·å”®é¡", "ç‡Ÿæ”¶", "æ•¸é‡"],
                "description": "è¨ˆç®—ç¸½å’Œï¼Œé©ç”¨æ–¼éŠ·å”®é¡çµ±è¨ˆã€æ•¸é‡åˆè¨ˆã€ç‡Ÿæ”¶åˆ†æç­‰"
            },
            "count": {
                "keywords": ["æ•¸é‡", "äººæ•¸", "å€‹æ•¸", "count", "çµ±è¨ˆ", "é »æ¬¡", "æ¬¡æ•¸"],
                "description": "è¨ˆç®—æ•¸é‡ï¼Œé©ç”¨æ–¼äººå“¡çµ±è¨ˆã€é »æ¬¡åˆ†æã€è¨ˆæ•¸çµ±è¨ˆç­‰"
            },
            "max": {
                "keywords": ["æœ€å¤§", "æœ€é«˜", "max", "maximum", "å³°å€¼", "é ‚é»"],
                "description": "æ‰¾å‡ºæœ€å¤§å€¼ï¼Œé©ç”¨æ–¼æœ€é«˜è–ªè³‡ã€å³°å€¼åˆ†æã€æ¥µå€¼çµ±è¨ˆç­‰"
            },
            "min": {
                "keywords": ["æœ€å°", "æœ€ä½", "min", "minimum", "åŸºæº–", "åº•ç·š"],
                "description": "æ‰¾å‡ºæœ€å°å€¼ï¼Œé©ç”¨æ–¼æœ€ä½è–ªè³‡ã€åŸºæº–åˆ†æã€æ¥µå€¼çµ±è¨ˆç­‰"
            }
        }

        # æ ¹æ“šé—œéµè©åŒ¹é…å»ºè­°æ“ä½œ
        best_match = "sum"  # é»˜èª
        best_score = 0

        for operation, info in suggestions.items():
            score = sum(1 for keyword in info["keywords"] if keyword in purpose_lower)
            if score > best_score:
                best_score = score
                best_match = operation

        result = {
            "success": True,
            "analysis_purpose": analysis_purpose,
            "suggested_operation": best_match,
            "description": suggestions[best_match]["description"],
            "all_options": {op: info["description"] for op, info in suggestions.items()},
            "usage_example": f'group_by_analysis_tool("@current", "group_column", "value_column", "{best_match}", session_id)'
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ å»ºè­°åˆ†ææ“ä½œå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def filter_and_analyze_tool(file_path: str, filter_conditions: str,
                                 group_column: str, value_column: str,
                                 operation: str = "mean", selected_columns: str = None,
                                 session_id: str = "default") -> str:
    """
    ä¸€æ­¥å®Œæˆéæ¿¾å’Œåˆ†çµ„åˆ†æçš„çµ„åˆå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        filter_conditions: éæ¿¾æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ (mean, sum, count, max, min)
        selected_columns: è¦ä¿ç•™çš„åˆ—åJSONæ•¸çµ„ï¼Œä¾‹å¦‚: ["å§“å", "éƒ¨é–€", "åŸºæœ¬è–ªè³‡"]
        session_id: æœƒè©±ID

    Returns:
        åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        logger.info(f"ğŸ”„ filter_and_analyze_tool é–‹å§‹åŸ·è¡Œ:")
        logger.info(f"  - file_path: {file_path}")
        logger.info(f"  - filter_conditions: {filter_conditions}")
        logger.info(f"  - group_column: {group_column}")
        logger.info(f"  - value_column: {value_column}")
        logger.info(f"  - operation: {operation}")
        logger.info(f"  - selected_columns: {selected_columns}")

        # æ­¥é©Ÿ1: éæ¿¾æ•¸æ“šä¸¦é¸æ“‡åˆ—
        filter_result = await filter_data_tool(
            file_path,
            filter_conditions,
            session_id,
            save_filtered_data=True,
            selected_columns=selected_columns
        )

        filter_data = json.loads(filter_result)
        if not filter_data.get('success', False):
            return filter_result  # è¿”å›éæ¿¾éŒ¯èª¤

        logger.info(f"âœ… éæ¿¾å®Œæˆ: {filter_data.get('filtered_rows', 0)} è¡Œ")

        # æ­¥é©Ÿ2: å°éæ¿¾å¾Œçš„æ•¸æ“šé€²è¡Œåˆ†çµ„åˆ†æ
        analysis_result = await group_by_analysis_tool(
            "@current",  # ä½¿ç”¨éæ¿¾å¾Œçš„æ•¸æ“š
            group_column,
            value_column,
            operation,
            session_id,
            data_source="current"
        )

        analysis_data = json.loads(analysis_result)
        if not analysis_data.get('success', False):
            return analysis_result  # è¿”å›åˆ†æéŒ¯èª¤

        # æ­¥é©Ÿ3: çµ„åˆçµæœ
        combined_result = {
            "success": True,
            "tool_type": "filter_and_analyze",
            "filter_info": {
                "original_rows": filter_data.get('original_rows', 0),
                "filtered_rows": filter_data.get('filtered_rows', 0),
                "selected_columns": json.loads(selected_columns) if selected_columns else "all",
                "filter_conditions": json.loads(filter_conditions)
            },
            "analysis_info": {
                "group_column": group_column,
                "value_column": value_column,
                "operation": operation
            },
            "results": analysis_data.get('results', {}),
            "summary": analysis_data.get('summary', {}),
            "temp_file_path": filter_data.get('temp_file_path'),
            "message": f"æˆåŠŸéæ¿¾ {filter_data.get('filtered_rows', 0)} è¡Œæ•¸æ“šä¸¦å®Œæˆ {operation} åˆ†æ"
        }

        logger.info(f"âœ… filter_and_analyze_tool åŸ·è¡Œå®Œæˆ")
        return json.dumps(combined_result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ éæ¿¾åˆ†æçµ„åˆå·¥å…·å¤±æ•—: {e}")
        import traceback
        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def create_data_file_tool(file_path: str, data: str, file_type: str = "csv", session_id: str = "default") -> str:
    """
    å‰µå»ºæ–°çš„æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        data: æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
        file_type: æ–‡ä»¶é¡å‹ (csv, json, xlsx)
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£ææ•¸æ“š
        data_dict = json.loads(data)

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # å‰µå»ºDataFrame
        if isinstance(data_dict, list):
            df = pd.DataFrame(data_dict)
        elif isinstance(data_dict, dict):
            df = pd.DataFrame([data_dict])
        else:
            return f'{{"success": false, "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹ä¿å­˜
        if file_type.lower() == 'csv':
            df.to_csv(file_path, index=False, encoding='utf-8')
        elif file_type.lower() == 'json':
            df.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_type.lower() == 'xlsx':
            df.to_excel(file_path, index=False)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file_type}"}}'

        result = {
            "success": True,
            "file_path": file_path,
            "file_type": file_type,
            "rows_created": len(df),
            "columns": list(df.columns)
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def update_data_rows_tool(file_path: str, update_conditions: str, new_values: str, session_id: str = "default") -> str:
    """
    æ›´æ–°æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        update_conditions: æ›´æ–°æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        new_values: æ–°å€¼çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        æ›´æ–°çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£ææ¢ä»¶å’Œæ–°å€¼
        conditions = json.loads(update_conditions)
        values = json.loads(new_values)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨æ›´æ–°æ¢ä»¶
        mask = pd.Series([True] * len(df))

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in df.columns:
                continue

            if operator == '==':
                mask &= (df[column] == value)
            elif operator == '!=':
                mask &= (df[column] != value)
            elif operator == '>':
                mask &= (df[column] > value)
            elif operator == '<':
                mask &= (df[column] < value)
            elif operator == 'contains':
                mask &= df[column].str.contains(str(value), na=False)

        # æ›´æ–°æ•¸æ“š
        updated_rows = mask.sum()

        for column, new_value in values.items():
            if column in df.columns:
                df.loc[mask, column] = new_value

        # ä¿å­˜æ–‡ä»¶
        if file_ext == '.csv':
            df.to_csv(file_path, index=False, encoding='utf-8')
        elif file_ext == '.json':
            df.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_ext == '.xlsx':
            df.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "updated_rows": int(updated_rows),
            "total_rows": len(df),
            "update_conditions": conditions,
            "new_values": values
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def delete_data_rows_tool(file_path: str, delete_conditions: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        delete_conditions: åˆªé™¤æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£æåˆªé™¤æ¢ä»¶
        conditions = json.loads(delete_conditions)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        original_rows = len(df)

        # æ‡‰ç”¨åˆªé™¤æ¢ä»¶
        mask = pd.Series([False] * len(df))

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in df.columns:
                continue

            if operator == '==':
                mask |= (df[column] == value)
            elif operator == '!=':
                mask |= (df[column] != value)
            elif operator == '>':
                mask |= (df[column] > value)
            elif operator == '<':
                mask |= (df[column] < value)
            elif operator == 'contains':
                mask |= df[column].str.contains(str(value), na=False)

        # åˆªé™¤æ•¸æ“š
        df_filtered = df[~mask]
        deleted_rows = original_rows - len(df_filtered)

        # ä¿å­˜æ–‡ä»¶
        if file_ext == '.csv':
            df_filtered.to_csv(file_path, index=False, encoding='utf-8')
        elif file_ext == '.json':
            df_filtered.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_ext == '.xlsx':
            df_filtered.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "deleted_rows": int(deleted_rows),
            "remaining_rows": len(df_filtered),
            "original_rows": original_rows,
            "delete_conditions": conditions
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# æ·»åŠ æ›´å¤šå·¥å…·
@tool
async def highlight_file_sections_tool(file_path: str, ranges: str, session_id: str = "default") -> str:
    """
    é«˜äº®æ–‡ä»¶å€åŸŸ

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        ranges: ç¯„åœåˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        é«˜äº®çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        ranges_list = json.loads(ranges)
        result = await local_file_tools.highlight_file_sections(file_path, ranges_list, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é«˜äº®æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def save_file_tool(file_path: str, content: str, encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    ä¿å­˜æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        ä¿å­˜çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.save_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def create_file_tool(file_path: str, content: str = "", encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    å‰µå»ºæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.create_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def correlation_analysis_tool(file_path: str, x_column: str, y_column: str, session_id: str = "default") -> str:
    """
    ç›¸é—œæ€§åˆ†æ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        session_id: æœƒè©±ID

    Returns:
        ç›¸é—œæ€§åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.correlation_analysis(file_path, x_column, y_column, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç›¸é—œæ€§åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def linear_prediction_tool(file_path: str, x_column: str, y_column: str, target_x_value: float, session_id: str = "default") -> str:
    """
    ç·šæ€§é æ¸¬

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        target_x_value: ç›®æ¨™Xå€¼
        session_id: æœƒè©±ID

    Returns:
        é æ¸¬çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.linear_prediction(file_path, x_column, y_column, target_x_value, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·šæ€§é æ¸¬å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def edit_data_file_tool(file_path: str, operation: str, data: str, session_id: str = "default") -> str:
    """
    ç·¨è¼¯æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        operation: æ“ä½œé¡å‹ (add_row, delete_row, update_cell, update_row)
        data: æ“ä½œæ•¸æ“šçš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [edit_data_file_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸:")
    logger.info(f"   - file_path: '{file_path}'")
    logger.info(f"   - operation: '{operation}'")
    logger.info(f"   - data: '{data}'")
    logger.info(f"   - session_id: '{session_id}'")

    try:
        # æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹")
        valid_operations = ['add_row', 'delete_row', 'update_cell', 'update_row']
        if operation not in valid_operations:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œé¡å‹: {operation}ï¼Œæ”¯æŒçš„æ“ä½œ: {valid_operations}")
        logger.info(f"âœ“ æ“ä½œé¡å‹æœ‰æ•ˆ: {operation}")

        # æ­¥é©Ÿ2: è§£ææ•¸æ“š
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è§£ææ“ä½œæ•¸æ“š")
        import json
        try:
            parsed_data = json.loads(data)
            logger.info(f"âœ“ æ•¸æ“šè§£ææˆåŠŸ: {type(parsed_data)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}")

        # æ­¥é©Ÿ3: èª¿ç”¨ç·¨è¼¯å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: èª¿ç”¨ data_file_tools.edit_data_file")
        result = await data_file_tools.edit_data_file(file_path, operation, data, session_id)

        # æ­¥é©Ÿ4: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ4: è™•ç†ç·¨è¼¯çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            logger.info(f"âœ… ç·¨è¼¯æ“ä½œæˆåŠŸå®Œæˆ")
            if 'affected_rows' in result:
                logger.info(f"ğŸ“Š å½±éŸ¿è¡Œæ•¸: {result['affected_rows']}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [edit_data_file_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [edit_data_file_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def delete_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.delete_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# ç²å–æ‰€æœ‰æœ¬åœ°æ–‡ä»¶å·¥å…·
def get_langchain_local_file_tools() -> List:
    """ç²å–æ‰€æœ‰ LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·"""
    tools = [
        # åŸºæœ¬æ–‡ä»¶æ“ä½œå·¥å…·
        read_file_with_summary_tool,
        edit_file_by_lines_tool,
        highlight_file_sections_tool,
        save_file_tool,
        create_file_tool,
        delete_file_tool,
        # æ•¸æ“šæ–‡ä»¶å·¥å…·
        read_data_file_tool,
        edit_data_file_tool,
        # æ•¸æ“šåˆ†æå·¥å…·
        get_data_info_tool,
        group_by_analysis_tool,
        threshold_analysis_tool,
        correlation_analysis_tool,
        linear_prediction_tool,

        # æ–°å¢çš„æ•¸æ“šCRUDå·¥å…·
        filter_data_tool,
        filter_and_analyze_tool,
        cleanup_temp_files_tool,
        get_session_data_status_tool,
        clear_session_data_tool,
        suggest_analysis_operation_tool,
        create_data_file_tool,
        update_data_rows_tool,
        delete_data_rows_tool,
    ]

    # æ·»åŠ æ“´å±•å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if EXTENDED_TOOLS_AVAILABLE:
        try:
            # æ·»åŠ  Task Memory å·¥å…·
            tools.extend(get_langchain_task_memory_tools())
            logger.info("âœ… Task Memory å·¥å…·å·²æ·»åŠ ")

            # ç¹ªåœ–å·¥å…·å·²ç§»é™¤ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒæ–‡ä»¶æ“ä½œåŠŸèƒ½
            logger.info("âš ï¸ ç¹ªåœ–å·¥å…·å·²ç§»é™¤")

            # æ·»åŠ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·
            tools.extend(get_batch_processor_tools())
            logger.info("âœ… æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·å·²æ·»åŠ ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ æ“´å±•å·¥å…·å¤±æ•—: {e}")

    return tools
