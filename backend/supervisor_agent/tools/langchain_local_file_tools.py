"""
LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·
æä¾›æ¨™æº–çš„ LangChain tool æ ¼å¼
"""

import logging
import sys
from pathlib import Path
from typing import Dict, Any, List
from langchain_core.tools import tool

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å·¥å…·
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

# å°å…¥åŸå§‹å·¥å…·å‡½æ•¸
from tools.local_file_tools import local_file_tools
from tools.data_file_tools import data_file_tools
from tools.data_analysis_tools import data_analysis_tools

logger = logging.getLogger(__name__)

# å°å…¥æ–°çš„å·¥å…·æ¨¡çµ„
# åˆå§‹åŒ–æ“´å±•å·¥å…·å‡½æ•¸
get_langchain_task_memory_tools = lambda: []
get_langchain_plotting_tools = lambda: []
get_batch_processor_tools = lambda: []

EXTENDED_TOOLS_AVAILABLE = False

try:
    # ä½¿ç”¨çµ•å°å°å…¥é¿å…ç›¸å°å°å…¥å•é¡Œ
    import importlib.util

    # è·³étask_memory_toolsçš„å°å…¥ï¼Œå› ç‚ºå®ƒä¾è³´è¤‡é›œçš„å¤–éƒ¨å­˜å„²ç³»çµ±
    # åœ¨ç°¡åŒ–ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å€‘å°ˆæ³¨æ–¼æ ¸å¿ƒçš„æ–‡ä»¶æ“ä½œå’Œæ•¸æ“šè™•ç†åŠŸèƒ½
    logger.info("âš ï¸ è·³éTask Memoryå·¥å…·å°å…¥ï¼ˆé¿å…è¤‡é›œä¾è³´ï¼‰")
    logger.info("âš ï¸ è·³éPlottingå·¥å…·å°å…¥ï¼ˆå°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½ï¼‰")

    # å‹•æ…‹å°å…¥batch_processor_tool
    batch_path = current_dir / "langchain_batch_processor_tool.py"
    if batch_path.exists():
        try:
            spec = importlib.util.spec_from_file_location("batch_processor_tool", batch_path)
            batch_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(batch_module)
            get_batch_processor_tools = batch_module.get_batch_processor_tools
            logger.info("âœ… Batch Processorå·¥å…·å°å…¥æˆåŠŸ")
            EXTENDED_TOOLS_AVAILABLE = True  # è‡³å°‘batch processoræˆåŠŸäº†
        except Exception as e:
            logger.warning(f"âš ï¸ Batch Processorå·¥å…·å°å…¥å¤±æ•—: {e}")

except Exception as e:
    logger.warning(f"æ“´å±•å·¥å…·å°å…¥éç¨‹å¤±æ•—: {e}")

@tool
async def read_file_with_summary_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åŒ…å«æ–‡ä»¶å…§å®¹å’Œæ‘˜è¦çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [read_file_with_summary_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: èª¿ç”¨åº•å±¤å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: èª¿ç”¨ local_file_tools.read_file_with_summary")
        result = await local_file_tools.read_file_with_summary(file_path, session_id)

        # æ­¥é©Ÿ2: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è™•ç†å·¥å…·è¿”å›çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [read_file_with_summary_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [read_file_with_summary_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def edit_file_by_lines_tool(file_path: str, start_line: int, end_line: int, 
                                 new_content: str, session_id: str = "default") -> str:
    """
    æŒ‰è¡Œç·¨è¼¯æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        start_line: é–‹å§‹è¡Œè™Ÿ
        end_line: çµæŸè¡Œè™Ÿ
        new_content: æ–°å…§å®¹
        session_id: æœƒè©±ID
        
    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.edit_file_by_lines(file_path, start_line, end_line, new_content, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·¨è¼¯æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def get_data_info_tool(file_path: str, session_id: str = "default") -> str:
    """
    æ™ºèƒ½ç²å–æ–‡ä»¶ä¿¡æ¯ - è‡ªå‹•åˆ¤æ–·æ–‡ä»¶é¡å‹ä¸¦ä½¿ç”¨ç›¸æ‡‰çš„è™•ç†æ–¹å¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ–‡ä»¶ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [get_data_info_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        import os
        from pathlib import Path
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_size = os.path.getsize(file_path)
        file_ext = Path(file_path).suffix.lower()
        logger.info(f"âœ“ æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} bytesï¼Œå‰¯æª”å: {file_ext}")

        # æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: åˆ¤æ–·æ–‡ä»¶é¡å‹")
        data_file_extensions = ['.csv', '.json', '.xlsx', '.xls', '.parquet']
        text_file_extensions = ['.txt', '.md', '.py', '.js', '.html', '.css', '.xml', '.yaml', '.yml']

        if file_ext in data_file_extensions:
            # æ•¸æ“šæ–‡ä»¶ - ä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·
            logger.info(f"ğŸ“Š è­˜åˆ¥ç‚ºæ•¸æ“šæ–‡ä»¶ï¼Œä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·")
            result = await data_analysis_tools.get_data_info(file_path, session_id)

        elif file_ext in text_file_extensions:
            # æ–‡æœ¬æ–‡ä»¶ - ä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·ç”Ÿæˆæ‘˜è¦
            logger.info(f"ğŸ“„ è­˜åˆ¥ç‚ºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·")
            file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)

            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            result = {
                "success": True,
                "file_type": "text_file",
                "file_path": file_path,
                "file_size": file_size,
                "file_extension": file_ext,
                "summary": file_summary,
                "analysis_type": "text_file_summary"
            }

        else:
            # æœªçŸ¥æ–‡ä»¶é¡å‹ - å˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†
            logger.info(f"âš ï¸ æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†")
            try:
                file_summary = await local_file_tools.read_file_with_summary(file_path, session_id)
                result = {
                    "success": True,
                    "file_type": "unknown_text_file",
                    "file_path": file_path,
                    "file_size": file_size,
                    "file_extension": file_ext,
                    "summary": file_summary,
                    "analysis_type": "text_file_summary",
                    "warning": f"æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå·²ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†"
                }
            except Exception as text_error:
                # å®Œå…¨ç„¡æ³•è™•ç†
                result = {
                    "success": False,
                    "error": f"ç„¡æ³•è™•ç†æ–‡ä»¶é¡å‹ {file_ext}ï¼Œæ—¢ä¸æ˜¯æ”¯æŒçš„æ•¸æ“šæ ¼å¼ï¼Œä¹Ÿç„¡æ³•ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–: {str(text_error)}",
                    "file_path": file_path,
                    "file_extension": file_ext,
                    "supported_data_formats": data_file_extensions,
                    "supported_text_formats": text_file_extensions
                }

        # æ­¥é©Ÿ3: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: è™•ç†åˆ†æçµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            if result.get('file_type') == 'text_file':
                logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æ‘˜è¦å·²ç”Ÿæˆ")
            else:
                data_shape = result.get('data_shape', [0, 0])
                logger.info(f"ğŸ“Š æ•¸æ“šå½¢ç‹€: {data_shape[0]} è¡Œ Ã— {data_shape[1]} åˆ—")
                logger.info(f"ğŸ“Š æ•¸å€¼åˆ—: {result.get('numeric_columns', [])}")
                logger.info(f"ğŸ“Š åˆ†é¡åˆ—: {result.get('categorical_columns', [])}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [get_data_info_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [get_data_info_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def group_by_analysis_tool(file_path: str, group_column: str, value_column: str,
                                operation: str = "sum", session_id: str = "default") -> str:
    """
    é€šç”¨åˆ†çµ„åˆ†æå·¥å…·
    
    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ (mean, sum, count, min, max)
        session_id: æœƒè©±ID
        
    Returns:
        åˆ†çµ„åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.group_by_analysis(file_path, group_column, value_column, operation, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆ†çµ„åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def threshold_analysis_tool(file_path: str, value_column: str, threshold: float, 
                                 comparison: str = "greater", session_id: str = "default") -> str:
    """
    é€šç”¨é–¾å€¼åˆ†æå·¥å…·
    
    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        value_column: æ•¸å€¼åˆ—å
        threshold: é–¾å€¼
        comparison: æ¯”è¼ƒæ–¹å¼ (greater, less, equal)
        session_id: æœƒè©±ID
        
    Returns:
        é–¾å€¼åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.threshold_analysis(file_path, value_column, threshold, comparison, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é–¾å€¼åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def read_data_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_file_tools.read_data_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ è®€å–æ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def filter_data_tool(file_path: str, filter_conditions: str, session_id: str = "default") -> str:
    """
    æ ¹æ“šæ¢ä»¶éæ¿¾æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        filter_conditions: éæ¿¾æ¢ä»¶çš„JSONå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: {"column": "age", "operator": ">", "value": 25}
        session_id: æœƒè©±ID

    Returns:
        éæ¿¾å¾Œçš„æ•¸æ“šJSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£æéæ¿¾æ¢ä»¶
        conditions = json.loads(filter_conditions)

        # è®€å–æ•¸æ“šæ–‡ä»¶
        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹è®€å–
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        if isinstance(conditions, dict):
            conditions = [conditions]  # è½‰æ›ç‚ºåˆ—è¡¨

        filtered_df = df.copy()

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in filtered_df.columns:
                continue

            if operator == '>':
                filtered_df = filtered_df[filtered_df[column] > value]
            elif operator == '<':
                filtered_df = filtered_df[filtered_df[column] < value]
            elif operator == '>=':
                filtered_df = filtered_df[filtered_df[column] >= value]
            elif operator == '<=':
                filtered_df = filtered_df[filtered_df[column] <= value]
            elif operator == '==':
                filtered_df = filtered_df[filtered_df[column] == value]
            elif operator == '!=':
                filtered_df = filtered_df[filtered_df[column] != value]
            elif operator == 'contains':
                filtered_df = filtered_df[filtered_df[column].str.contains(str(value), na=False)]
            elif operator == 'in':
                filtered_df = filtered_df[filtered_df[column].isin(value)]

        # è¿”å›çµæœ
        result = {
            "success": True,
            "original_rows": len(df),
            "filtered_rows": len(filtered_df),
            "data": filtered_df.to_dict('records'),
            "columns": list(filtered_df.columns),
            "filter_conditions": conditions
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šéæ¿¾å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def create_data_file_tool(file_path: str, data: str, file_type: str = "csv", session_id: str = "default") -> str:
    """
    å‰µå»ºæ–°çš„æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        data: æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
        file_type: æ–‡ä»¶é¡å‹ (csv, json, xlsx)
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£ææ•¸æ“š
        data_dict = json.loads(data)

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # å‰µå»ºDataFrame
        if isinstance(data_dict, list):
            df = pd.DataFrame(data_dict)
        elif isinstance(data_dict, dict):
            df = pd.DataFrame([data_dict])
        else:
            return f'{{"success": false, "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹ä¿å­˜
        if file_type.lower() == 'csv':
            df.to_csv(file_path, index=False, encoding='utf-8')
        elif file_type.lower() == 'json':
            df.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_type.lower() == 'xlsx':
            df.to_excel(file_path, index=False)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file_type}"}}'

        result = {
            "success": True,
            "file_path": file_path,
            "file_type": file_type,
            "rows_created": len(df),
            "columns": list(df.columns)
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def update_data_rows_tool(file_path: str, update_conditions: str, new_values: str, session_id: str = "default") -> str:
    """
    æ›´æ–°æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        update_conditions: æ›´æ–°æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        new_values: æ–°å€¼çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        æ›´æ–°çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£ææ¢ä»¶å’Œæ–°å€¼
        conditions = json.loads(update_conditions)
        values = json.loads(new_values)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨æ›´æ–°æ¢ä»¶
        mask = pd.Series([True] * len(df))

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in df.columns:
                continue

            if operator == '==':
                mask &= (df[column] == value)
            elif operator == '!=':
                mask &= (df[column] != value)
            elif operator == '>':
                mask &= (df[column] > value)
            elif operator == '<':
                mask &= (df[column] < value)
            elif operator == 'contains':
                mask &= df[column].str.contains(str(value), na=False)

        # æ›´æ–°æ•¸æ“š
        updated_rows = mask.sum()

        for column, new_value in values.items():
            if column in df.columns:
                df.loc[mask, column] = new_value

        # ä¿å­˜æ–‡ä»¶
        if file_ext == '.csv':
            df.to_csv(file_path, index=False, encoding='utf-8')
        elif file_ext == '.json':
            df.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_ext == '.xlsx':
            df.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "updated_rows": int(updated_rows),
            "total_rows": len(df),
            "update_conditions": conditions,
            "new_values": values
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def delete_data_rows_tool(file_path: str, delete_conditions: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        delete_conditions: åˆªé™¤æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£æåˆªé™¤æ¢ä»¶
        conditions = json.loads(delete_conditions)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == '.csv':
            df = pd.read_csv(file_path)
        elif file_ext == '.json':
            df = pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        original_rows = len(df)

        # æ‡‰ç”¨åˆªé™¤æ¢ä»¶
        mask = pd.Series([False] * len(df))

        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')

            if column not in df.columns:
                continue

            if operator == '==':
                mask |= (df[column] == value)
            elif operator == '!=':
                mask |= (df[column] != value)
            elif operator == '>':
                mask |= (df[column] > value)
            elif operator == '<':
                mask |= (df[column] < value)
            elif operator == 'contains':
                mask |= df[column].str.contains(str(value), na=False)

        # åˆªé™¤æ•¸æ“š
        df_filtered = df[~mask]
        deleted_rows = original_rows - len(df_filtered)

        # ä¿å­˜æ–‡ä»¶
        if file_ext == '.csv':
            df_filtered.to_csv(file_path, index=False, encoding='utf-8')
        elif file_ext == '.json':
            df_filtered.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
        elif file_ext == '.xlsx':
            df_filtered.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "deleted_rows": int(deleted_rows),
            "remaining_rows": len(df_filtered),
            "original_rows": original_rows,
            "delete_conditions": conditions
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# æ·»åŠ æ›´å¤šå·¥å…·
@tool
async def highlight_file_sections_tool(file_path: str, ranges: str, session_id: str = "default") -> str:
    """
    é«˜äº®æ–‡ä»¶å€åŸŸ

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        ranges: ç¯„åœåˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        é«˜äº®çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        ranges_list = json.loads(ranges)
        result = await local_file_tools.highlight_file_sections(file_path, ranges_list, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é«˜äº®æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def save_file_tool(file_path: str, content: str, encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    ä¿å­˜æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        ä¿å­˜çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.save_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def create_file_tool(file_path: str, content: str = "", encoding: str = "utf-8", session_id: str = "default") -> str:
    """
    å‰µå»ºæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.create_file(file_path, content, encoding, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def correlation_analysis_tool(file_path: str, x_column: str, y_column: str, session_id: str = "default") -> str:
    """
    ç›¸é—œæ€§åˆ†æ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        session_id: æœƒè©±ID

    Returns:
        ç›¸é—œæ€§åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.correlation_analysis(file_path, x_column, y_column, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç›¸é—œæ€§åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def linear_prediction_tool(file_path: str, x_column: str, y_column: str, target_x_value: float, session_id: str = "default") -> str:
    """
    ç·šæ€§é æ¸¬

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        target_x_value: ç›®æ¨™Xå€¼
        session_id: æœƒè©±ID

    Returns:
        é æ¸¬çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.linear_prediction(file_path, x_column, y_column, target_x_value, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·šæ€§é æ¸¬å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

@tool
async def edit_data_file_tool(file_path: str, operation: str, data: str, session_id: str = "default") -> str:
    """
    ç·¨è¼¯æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        operation: æ“ä½œé¡å‹ (add_row, delete_row, update_cell, update_row)
        data: æ“ä½œæ•¸æ“šçš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [edit_data_file_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸:")
    logger.info(f"   - file_path: '{file_path}'")
    logger.info(f"   - operation: '{operation}'")
    logger.info(f"   - data: '{data}'")
    logger.info(f"   - session_id: '{session_id}'")

    try:
        # æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹")
        valid_operations = ['add_row', 'delete_row', 'update_cell', 'update_row']
        if operation not in valid_operations:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œé¡å‹: {operation}ï¼Œæ”¯æŒçš„æ“ä½œ: {valid_operations}")
        logger.info(f"âœ“ æ“ä½œé¡å‹æœ‰æ•ˆ: {operation}")

        # æ­¥é©Ÿ2: è§£ææ•¸æ“š
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è§£ææ“ä½œæ•¸æ“š")
        import json
        try:
            parsed_data = json.loads(data)
            logger.info(f"âœ“ æ•¸æ“šè§£ææˆåŠŸ: {type(parsed_data)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}")

        # æ­¥é©Ÿ3: èª¿ç”¨ç·¨è¼¯å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: èª¿ç”¨ data_file_tools.edit_data_file")
        result = await data_file_tools.edit_data_file(file_path, operation, data, session_id)

        # æ­¥é©Ÿ4: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ4: è™•ç†ç·¨è¼¯çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get('success'):
            logger.info(f"âœ… ç·¨è¼¯æ“ä½œæˆåŠŸå®Œæˆ")
            if 'affected_rows' in result:
                logger.info(f"ğŸ“Š å½±éŸ¿è¡Œæ•¸: {result['affected_rows']}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [edit_data_file_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [edit_data_file_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result

@tool
async def delete_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.delete_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'

# ç²å–æ‰€æœ‰æœ¬åœ°æ–‡ä»¶å·¥å…·
def get_langchain_local_file_tools() -> List:
    """ç²å–æ‰€æœ‰ LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·"""
    tools = [
        # åŸºæœ¬æ–‡ä»¶æ“ä½œå·¥å…·
        read_file_with_summary_tool,
        edit_file_by_lines_tool,
        highlight_file_sections_tool,
        save_file_tool,
        create_file_tool,
        delete_file_tool,
        # æ•¸æ“šæ–‡ä»¶å·¥å…·
        read_data_file_tool,
        edit_data_file_tool,
        # æ•¸æ“šåˆ†æå·¥å…·
        get_data_info_tool,
        group_by_analysis_tool,
        threshold_analysis_tool,
        correlation_analysis_tool,
        linear_prediction_tool,

        # æ–°å¢çš„æ•¸æ“šCRUDå·¥å…·
        filter_data_tool,
        create_data_file_tool,
        update_data_rows_tool,
        delete_data_rows_tool,
    ]

    # æ·»åŠ æ“´å±•å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if EXTENDED_TOOLS_AVAILABLE:
        try:
            # æ·»åŠ  Task Memory å·¥å…·
            tools.extend(get_langchain_task_memory_tools())
            logger.info("âœ… Task Memory å·¥å…·å·²æ·»åŠ ")

            # ç¹ªåœ–å·¥å…·å·²ç§»é™¤ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒæ–‡ä»¶æ“ä½œåŠŸèƒ½
            logger.info("âš ï¸ ç¹ªåœ–å·¥å…·å·²ç§»é™¤")

            # æ·»åŠ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·
            tools.extend(get_batch_processor_tools())
            logger.info("âœ… æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·å·²æ·»åŠ ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ æ“´å±•å·¥å…·å¤±æ•—: {e}")

    return tools
