"""
LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·
æä¾›æ¨™æº–çš„ LangChain tool æ ¼å¼
"""

import logging
import sys
from pathlib import Path
from typing import Dict, Any, List
from langchain_core.tools import tool
from pydantic import Field
import json

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å·¥å…·
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

# å°å…¥åŸå§‹å·¥å…·å‡½æ•¸
from src.tools.local_file_tools import local_file_tools
from src.tools.data_file_tools import data_file_tools
from src.tools.data_analysis_tools import data_analysis_tools

# å°å…¥æœƒè©±æ•¸æ“šç®¡ç†å™¨
import sys
from pathlib import Path

core_dir = Path(__file__).parent.parent / "core"
sys.path.insert(0, str(core_dir))
try:
    from supervisor_agent.core.session_data_manager import session_data_manager
except ImportError:
    try:
        from session_data_manager import session_data_manager
    except ImportError:
        # å‰µå»ºä¸€å€‹ç°¡å–®çš„æ›¿ä»£å“
        class SimpleSessionManager:
            def get_temp_file_path(self, session_id: str, filename: str) -> str:
                from pathlib import Path
                temp_dir = Path("temp") / session_id
                temp_dir.mkdir(parents=True, exist_ok=True)
                return str(temp_dir / filename)
        session_data_manager = SimpleSessionManager()

logger = logging.getLogger(__name__)

# å°å…¥æ–°çš„å·¥å…·æ¨¡çµ„
# åˆå§‹åŒ–æ“´å±•å·¥å…·å‡½æ•¸
get_langchain_task_memory_tools = lambda: []
get_langchain_plotting_tools = lambda: []
get_batch_processor_tools = lambda: []

# å°å…¥ Gmail å°ˆç”¨æŒ‡ç´‹æœå°‹å·¥å…·
try:
    from .fingerprint_search_tool import fingerprint_search_csv
    FINGERPRINT_SEARCH_AVAILABLE = True
    logger.info("âœ… Gmail æŒ‡ç´‹æœå°‹å·¥å…·å°å…¥æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ Gmail æŒ‡ç´‹æœå°‹å·¥å…·å°å…¥å¤±æ•—: {e}")
    FINGERPRINT_SEARCH_AVAILABLE = False

# å°å…¥é€šç”¨æŒ‡ç´‹æœå°‹å·¥å…·
try:
    from .flexible_fingerprint_search_tool import flexible_fingerprint_search_csv
    FLEXIBLE_FINGERPRINT_SEARCH_AVAILABLE = True
    logger.info("âœ… é€šç”¨æŒ‡ç´‹æœå°‹å·¥å…·å°å…¥æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ é€šç”¨æŒ‡ç´‹æœå°‹å·¥å…·å°å…¥å¤±æ•—: {e}")
    FLEXIBLE_FINGERPRINT_SEARCH_AVAILABLE = False

# å°å…¥å¤šæª”æ¡ˆåˆ†æå·¥å…·
try:
    from .multi_file_analysis_tools import (
        multi_file_reader_tool,
        multi_file_filter_tool,
        multi_file_analyzer_tool,
        multi_file_data_analyzer_tool
    )
    MULTI_FILE_TOOLS_AVAILABLE = True
    logger.info("âœ… å¤šæª”æ¡ˆåˆ†æå·¥å…·å°å…¥æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ å¤šæª”æ¡ˆåˆ†æå·¥å…·å°å…¥å¤±æ•—: {e}")
    MULTI_FILE_TOOLS_AVAILABLE = False

# å°å…¥ CSV æ ¼å¼è½‰æ›å·¥å…·
try:
    src_dir = Path(__file__).parent.parent.parent / "src" / "tools"
    sys.path.insert(0, str(src_dir))
    from csv_format_converter import convert_gmail_csv_format, preview_gmail_csv_conversion
    CSV_CONVERTER_AVAILABLE = True
    logger.info("âœ… CSV æ ¼å¼è½‰æ›å·¥å…·å°å…¥æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ CSV æ ¼å¼è½‰æ›å·¥å…·å°å…¥å¤±æ•—: {e}")
    CSV_CONVERTER_AVAILABLE = False

EXTENDED_TOOLS_AVAILABLE = False

try:
    # ä½¿ç”¨çµ•å°å°å…¥é¿å…ç›¸å°å°å…¥å•é¡Œ
    import importlib.util

    # è·³étask_memory_toolsçš„å°å…¥ï¼Œå› ç‚ºå®ƒä¾è³´è¤‡é›œçš„å¤–éƒ¨å­˜å„²ç³»çµ±
    # åœ¨ç°¡åŒ–ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å€‘å°ˆæ³¨æ–¼æ ¸å¿ƒçš„æ–‡ä»¶æ“ä½œå’Œæ•¸æ“šè™•ç†åŠŸèƒ½
    logger.info("âš ï¸ è·³éTask Memoryå·¥å…·å°å…¥ï¼ˆé¿å…è¤‡é›œä¾è³´ï¼‰")
    logger.info("âš ï¸ è·³éPlottingå·¥å…·å°å…¥ï¼ˆå°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½ï¼‰")

    # å‹•æ…‹å°å…¥batch_processor_tool
    batch_path = current_dir / "langchain_batch_processor_tool.py"
    if batch_path.exists():
        try:
            spec = importlib.util.spec_from_file_location(
                "batch_processor_tool", batch_path
            )
            batch_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(batch_module)
            get_batch_processor_tools = batch_module.get_batch_processor_tools
            logger.info("âœ… Batch Processorå·¥å…·å°å…¥æˆåŠŸ")
            EXTENDED_TOOLS_AVAILABLE = True  # è‡³å°‘batch processoræˆåŠŸäº†
        except Exception as e:
            logger.warning(f"âš ï¸ Batch Processorå·¥å…·å°å…¥å¤±æ•—: {e}")

except Exception as e:
    logger.warning(f"æ“´å±•å·¥å…·å°å…¥éç¨‹å¤±æ•—: {e}")


@tool
async def read_file_with_summary_tool(
    file_path: str, session_id: str = "default"
) -> str:
    """
    è®€å–æ–‡ä»¶ä¸¦ç”Ÿæˆæ‘˜è¦

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åŒ…å«æ–‡ä»¶å…§å®¹å’Œæ‘˜è¦çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [read_file_with_summary_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: èª¿ç”¨åº•å±¤å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: èª¿ç”¨ local_file_tools.read_file_with_summary")
        result = await local_file_tools.read_file_with_summary(file_path, session_id)

        # æ­¥é©Ÿ2: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è™•ç†å·¥å…·è¿”å›çµæœ")
        result_str = str(result)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [read_file_with_summary_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [read_file_with_summary_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result


@tool
async def edit_file_by_lines_tool(
    file_path: str,
    start_line: int,
    end_line: int,
    new_content: str,
    session_id: str = "default",
) -> str:
    """
    æŒ‰è¡Œç·¨è¼¯æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        start_line: é–‹å§‹è¡Œè™Ÿ
        end_line: çµæŸè¡Œè™Ÿ
        new_content: æ–°å…§å®¹
        session_id: æœƒè©±ID

    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.edit_file_by_lines(
            file_path, start_line, end_line, new_content, session_id
        )
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ ç·¨è¼¯æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def get_data_info_tool(file_path: str, session_id: str = "default") -> str:
    """
    æ™ºèƒ½ç²å–æ–‡ä»¶ä¿¡æ¯ - è‡ªå‹•åˆ¤æ–·æ–‡ä»¶é¡å‹ä¸¦ä½¿ç”¨ç›¸æ‡‰çš„è™•ç†æ–¹å¼

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ–‡ä»¶ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [get_data_info_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸: file_path='{file_path}', session_id='{session_id}'")
    print(f"DEBUG: get_data_info_tool called with file_path='{file_path}', session_id='{session_id}'")

    try:
        # æ­¥é©Ÿ1: æª¢æŸ¥æ˜¯å¦ç‚ºåˆä½µè³‡æ–™é›†çš„è™›æ“¬è·¯å¾‘
        import os
        from pathlib import Path

        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: æª¢æŸ¥æ–‡ä»¶é¡å‹")

        # ç§»é™¤èˆŠçš„è™›æ“¬è·¯å¾‘è™•ç†é‚è¼¯ï¼Œç¾åœ¨ç›´æ¥è™•ç†å¯¦éš›æª”æ¡ˆ

        # æª¢æŸ¥å¯¦éš›æ–‡ä»¶å­˜åœ¨æ€§
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: æª¢æŸ¥å¯¦éš›æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_size = os.path.getsize(file_path)
        file_ext = Path(file_path).suffix.lower()
        logger.info(f"âœ“ æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} bytesï¼Œå‰¯æª”å: {file_ext}")

        # æ­¥é©Ÿ3: åˆ¤æ–·æ–‡ä»¶é¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: åˆ¤æ–·æ–‡ä»¶é¡å‹")
        data_file_extensions = [".csv", ".json", ".xlsx", ".xls", ".parquet"]
        text_file_extensions = [
            ".txt",
            ".md",
            ".py",
            ".js",
            ".html",
            ".css",
            ".xml",
            ".yaml",
            ".yml",
        ]

        if file_ext in data_file_extensions:
            # æ•¸æ“šæ–‡ä»¶ - ä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·
            logger.info(f"ğŸ“Š è­˜åˆ¥ç‚ºæ•¸æ“šæ–‡ä»¶ï¼Œä½¿ç”¨æ•¸æ“šåˆ†æå·¥å…·")
            result = await data_analysis_tools.get_data_info(file_path, session_id)

        elif file_ext in text_file_extensions:
            # æ–‡æœ¬æ–‡ä»¶ - ä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·ç”Ÿæˆæ‘˜è¦
            logger.info(f"ğŸ“„ è­˜åˆ¥ç‚ºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è®€å–å·¥å…·")
            file_summary = await local_file_tools.read_file_with_summary(
                file_path, session_id
            )

            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            result = {
                "success": True,
                "file_type": "text_file",
                "file_path": file_path,
                "file_size": file_size,
                "file_extension": file_ext,
                "summary": file_summary,
                "analysis_type": "text_file_summary",
            }

        else:
            # æœªçŸ¥æ–‡ä»¶é¡å‹ - å˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†
            logger.info(f"âš ï¸ æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå˜—è©¦ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†")
            try:
                file_summary = await local_file_tools.read_file_with_summary(
                    file_path, session_id
                )
                result = {
                    "success": True,
                    "file_type": "unknown_text_file",
                    "file_path": file_path,
                    "file_size": file_size,
                    "file_extension": file_ext,
                    "summary": file_summary,
                    "analysis_type": "text_file_summary",
                    "warning": f"æœªçŸ¥æ–‡ä»¶é¡å‹ {file_ext}ï¼Œå·²ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è™•ç†",
                }
            except Exception as text_error:
                # å®Œå…¨ç„¡æ³•è™•ç†
                result = {
                    "success": False,
                    "error": f"ç„¡æ³•è™•ç†æ–‡ä»¶é¡å‹ {file_ext}ï¼Œæ—¢ä¸æ˜¯æ”¯æŒçš„æ•¸æ“šæ ¼å¼ï¼Œä¹Ÿç„¡æ³•ä½œç‚ºæ–‡æœ¬æ–‡ä»¶è®€å–: {str(text_error)}",
                    "file_path": file_path,
                    "file_extension": file_ext,
                    "supported_data_formats": data_file_extensions,
                    "supported_text_formats": text_file_extensions,
                }

        # æ­¥é©Ÿ3: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: è™•ç†åˆ†æçµæœ")
        result_str = json.dumps(result, ensure_ascii=False)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get("success"):
            if result.get("file_type") == "text_file":
                logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æ‘˜è¦å·²ç”Ÿæˆ")
            else:
                data_shape = result.get("data_shape", [0, 0])
                logger.info(f"ğŸ“Š æ•¸æ“šå½¢ç‹€: {data_shape[0]} è¡Œ Ã— {data_shape[1]} åˆ—")
                logger.info(f"ğŸ“Š æ•¸å€¼åˆ—: {result.get('numeric_columns', [])}")
                logger.info(f"ğŸ“Š åˆ†é¡åˆ—: {result.get('categorical_columns', [])}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [get_data_info_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [get_data_info_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result


@tool
async def group_by_analysis_tool(
    file_path: str,
    group_column: str,
    value_column: str,
    operation: str = "sum",
    session_id: str = "default",
    data_source: str = "file",
) -> str:
    """
    é€šç”¨åˆ†çµ„åˆ†æå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘ï¼Œæ”¯æŒç‰¹æ®Šå€¼ "@current" ä½¿ç”¨ç•¶å‰æœƒè©±çš„æœ€æ–°æ•¸æ“š
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ï¼Œæ ¹æ“šåˆ†æéœ€æ±‚é¸æ“‡ï¼š
                  - "mean": å¹³å‡å€¼ï¼ˆè–ªè³‡åˆ†æã€ç¸¾æ•ˆè©•ä¼°ï¼‰
                  - "sum": ç¸½å’Œï¼ˆéŠ·å”®é¡ã€æ•¸é‡çµ±è¨ˆï¼‰
                  - "count": è¨ˆæ•¸ï¼ˆäººå“¡çµ±è¨ˆã€é »æ¬¡åˆ†æï¼‰
                  - "max": æœ€å¤§å€¼ï¼ˆæœ€é«˜è–ªè³‡ã€å³°å€¼åˆ†æï¼‰
                  - "min": æœ€å°å€¼ï¼ˆæœ€ä½è–ªè³‡ã€åŸºæº–åˆ†æï¼‰
        session_id: æœƒè©±ID
        data_source: æ•¸æ“šæºé¡å‹ ("file": å¾æ–‡ä»¶åŠ è¼‰, "current": ä½¿ç”¨ç•¶å‰æœƒè©±æ•¸æ“š)

    Returns:
        åˆ†çµ„åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        logger.info(f"ğŸ”„ group_by_analysis_tool é–‹å§‹åŸ·è¡Œ:")
        logger.info(f"  - åŸå§‹ file_path: {file_path}")
        logger.info(f"  - group_column: {group_column}")
        logger.info(f"  - value_column: {value_column}")
        logger.info(f"  - operation: {operation}")
        logger.info(f"  - session_id: {session_id}")
        logger.info(f"  - data_source: {data_source}")

        # æ ¹æ“š data_source åƒæ•¸æ±ºå®šæ•¸æ“šä¾†æº
        if data_source == "current" or file_path in ["@current", "current", "latest"]:
            # ä½¿ç”¨æœƒè©±æ•¸æ“šç®¡ç†å™¨è§£æè·¯å¾‘
            resolved_file_path = session_data_manager.resolve_file_path(
                session_id, file_path
            )
            logger.info(f"ğŸ”„ ä½¿ç”¨æœƒè©±æ•¸æ“š: {file_path} -> {resolved_file_path}")
        else:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„æ–‡ä»¶è·¯å¾‘
            resolved_file_path = file_path
            logger.info(f"ğŸ”„ ä½¿ç”¨æŒ‡å®šæ–‡ä»¶: {resolved_file_path}")

        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–æ˜¯å¦ç‚ºå¤šæª”æ¡ˆè™›æ“¬è·¯å¾‘
        from pathlib import Path

        if not Path(resolved_file_path).exists():
            error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {resolved_file_path}"
            logger.error(f"âŒ {error_msg}")
            return f'{{"success": false, "error": "{error_msg}"}}'
        else:
            # å–®ä¸€æª”æ¡ˆæƒ…æ³
            result = await data_analysis_tools.group_by_analysis(
                resolved_file_path, group_column, value_column, operation, session_id
            )
        logger.info(f"âœ… group_by_analysis_tool åŸ·è¡Œå®Œæˆ")
        import json

        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ åˆ†çµ„åˆ†æå¤±æ•—: {e}")
        import traceback

        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def compare_datasets_tool(
    file_paths: str,
    analysis_focus: str = "general",
    session_id: str = "default"
) -> str:
    """
    æ¯”è¼ƒå¤šå€‹è³‡æ–™é›†çš„å·¥å…·

    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚: "path1.csv,path2.csv"
        analysis_focus: åˆ†æé‡é»ï¼Œä¾‹å¦‚: "topic_distribution", "sentiment", "keywords", "general"
        session_id: æœƒè©±ID

    Returns:
        æ¯”è¼ƒåˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        logger.info(f"ğŸ”„ compare_datasets_tool é–‹å§‹åŸ·è¡Œ:")
        logger.info(f"  - file_paths: {file_paths}")
        logger.info(f"  - analysis_focus: {analysis_focus}")

        # è§£ææª”æ¡ˆè·¯å¾‘
        paths = [path.strip() for path in file_paths.split(",")]

        if len(paths) < 2:
            return f'{{"success": false, "error": "éœ€è¦è‡³å°‘2å€‹æª”æ¡ˆé€²è¡Œæ¯”è¼ƒ"}}'

        # è®€å–æ‰€æœ‰æª”æ¡ˆ
        import pandas as pd
        datasets = []

        for path in paths:
            try:
                if not os.path.exists(path):
                    logger.warning(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {path}")
                    continue

                df = pd.read_csv(path, encoding='utf-8-sig')
                filename = os.path.basename(path)
                source = filename.split('_')[0] if '_' in filename else filename

                datasets.append({
                    "source": source,
                    "filename": filename,
                    "path": path,
                    "data": df,
                    "row_count": len(df),
                    "columns": list(df.columns)
                })

                logger.info(f"âœ… è®€å–æª”æ¡ˆ: {filename} ({len(df)} è¡Œ)")

            except Exception as e:
                logger.error(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {path}: {e}")
                continue

        if len(datasets) < 2:
            return f'{{"success": false, "error": "æˆåŠŸè®€å–çš„æª”æ¡ˆå°‘æ–¼2å€‹"}}'

        # åŸ·è¡Œæ¯”è¼ƒåˆ†æ
        comparison_result = await _perform_dataset_comparison(datasets, analysis_focus, session_id)

        logger.info(f"âœ… compare_datasets_tool åŸ·è¡Œå®Œæˆ")
        return json.dumps(comparison_result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ è³‡æ–™é›†æ¯”è¼ƒå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


async def _perform_dataset_comparison(datasets, analysis_focus, session_id):
    """åŸ·è¡Œè³‡æ–™é›†æ¯”è¼ƒåˆ†æ"""
    try:
        import pandas as pd

        # åŸºæœ¬çµ±è¨ˆæ¯”è¼ƒ
        basic_stats = {}
        common_columns = None

        for dataset in datasets:
            source = dataset["source"]
            df = dataset["data"]

            # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
            stats = {
                "row_count": len(df),
                "column_count": len(df.columns),
                "columns": list(df.columns)
            }

            # æ‰¾å‡ºå…±åŒæ¬„ä½
            if common_columns is None:
                common_columns = set(df.columns)
            else:
                common_columns = common_columns.intersection(set(df.columns))

            basic_stats[source] = stats

        common_columns = list(common_columns)

        # æ ¹æ“šåˆ†æé‡é»é€²è¡Œä¸åŒçš„æ¯”è¼ƒ
        detailed_analysis = {}

        if analysis_focus == "topic_distribution" or analysis_focus == "general":
            # ä¸»é¡Œåˆ†ä½ˆæ¯”è¼ƒ
            detailed_analysis["topic_analysis"] = await _compare_topic_distribution(datasets, common_columns)

        if analysis_focus == "keywords" or analysis_focus == "general":
            # é—œéµå­—æ¯”è¼ƒ
            detailed_analysis["keyword_analysis"] = await _compare_keywords(datasets, common_columns)

        if analysis_focus == "general":
            # æ•¸å€¼æ¬„ä½æ¯”è¼ƒ
            detailed_analysis["numeric_analysis"] = await _compare_numeric_fields(datasets, common_columns)

        return {
            "success": True,
            "session_id": session_id,
            "analysis_focus": analysis_focus,
            "datasets_compared": len(datasets),
            "common_columns": common_columns,
            "basic_statistics": basic_stats,
            "detailed_analysis": detailed_analysis,
            "summary": _generate_comparison_summary(basic_stats, detailed_analysis)
        }

    except Exception as e:
        logger.error(f"âŒ æ¯”è¼ƒåˆ†æåŸ·è¡Œå¤±æ•—: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def _compare_topic_distribution(datasets, common_columns):
    """æ¯”è¼ƒä¸»é¡Œåˆ†ä½ˆ"""
    try:
        # å°‹æ‰¾å¯èƒ½çš„ä¸»é¡Œæ¬„ä½
        topic_columns = []
        for col in common_columns:
            if any(keyword in col.lower() for keyword in ['topic', 'subject', 'title', 'content', 'ä¸»é¡Œ', 'æ¨™é¡Œ', 'å…§å®¹']):
                topic_columns.append(col)

        if not topic_columns:
            return {"message": "æœªæ‰¾åˆ°ä¸»é¡Œç›¸é—œæ¬„ä½"}

        topic_analysis = {}
        for dataset in datasets:
            source = dataset["source"]
            df = dataset["data"]

            source_topics = {}
            for col in topic_columns:
                if col in df.columns:
                    # çµ±è¨ˆè©²æ¬„ä½çš„å€¼åˆ†ä½ˆ
                    value_counts = df[col].value_counts().head(10).to_dict()
                    source_topics[col] = value_counts

            topic_analysis[source] = source_topics

        return topic_analysis

    except Exception as e:
        logger.error(f"âŒ ä¸»é¡Œåˆ†ä½ˆæ¯”è¼ƒå¤±æ•—: {e}")
        return {"error": str(e)}


async def _compare_keywords(datasets, common_columns):
    """æ¯”è¼ƒé—œéµå­—"""
    try:
        # å°‹æ‰¾æ–‡æœ¬æ¬„ä½
        text_columns = []
        for col in common_columns:
            if any(keyword in col.lower() for keyword in ['content', 'text', 'message', 'title', 'å…§å®¹', 'æ¨™é¡Œ', 'è¨Šæ¯']):
                text_columns.append(col)

        if not text_columns:
            return {"message": "æœªæ‰¾åˆ°æ–‡æœ¬æ¬„ä½"}

        keyword_analysis = {}
        for dataset in datasets:
            source = dataset["source"]
            df = dataset["data"]

            # ç°¡å–®çš„é—œéµå­—çµ±è¨ˆï¼ˆé€™è£¡å¯ä»¥ç”¨æ›´è¤‡é›œçš„ NLP æ–¹æ³•ï¼‰
            all_text = ""
            for col in text_columns:
                if col in df.columns:
                    all_text += " ".join(df[col].astype(str).tolist())

            # ç°¡å–®çš„è©é »çµ±è¨ˆ
            words = all_text.split()
            word_freq = {}
            for word in words:
                if len(word) > 2:  # éæ¿¾çŸ­è©
                    word_freq[word] = word_freq.get(word, 0) + 1

            # å–å‰10å€‹é«˜é »è©
            top_words = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10])
            keyword_analysis[source] = top_words

        return keyword_analysis

    except Exception as e:
        logger.error(f"âŒ é—œéµå­—æ¯”è¼ƒå¤±æ•—: {e}")
        return {"error": str(e)}


async def _compare_numeric_fields(datasets, common_columns):
    """æ¯”è¼ƒæ•¸å€¼æ¬„ä½"""
    try:
        import pandas as pd

        # æ‰¾å‡ºæ•¸å€¼æ¬„ä½
        numeric_columns = []
        for dataset in datasets:
            df = dataset["data"]
            for col in common_columns:
                if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
                    if col not in numeric_columns:
                        numeric_columns.append(col)

        if not numeric_columns:
            return {"message": "æœªæ‰¾åˆ°å…±åŒçš„æ•¸å€¼æ¬„ä½"}

        numeric_analysis = {}
        for dataset in datasets:
            source = dataset["source"]
            df = dataset["data"]

            source_stats = {}
            for col in numeric_columns:
                if col in df.columns:
                    stats = {
                        "mean": float(df[col].mean()),
                        "median": float(df[col].median()),
                        "std": float(df[col].std()),
                        "min": float(df[col].min()),
                        "max": float(df[col].max())
                    }
                    source_stats[col] = stats

            numeric_analysis[source] = source_stats

        return numeric_analysis

    except Exception as e:
        logger.error(f"âŒ æ•¸å€¼æ¬„ä½æ¯”è¼ƒå¤±æ•—: {e}")
        return {"error": str(e)}


def _generate_comparison_summary(basic_stats, detailed_analysis):
    """ç”Ÿæˆæ¯”è¼ƒæ‘˜è¦"""
    try:
        summary = []

        # åŸºæœ¬çµ±è¨ˆæ‘˜è¦
        sources = list(basic_stats.keys())
        summary.append(f"æ¯”è¼ƒäº† {len(sources)} å€‹è³‡æ–™é›†: {', '.join(sources)}")

        for source, stats in basic_stats.items():
            summary.append(f"{source}: {stats['row_count']} è¡Œè³‡æ–™ï¼Œ{stats['column_count']} å€‹æ¬„ä½")

        # è©³ç´°åˆ†ææ‘˜è¦
        if "topic_analysis" in detailed_analysis:
            summary.append("å·²é€²è¡Œä¸»é¡Œåˆ†ä½ˆæ¯”è¼ƒ")

        if "keyword_analysis" in detailed_analysis:
            summary.append("å·²é€²è¡Œé—œéµå­—æ¯”è¼ƒ")

        if "numeric_analysis" in detailed_analysis:
            summary.append("å·²é€²è¡Œæ•¸å€¼æ¬„ä½æ¯”è¼ƒ")

        return summary

    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±æ•—: {e}")
        return ["æ‘˜è¦ç”Ÿæˆå¤±æ•—"]


@tool
async def threshold_analysis_tool(
    file_path: str,
    value_column: str,
    threshold: float,
    comparison: str = "greater",
    session_id: str = "default",
) -> str:
    """
    é€šç”¨é–¾å€¼åˆ†æå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘ï¼Œæ”¯æŒç‰¹æ®Šå€¼ "@current" ä½¿ç”¨ç•¶å‰æœƒè©±çš„æœ€æ–°æ•¸æ“š
        value_column: æ•¸å€¼åˆ—å
        threshold: é–¾å€¼
        comparison: æ¯”è¼ƒæ–¹å¼ (greater, less, equal)
        session_id: æœƒè©±ID

    Returns:
        é–¾å€¼åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # è§£ææ–‡ä»¶è·¯å¾‘
        resolved_file_path = session_data_manager.resolve_file_path(
            session_id, file_path
        )
        logger.info(f"ğŸ”„ threshold_analysis_tool: {file_path} -> {resolved_file_path}")

        result = await data_analysis_tools.threshold_analysis(
            resolved_file_path, value_column, threshold, comparison, session_id
        )
        import json

        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ é–¾å€¼åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def read_data_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    è®€å–æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_file_tools.read_data_file(file_path, session_id=session_id)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ è®€å–æ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def filter_data_tool(
    file_path: str,
    filter_conditions: str,
    session_id: str = "default",
    save_filtered_data: bool = False,
    selected_columns: str = None,
) -> str:
    """
    æ ¹æ“šæ¢ä»¶éæ¿¾æ•¸æ“šæ–‡ä»¶ï¼Œæ”¯æŒåˆ—é¸æ“‡

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        filter_conditions: éæ¿¾æ¢ä»¶çš„JSONå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: {"column": "age", "operator": ">", "value": 25}
        session_id: æœƒè©±ID
        save_filtered_data: æ˜¯å¦å°‡éæ¿¾å¾Œçš„æ•¸æ“šä¿å­˜ç‚ºè‡¨æ™‚æ–‡ä»¶ï¼Œä¾›å…¶ä»–å·¥å…·ä½¿ç”¨
        selected_columns: è¦ä¿ç•™çš„åˆ—åJSONæ•¸çµ„ï¼Œä¾‹å¦‚: ["å§“å", "éƒ¨é–€", "åŸºæœ¬è–ªè³‡"]ï¼Œå¦‚æœç‚ºNoneå‰‡ä¿ç•™æ‰€æœ‰åˆ—

    Returns:
        éæ¿¾å¾Œçš„æ•¸æ“šJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœsave_filtered_data=Trueï¼Œé‚„æœƒåŒ…å«è‡¨æ™‚æ–‡ä»¶è·¯å¾‘
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£æéæ¿¾æ¢ä»¶
        conditions = json.loads(filter_conditions)

        # è®€å–æ•¸æ“šæ–‡ä»¶
        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹è®€å–
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == ".csv":
            df = pd.read_csv(file_path)
        elif file_ext == ".json":
            df = pd.read_json(file_path)
        elif file_ext in [".xlsx", ".xls"]:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        if isinstance(conditions, dict):
            conditions = [conditions]  # è½‰æ›ç‚ºåˆ—è¡¨

        filtered_df = df.copy()

        for condition in conditions:
            column = condition.get("column")
            operator = condition.get("operator")
            value = condition.get("value")

            if column not in filtered_df.columns:
                continue

            if operator == ">":
                filtered_df = filtered_df[filtered_df[column] > value]
            elif operator == "<":
                filtered_df = filtered_df[filtered_df[column] < value]
            elif operator == ">=":
                filtered_df = filtered_df[filtered_df[column] >= value]
            elif operator == "<=":
                filtered_df = filtered_df[filtered_df[column] <= value]
            elif operator == "==":
                filtered_df = filtered_df[filtered_df[column] == value]
            elif operator == "!=":
                filtered_df = filtered_df[filtered_df[column] != value]
            elif operator == "contains":
                filtered_df = filtered_df[
                    filtered_df[column].str.contains(str(value), na=False)
                ]
            elif operator == "in":
                filtered_df = filtered_df[filtered_df[column].isin(value)]

        # è™•ç†åˆ—é¸æ“‡
        if selected_columns:
            try:
                columns_list = (
                    json.loads(selected_columns)
                    if isinstance(selected_columns, str)
                    else selected_columns
                )
                if isinstance(columns_list, list):
                    # æª¢æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                    available_columns = [
                        col for col in columns_list if col in filtered_df.columns
                    ]
                    missing_columns = [
                        col for col in columns_list if col not in filtered_df.columns
                    ]

                    if missing_columns:
                        logger.warning(f"âš ï¸ ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {missing_columns}")

                    if available_columns:
                        filtered_df = filtered_df[available_columns]
                        logger.info(f"âœ… å·²é¸æ“‡åˆ—: {available_columns}")
                    else:
                        logger.warning(f"âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„åˆ—å¯é¸æ“‡ï¼Œä¿ç•™æ‰€æœ‰åˆ—")
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"âš ï¸ åˆ—é¸æ“‡åƒæ•¸æ ¼å¼éŒ¯èª¤: {e}ï¼Œä¿ç•™æ‰€æœ‰åˆ—")

        # æº–å‚™åŸºæœ¬çµæœ
        result = {
            "success": True,
            "original_rows": len(df),
            "filtered_rows": len(filtered_df),
            "columns": list(filtered_df.columns),
            "filter_conditions": conditions,
        }

        # å¦‚æœéœ€è¦ä¿å­˜è‡¨æ™‚æ–‡ä»¶
        if save_filtered_data and len(filtered_df) > 0:
            import tempfile
            import os
            from pathlib import Path

            # å‰µå»ºæœƒè©±ç´šè‡¨æ™‚ç›®éŒ„
            temp_dir = Path(tempfile.gettempdir()) / "agent_sessions" / session_id
            temp_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆè‡¨æ™‚æ–‡ä»¶å
            original_name = Path(file_path).stem
            timestamp = __import__("datetime").datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_filename = f"{original_name}_filtered_{timestamp}.json"
            temp_file_path = temp_dir / temp_filename

            # ä¿å­˜éæ¿¾å¾Œçš„æ•¸æ“šç‚ºJSONæ ¼å¼ï¼Œç¢ºä¿ä¸­æ–‡å­—ç¬¦æ­£ç¢ºé¡¯ç¤º
            filtered_df.to_json(temp_file_path, orient='records', indent=2, force_ascii=False)

            # æ›´æ–°æœƒè©±æ•¸æ“šç‹€æ…‹
            session_data_manager.update_data_state(
                session_id=session_id,
                original_file=file_path,
                current_file=str(temp_file_path),
                operation="filter",
                metadata={
                    "original_rows": len(df),
                    "filtered_rows": len(filtered_df),
                    "filter_conditions": conditions,
                },
                description=f"éæ¿¾æ¢ä»¶: {conditions}",
            )

            result.update(
                {
                    "temp_file_path": str(temp_file_path),
                    "temp_file_created": True,
                    "current_data_updated": True,
                    "message": f"éæ¿¾å¾Œçš„æ•¸æ“šå·²ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶ä¸¦è¨­ç‚ºç•¶å‰æ•¸æ“šæº: {temp_file_path}",
                }
            )

            # åªè¿”å›å‰10è¡Œæ•¸æ“šé è¦½ï¼Œé¿å…éŸ¿æ‡‰éå¤§
            result["data_preview"] = filtered_df.head(10).to_dict("records")
            logger.info(f"âœ… éæ¿¾å¾Œæ•¸æ“šå·²ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶: {temp_file_path}")
        else:
            # ä¸ä¿å­˜æ–‡ä»¶æ™‚ï¼Œè¿”å›å®Œæ•´æ•¸æ“šï¼ˆä½†é™åˆ¶åœ¨100è¡Œä»¥å…§ï¼‰
            max_rows = 100
            if len(filtered_df) > max_rows:
                result["data"] = filtered_df.head(max_rows).to_dict("records")
                result["data_truncated"] = True
                result["message"] = (
                    f"æ•¸æ“šå·²æˆªæ–·ï¼Œåªé¡¯ç¤ºå‰{max_rows}è¡Œã€‚å¦‚éœ€å®Œæ•´æ•¸æ“šï¼Œè«‹è¨­ç½®save_filtered_data=True"
                )
            else:
                result["data"] = filtered_df.to_dict("records")

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šéæ¿¾å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def cleanup_temp_files_tool(session_id: str = "default") -> str:
    """
    æ¸…ç†æœƒè©±çš„è‡¨æ™‚æ–‡ä»¶

    Args:
        session_id: æœƒè©±ID

    Returns:
        æ¸…ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import tempfile
        import shutil
        from pathlib import Path

        temp_dir = Path(tempfile.gettempdir()) / "agent_sessions" / session_id

        if temp_dir.exists():
            # è¨ˆç®—æ–‡ä»¶æ•¸é‡å’Œå¤§å°
            files = list(temp_dir.glob("*"))
            file_count = len(files)
            total_size = sum(f.stat().st_size for f in files if f.is_file())

            # åˆªé™¤æ•´å€‹æœƒè©±ç›®éŒ„
            shutil.rmtree(temp_dir)

            result = {
                "success": True,
                "cleaned_files": file_count,
                "freed_space_bytes": total_size,
                "message": f"å·²æ¸…ç† {file_count} å€‹è‡¨æ™‚æ–‡ä»¶ï¼Œé‡‹æ”¾ {total_size} å­—ç¯€ç©ºé–“",
            }
        else:
            result = {
                "success": True,
                "cleaned_files": 0,
                "message": "æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è‡¨æ™‚æ–‡ä»¶",
            }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def get_session_data_status_tool(session_id: str = "default") -> str:
    """
    ç²å–æœƒè©±çš„æ•¸æ“šç‹€æ…‹ä¿¡æ¯

    Args:
        session_id: æœƒè©±ID

    Returns:
        æœƒè©±æ•¸æ“šç‹€æ…‹çš„JSONå­—ç¬¦ä¸²
    """
    try:
        summary = session_data_manager.get_session_summary(session_id)
        history = session_data_manager.get_data_history(session_id)

        result = {
            "success": True,
            "session_summary": summary,
            "data_history": history,
            "message": f"æœƒè©± {session_id} æ•¸æ“šç‹€æ…‹ä¿¡æ¯",
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ ç²å–æœƒè©±æ•¸æ“šç‹€æ…‹å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def clear_session_data_tool(session_id: str = "default") -> str:
    """
    æ¸…ç†æœƒè©±çš„æ•¸æ“šç‹€æ…‹ï¼ˆä¸åˆªé™¤å¯¦éš›æ–‡ä»¶ï¼‰

    Args:
        session_id: æœƒè©±ID

    Returns:
        æ¸…ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        result = session_data_manager.clear_session_data(session_id)
        result["success"] = True

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†æœƒè©±æ•¸æ“šç‹€æ…‹å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def suggest_analysis_operation_tool(analysis_purpose: str) -> str:
    """
    æ ¹æ“šåˆ†æç›®çš„å»ºè­°åˆé©çš„æ“ä½œé¡å‹

    Args:
        analysis_purpose: åˆ†æç›®çš„æè¿°ï¼Œä¾‹å¦‚ "è¨ˆç®—éƒ¨é–€å¹³å‡è–ªè³‡"ã€"çµ±è¨ˆå„éƒ¨é–€äººæ•¸"

    Returns:
        å»ºè­°çš„æ“ä½œé¡å‹å’Œèªªæ˜
    """
    try:
        import json
        purpose_lower = analysis_purpose.lower()

        suggestions = {
            "mean": {
                "keywords": [
                    "å¹³å‡",
                    "å‡å€¼",
                    "average",
                    "mean",
                    "è–ªè³‡åˆ†æ",
                    "ç¸¾æ•ˆ",
                    "è©•åˆ†",
                ],
                "description": "è¨ˆç®—å¹³å‡å€¼ï¼Œé©ç”¨æ–¼è–ªè³‡åˆ†æã€ç¸¾æ•ˆè©•ä¼°ã€è©•åˆ†çµ±è¨ˆç­‰",
            },
            "sum": {
                "keywords": [
                    "ç¸½å’Œ",
                    "ç¸½è¨ˆ",
                    "åˆè¨ˆ",
                    "sum",
                    "total",
                    "éŠ·å”®é¡",
                    "ç‡Ÿæ”¶",
                    "æ•¸é‡",
                ],
                "description": "è¨ˆç®—ç¸½å’Œï¼Œé©ç”¨æ–¼éŠ·å”®é¡çµ±è¨ˆã€æ•¸é‡åˆè¨ˆã€ç‡Ÿæ”¶åˆ†æç­‰",
            },
            "count": {
                "keywords": ["æ•¸é‡", "äººæ•¸", "å€‹æ•¸", "count", "çµ±è¨ˆ", "é »æ¬¡", "æ¬¡æ•¸"],
                "description": "è¨ˆç®—æ•¸é‡ï¼Œé©ç”¨æ–¼äººå“¡çµ±è¨ˆã€é »æ¬¡åˆ†æã€è¨ˆæ•¸çµ±è¨ˆç­‰",
            },
            "max": {
                "keywords": ["æœ€å¤§", "æœ€é«˜", "max", "maximum", "å³°å€¼", "é ‚é»"],
                "description": "æ‰¾å‡ºæœ€å¤§å€¼ï¼Œé©ç”¨æ–¼æœ€é«˜è–ªè³‡ã€å³°å€¼åˆ†æã€æ¥µå€¼çµ±è¨ˆç­‰",
            },
            "min": {
                "keywords": ["æœ€å°", "æœ€ä½", "min", "minimum", "åŸºæº–", "åº•ç·š"],
                "description": "æ‰¾å‡ºæœ€å°å€¼ï¼Œé©ç”¨æ–¼æœ€ä½è–ªè³‡ã€åŸºæº–åˆ†æã€æ¥µå€¼çµ±è¨ˆç­‰",
            },
        }

        # æ ¹æ“šé—œéµè©åŒ¹é…å»ºè­°æ“ä½œ
        best_match = "sum"  # é»˜èª
        best_score = 0

        for operation, info in suggestions.items():
            score = sum(1 for keyword in info["keywords"] if keyword in purpose_lower)
            if score > best_score:
                best_score = score
                best_match = operation

        result = {
            "success": True,
            "analysis_purpose": analysis_purpose,
            "suggested_operation": best_match,
            "description": suggestions[best_match]["description"],
            "all_options": {
                op: info["description"] for op, info in suggestions.items()
            },
            "usage_example": f'group_by_analysis_tool("@current", "group_column", "value_column", "{best_match}", session_id)',
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ å»ºè­°åˆ†ææ“ä½œå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def filter_and_analyze_tool(
    file_path: str,
    filter_conditions: str,
    group_column: str,
    value_column: str,
    operation: str = "mean",
    selected_columns: str = None,
    session_id: str = "default",
) -> str:
    """
    ä¸€æ­¥å®Œæˆéæ¿¾å’Œåˆ†çµ„åˆ†æçš„çµ„åˆå·¥å…·

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        filter_conditions: éæ¿¾æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        group_column: åˆ†çµ„åˆ—å
        value_column: æ•¸å€¼åˆ—å
        operation: æ“ä½œé¡å‹ (mean, sum, count, max, min)
        selected_columns: è¦ä¿ç•™çš„åˆ—åJSONæ•¸çµ„ï¼Œä¾‹å¦‚: ["å§“å", "éƒ¨é–€", "åŸºæœ¬è–ªè³‡"]
        session_id: æœƒè©±ID

    Returns:
        åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json

        logger.info(f"ğŸ”„ filter_and_analyze_tool é–‹å§‹åŸ·è¡Œ:")
        logger.info(f"  - file_path: {file_path}")
        logger.info(f"  - filter_conditions: {filter_conditions}")
        logger.info(f"  - group_column: {group_column}")
        logger.info(f"  - value_column: {value_column}")
        logger.info(f"  - operation: {operation}")
        logger.info(f"  - selected_columns: {selected_columns}")

        # æ­¥é©Ÿ1: éæ¿¾æ•¸æ“šä¸¦é¸æ“‡åˆ—
        filter_result = await filter_data_tool.ainvoke(
            {
                "file_path": file_path,
                "filter_conditions": filter_conditions,
                "session_id": session_id,
                "save_filtered_data": True,
                "selected_columns": selected_columns,
            }
        )

        filter_data = json.loads(filter_result)
        if not filter_data.get("success", False):
            return filter_result  # è¿”å›éæ¿¾éŒ¯èª¤

        logger.info(f"âœ… éæ¿¾å®Œæˆ: {filter_data.get('filtered_rows', 0)} è¡Œ")

        # æ­¥é©Ÿ2: å°éæ¿¾å¾Œçš„æ•¸æ“šé€²è¡Œåˆ†çµ„åˆ†æ
        analysis_result = await group_by_analysis_tool.ainvoke(
            {
                "file_path": "@current",  # ä½¿ç”¨éæ¿¾å¾Œçš„æ•¸æ“š
                "group_column": group_column,
                "value_column": value_column,
                "operation": operation,
                "session_id": session_id,
                "data_source": "current",
            }
        )

        analysis_data = json.loads(analysis_result)
        if not analysis_data.get("success", False):
            return analysis_result  # è¿”å›åˆ†æéŒ¯èª¤

        # æ­¥é©Ÿ3: çµ„åˆçµæœ
        combined_result = {
            "success": True,
            "tool_type": "filter_and_analyze",
            "filter_info": {
                "original_rows": filter_data.get("original_rows", 0),
                "filtered_rows": filter_data.get("filtered_rows", 0),
                "selected_columns": (
                    json.loads(selected_columns) if selected_columns else "all"
                ),
                "filter_conditions": json.loads(filter_conditions),
            },
            "analysis_info": {
                "group_column": group_column,
                "value_column": value_column,
                "operation": operation,
            },
            "results": analysis_data.get("results", {}),
            "summary": analysis_data.get("summary", {}),
            "temp_file_path": filter_data.get("temp_file_path"),
            "message": f"æˆåŠŸéæ¿¾ {filter_data.get('filtered_rows', 0)} è¡Œæ•¸æ“šä¸¦å®Œæˆ {operation} åˆ†æ",
        }

        logger.info(f"âœ… filter_and_analyze_tool åŸ·è¡Œå®Œæˆ")
        return json.dumps(combined_result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ éæ¿¾åˆ†æçµ„åˆå·¥å…·å¤±æ•—: {e}")
        import traceback

        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def create_data_file_tool(
    file_path: str, data: str, file_type: str = "csv", session_id: str = "default"
) -> str:
    """
    å‰µå»ºæ–°çš„æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        data: æ•¸æ“šå…§å®¹çš„JSONå­—ç¬¦ä¸²
        file_type: æ–‡ä»¶é¡å‹ (csv, json, xlsx)
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        # è§£ææ•¸æ“š
        data_dict = json.loads(data)

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # å‰µå»ºDataFrame
        if isinstance(data_dict, list):
            df = pd.DataFrame(data_dict)
        elif isinstance(data_dict, dict):
            df = pd.DataFrame([data_dict])
        else:
            return f'{{"success": false, "error": "æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º"}}'

        # æ ¹æ“šæ–‡ä»¶é¡å‹ä¿å­˜
        if file_type.lower() == 'csv':
            df.to_csv(file_path, index=False, encoding='utf-8')
        elif file_type.lower() == 'json':
            df.to_json(file_path, orient='records', indent=2, force_ascii=False)
        elif file_type.lower() == 'xlsx':
            df.to_excel(file_path, index=False)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file_type}"}}'

        result = {
            "success": True,
            "file_path": file_path,
            "file_type": file_type,
            "rows_created": len(df),
            "columns": list(df.columns),
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def update_data_rows_tool(
    file_path: str, update_conditions: str, new_values: str, session_id: str = "default"
) -> str:
    """
    æ›´æ–°æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        update_conditions: æ›´æ–°æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        new_values: æ–°å€¼çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        æ›´æ–°çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£ææ¢ä»¶å’Œæ–°å€¼
        conditions = json.loads(update_conditions)
        values = json.loads(new_values)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == ".csv":
            df = pd.read_csv(file_path)
        elif file_ext == ".json":
            df = pd.read_json(file_path)
        elif file_ext in [".xlsx", ".xls"]:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        # æ‡‰ç”¨æ›´æ–°æ¢ä»¶
        mask = pd.Series([True] * len(df))

        for condition in conditions:
            column = condition.get("column")
            operator = condition.get("operator")
            value = condition.get("value")

            if column not in df.columns:
                continue

            if operator == "==":
                mask &= df[column] == value
            elif operator == "!=":
                mask &= df[column] != value
            elif operator == ">":
                mask &= df[column] > value
            elif operator == "<":
                mask &= df[column] < value
            elif operator == "contains":
                mask &= df[column].str.contains(str(value), na=False)

        # æ›´æ–°æ•¸æ“š
        updated_rows = mask.sum()

        for column, new_value in values.items():
            if column in df.columns:
                df.loc[mask, column] = new_value

        # ä¿å­˜æ–‡ä»¶
        if file_ext == ".csv":
            df.to_csv(file_path, index=False, encoding="utf-8")
        elif file_ext == ".json":
            df.to_json(file_path, orient="records", ensure_ascii=False, indent=2)
        elif file_ext == ".xlsx":
            df.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "updated_rows": int(updated_rows),
            "total_rows": len(df),
            "update_conditions": conditions,
            "new_values": values,
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def delete_data_rows_tool(
    file_path: str, delete_conditions: str, session_id: str = "default"
) -> str:
    """
    åˆªé™¤æ•¸æ“šæ–‡ä»¶ä¸­çš„è¡Œ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        delete_conditions: åˆªé™¤æ¢ä»¶çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json
        import pandas as pd
        import os

        if not os.path.exists(file_path):
            return f'{{"success": false, "error": "æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}}'

        # è§£æåˆªé™¤æ¢ä»¶
        conditions = json.loads(delete_conditions)

        # è®€å–æ•¸æ“š
        file_ext = os.path.splitext(file_path)[1].lower()

        if file_ext == ".csv":
            df = pd.read_csv(file_path)
        elif file_ext == ".json":
            df = pd.read_json(file_path)
        elif file_ext in [".xlsx", ".xls"]:
            df = pd.read_excel(file_path)
        else:
            return f'{{"success": false, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}}'

        original_rows = len(df)

        # æ‡‰ç”¨åˆªé™¤æ¢ä»¶
        mask = pd.Series([False] * len(df))

        for condition in conditions:
            column = condition.get("column")
            operator = condition.get("operator")
            value = condition.get("value")

            if column not in df.columns:
                continue

            if operator == "==":
                mask |= df[column] == value
            elif operator == "!=":
                mask |= df[column] != value
            elif operator == ">":
                mask |= df[column] > value
            elif operator == "<":
                mask |= df[column] < value
            elif operator == "contains":
                mask |= df[column].str.contains(str(value), na=False)

        # åˆªé™¤æ•¸æ“š
        df_filtered = df[~mask]
        deleted_rows = original_rows - len(df_filtered)

        # ä¿å­˜æ–‡ä»¶
        if file_ext == '.csv':
            df_filtered.to_csv(file_path, index=False, encoding='utf-8')
        elif file_ext == '.json':
            df_filtered.to_json(file_path, orient='records', indent=2, force_ascii=False)
        elif file_ext == '.xlsx':
            df_filtered.to_excel(file_path, index=False)

        result = {
            "success": True,
            "file_path": file_path,
            "deleted_rows": int(deleted_rows),
            "remaining_rows": len(df_filtered),
            "original_rows": original_rows,
            "delete_conditions": conditions,
        }

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ•¸æ“šå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


# æ·»åŠ æ›´å¤šå·¥å…·
@tool
async def highlight_file_sections_tool(
    file_path: str, ranges: str, session_id: str = "default"
) -> str:
    """
    é«˜äº®æ–‡ä»¶å€åŸŸ

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        ranges: ç¯„åœåˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        é«˜äº®çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        import json

        ranges_list = json.loads(ranges)
        result = await local_file_tools.highlight_file_sections(
            file_path, ranges_list, session_id
        )
        return str(result)
    except Exception as e:
        logger.error(f"âŒ é«˜äº®æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def save_file_tool(
    file_path: str, content: str, encoding: str = "utf-8", session_id: str = "default"
) -> str:
    """
    ä¿å­˜æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        ä¿å­˜çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.save_file(
            file_path, content, encoding, session_id
        )
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def create_file_tool(
    file_path: str,
    content: str = "",
    encoding: str = "utf-8",
    session_id: str = "default",
) -> str:
    """
    å‰µå»ºæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        content: æ–‡ä»¶å…§å®¹
        encoding: ç·¨ç¢¼æ ¼å¼
        session_id: æœƒè©±ID

    Returns:
        å‰µå»ºçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.create_file(
            file_path, content, encoding, session_id
        )
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ å‰µå»ºæ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def correlation_analysis_tool(
    file_path: str, x_column: str, y_column: str, session_id: str = "default"
) -> str:
    """
    ç›¸é—œæ€§åˆ†æ

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        session_id: æœƒè©±ID

    Returns:
        ç›¸é—œæ€§åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.correlation_analysis(
            file_path, x_column, y_column, session_id
        )
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç›¸é—œæ€§åˆ†æå¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def linear_prediction_tool(
    file_path: str,
    x_column: str,
    y_column: str,
    target_x_value: float,
    session_id: str = "default",
) -> str:
    """
    ç·šæ€§é æ¸¬

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        x_column: Xè»¸è®Šé‡åˆ—å
        y_column: Yè»¸è®Šé‡åˆ—å
        target_x_value: ç›®æ¨™Xå€¼
        session_id: æœƒè©±ID

    Returns:
        é æ¸¬çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await data_analysis_tools.linear_prediction(
            file_path, x_column, y_column, target_x_value, session_id
        )
        return str(result)
    except Exception as e:
        logger.error(f"âŒ ç·šæ€§é æ¸¬å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def edit_data_file_tool(
    file_path: str, operation: str, data: str, session_id: str = "default"
) -> str:
    """
    ç·¨è¼¯æ•¸æ“šæ–‡ä»¶

    Args:
        file_path: æ•¸æ“šæ–‡ä»¶è·¯å¾‘
        operation: æ“ä½œé¡å‹ (add_row, delete_row, update_cell, update_row)
        data: æ“ä½œæ•¸æ“šçš„JSONå­—ç¬¦ä¸²
        session_id: æœƒè©±ID

    Returns:
        ç·¨è¼¯çµæœçš„JSONå­—ç¬¦ä¸²
    """
    # è¨˜éŒ„è¼¸å…¥
    logger.info(f"ğŸ”§ [edit_data_file_tool] é–‹å§‹åŸ·è¡Œ")
    logger.info(f"ğŸ“¥ è¼¸å…¥åƒæ•¸:")
    logger.info(f"   - file_path: '{file_path}'")
    logger.info(f"   - operation: '{operation}'")
    logger.info(f"   - data: '{data}'")
    logger.info(f"   - session_id: '{session_id}'")

    try:
        # æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ1: é©—è­‰æ“ä½œé¡å‹")
        valid_operations = ["add_row", "delete_row", "update_cell", "update_row"]
        if operation not in valid_operations:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ“ä½œé¡å‹: {operation}ï¼Œæ”¯æŒçš„æ“ä½œ: {valid_operations}"
            )
        logger.info(f"âœ“ æ“ä½œé¡å‹æœ‰æ•ˆ: {operation}")

        # æ­¥é©Ÿ2: è§£ææ•¸æ“š
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ2: è§£ææ“ä½œæ•¸æ“š")
        import json

        try:
            parsed_data = json.loads(data)
            logger.info(f"âœ“ æ•¸æ“šè§£ææˆåŠŸ: {type(parsed_data)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}")

        # æ­¥é©Ÿ3: èª¿ç”¨ç·¨è¼¯å·¥å…·
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ3: èª¿ç”¨ data_file_tools.edit_data_file")
        result = await data_file_tools.edit_data_file(
            file_path, operation, data, session_id
        )

        # æ­¥é©Ÿ4: è™•ç†çµæœ
        logger.info(f"ğŸ“‹ æ­¥é©Ÿ4: è™•ç†ç·¨è¼¯çµæœ")
        result_str = json.dumps(result, ensure_ascii=False)

        # è¨˜éŒ„è¼¸å‡º
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœé•·åº¦: {len(result_str)} å­—ç¬¦")
        if isinstance(result, dict) and result.get("success"):
            logger.info(f"âœ… ç·¨è¼¯æ“ä½œæˆåŠŸå®Œæˆ")
            if "affected_rows" in result:
                logger.info(f"ğŸ“Š å½±éŸ¿è¡Œæ•¸: {result['affected_rows']}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºçµæœå‰300å­—ç¬¦: {result_str[:300]}")
        logger.info(f"âœ… [edit_data_file_tool] åŸ·è¡Œå®Œæˆ")

        return result_str
    except Exception as e:
        logger.error(f"âŒ [edit_data_file_tool] åŸ·è¡Œå¤±æ•—: {e}")
        error_result = f'{{"success": false, "error": "{str(e)}"}}'
        logger.info(f"ğŸ“¤ éŒ¯èª¤è¼¸å‡º: {error_result}")
        return error_result


@tool
async def delete_file_tool(file_path: str, session_id: str = "default") -> str:
    """
    åˆªé™¤æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        session_id: æœƒè©±ID

    Returns:
        åˆªé™¤çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = await local_file_tools.delete_file(file_path, session_id)
        return str(result)
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤æ–‡ä»¶å¤±æ•—: {e}")
        return f'{{"success": false, "error": "{str(e)}"}}'


@tool
async def gmail_fingerprint_search_tool(
    file_path: str,
    search_query: str,
    session_id: str = "default",
    similarity_threshold: float = 0.7,
    max_results: int = None,
    save_results: bool = True
) -> str:
    """
    ä½¿ç”¨æŒ‡ç´‹æœå°‹æŠ€è¡“åœ¨ Gmail CSV æ–‡ä»¶ä¸­é€²è¡Œæ™ºèƒ½æ–‡å­—æœå°‹

    åŸºæ–¼ Google æŒ‡ç´‹æœå°‹æ¦‚å¿µï¼Œçµåˆèªç¾©æœå°‹å’Œé—œéµå­—åŒ¹é…ï¼Œ
    èƒ½å¤ æ‰¾åˆ°èˆ‡æŸ¥è©¢èªç¾©ç›¸é—œçš„å…§å®¹ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ç²¾ç¢ºåŒ¹é…ã€‚

    å°ˆé–€é‡å° Gmail CSV æ ¼å¼å„ªåŒ–ï¼Œæœå°‹ 'subject' å’Œ 'content' æ¬„ä½ã€‚

    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        search_query: æœå°‹æŸ¥è©¢è©ï¼ˆæ”¯æ´è‡ªç„¶èªè¨€æè¿°ï¼‰
        session_id: æœƒè©±ID
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-6.2)ï¼Œè¶Šé«˜è¶Šåš´æ ¼ï¼Œé è¨­ 0.7
        max_results: æœ€å¤§è¿”å›çµæœæ•¸ (None è¡¨ç¤ºä¸é™åˆ¶ï¼Œæ ¹æ“šé–¾å€¼è‡ªç„¶éæ¿¾)
        save_results: æ˜¯å¦å°‡çµæœä¿å­˜ç‚ºæ–°çš„ CSV æª”æ¡ˆ

    Returns:
        æœå°‹çµæœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«åŒ¹é…æ•¸é‡ã€çµæœæª”æ¡ˆè·¯å¾‘å’Œæ¨£æœ¬æ•¸æ“š

    Examples:
        - search_query: "è²¡å‹™ç›¸é—œçš„éƒµä»¶" - æœƒæ‰¾åˆ°åŒ…å«é‡‘é¡ã€ç™¼ç¥¨ã€ä»˜æ¬¾ç­‰å…§å®¹
        - search_query: "å®¢æˆ¶æŠ•è¨´" - æœƒæ‰¾åˆ°åŒ…å«å•é¡Œã€æŠ±æ€¨ã€é€€è²¨ç­‰å…§å®¹
        - search_query: "ç”¢å“è©¢å•" - æœƒæ‰¾åˆ°åŒ…å«ç”¢å“åç¨±ã€è¦æ ¼ã€åƒ¹æ ¼ç­‰å…§å®¹
    """
    try:
        if not FINGERPRINT_SEARCH_AVAILABLE:
            return json.dumps({
                "success": False,
                "error": "æŒ‡ç´‹æœå°‹åŠŸèƒ½ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç›¸é—œä¾è³´"
            }, ensure_ascii=False)

        logger.info(f"ğŸ” åŸ·è¡ŒæŒ‡ç´‹æœå°‹: '{search_query}' in {file_path}")

        result = await fingerprint_search_csv(
            file_path=file_path,
            search_query=search_query,
            session_id=session_id,
            similarity_threshold=similarity_threshold,
            max_results=max_results,
            save_results=save_results
        )

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ æŒ‡ç´‹æœå°‹å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": f"æŒ‡ç´‹æœå°‹å¤±æ•—: {str(e)}"
        }, ensure_ascii=False)


@tool
async def fingerprint_search_tool(
    file_path: str,
    search_query: str,
    search_columns: str,
    session_id: str = "default",
    similarity_threshold: float = 0.7,
    max_results: int = None,
    save_results: bool = True
) -> str:
    """
    ä½¿ç”¨æŒ‡ç´‹æœå°‹æŠ€è¡“åœ¨ CSV æ–‡ä»¶ä¸­é€²è¡Œæ™ºèƒ½æ–‡å­—æœå°‹ï¼ˆå¯æŒ‡å®šæœå°‹æ¬„ä½ï¼‰

    é€šç”¨ç‰ˆæœ¬çš„æŒ‡ç´‹æœå°‹å·¥å…·ï¼Œå…è¨±ç”¨æˆ¶æŒ‡å®šè¦æœå°‹çš„æ¬„ä½åç¨±ï¼Œ
    é©ç”¨æ–¼å„ç¨® CSV æ ¼å¼ï¼Œä¸é™æ–¼ Gmail æ•¸æ“šã€‚

    åŸºæ–¼ Google æŒ‡ç´‹æœå°‹æ¦‚å¿µï¼Œçµåˆèªç¾©æœå°‹å’Œé—œéµå­—åŒ¹é…ï¼Œ
    èƒ½å¤ æ‰¾åˆ°èˆ‡æŸ¥è©¢èªç¾©ç›¸é—œçš„å…§å®¹ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ç²¾ç¢ºåŒ¹é…ã€‚

    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        search_query: æœå°‹æŸ¥è©¢è©ï¼ˆæ”¯æ´è‡ªç„¶èªè¨€æè¿°ï¼‰
        search_columns: è¦æœå°‹çš„æ¬„ä½åç¨±ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼ˆä¾‹å¦‚ï¼š"title,description" æˆ– "content,body,summary"ï¼‰
        session_id: æœƒè©±ID
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-6.2)ï¼Œè¶Šé«˜è¶Šåš´æ ¼ï¼Œé è¨­ 0.7
        max_results: æœ€å¤§è¿”å›çµæœæ•¸ (None è¡¨ç¤ºä¸é™åˆ¶ï¼Œæ ¹æ“šé–¾å€¼è‡ªç„¶éæ¿¾)
        save_results: æ˜¯å¦å°‡çµæœä¿å­˜ç‚ºæ–°çš„ CSV æª”æ¡ˆ

    Returns:
        æœå°‹çµæœçš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«åŒ¹é…æ•¸é‡ã€çµæœæª”æ¡ˆè·¯å¾‘å’Œæ¨£æœ¬æ•¸æ“š

    Examples:
        - search_columns: "subject,content" - æœå°‹ä¸»é¡Œå’Œå…§å®¹æ¬„ä½
        - search_columns: "title,description,body" - æœå°‹æ¨™é¡Œã€æè¿°å’Œæ­£æ–‡æ¬„ä½
        - search_columns: "name,address,phone" - æœå°‹å§“åã€åœ°å€å’Œé›»è©±æ¬„ä½
        - search_query: "è²¡å‹™ç›¸é—œçš„éƒµä»¶" - æœƒæ‰¾åˆ°åŒ…å«é‡‘é¡ã€ç™¼ç¥¨ã€ä»˜æ¬¾ç­‰å…§å®¹
        - search_query: "å®¢æˆ¶æŠ•è¨´" - æœƒæ‰¾åˆ°åŒ…å«å•é¡Œã€æŠ±æ€¨ã€é€€è²¨ç­‰å…§å®¹
    """
    try:
        if not FLEXIBLE_FINGERPRINT_SEARCH_AVAILABLE:
            return json.dumps({
                "success": False,
                "error": "éˆæ´»æŒ‡ç´‹æœå°‹åŠŸèƒ½ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç›¸é—œä¾è³´"
            }, ensure_ascii=False)

        # è§£ææœå°‹æ¬„ä½
        search_columns_list = [col.strip() for col in search_columns.split(',') if col.strip()]

        if not search_columns_list:
            return json.dumps({
                "success": False,
                "error": "è«‹æä¾›æœ‰æ•ˆçš„æœå°‹æ¬„ä½åç¨±"
            }, ensure_ascii=False)

        logger.info(f"ğŸ” åŸ·è¡Œéˆæ´»æŒ‡ç´‹æœå°‹: '{search_query}' in columns {search_columns_list} from {file_path}")

        # æª¢æŸ¥æ˜¯å¦ç‚ºåˆä½µè³‡æ–™é›†
        if file_path.endswith('combined_datasets'):
            logger.info(f"ğŸ”„ è™•ç†åˆä½µè³‡æ–™é›†æœå°‹: {file_path}")
            result = await _handle_combined_dataset_search(
                file_path=file_path,
                search_query=search_query,
                search_columns=search_columns_list,
                session_id=session_id,
                similarity_threshold=similarity_threshold,
                max_results=max_results,
                save_results=save_results
            )
        else:
            result = await flexible_fingerprint_search_csv(
                file_path=file_path,
                search_query=search_query,
                search_columns=search_columns_list,
                session_id=session_id,
                similarity_threshold=similarity_threshold,
                max_results=max_results,
                save_results=save_results
            )

        return json.dumps(result, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ éˆæ´»æŒ‡ç´‹æœå°‹å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": f"éˆæ´»æŒ‡ç´‹æœå°‹å¤±æ•—: {str(e)}"
        }, ensure_ascii=False)


async def _handle_combined_dataset_search(
    file_path: str,
    search_query: str,
    search_columns: List[str],
    session_id: str = "default",
    similarity_threshold: float = 0.7,
    max_results: int = None,
    save_results: bool = True
) -> Dict[str, Any]:
    """
    è™•ç†åˆä½µè³‡æ–™é›†çš„æœå°‹

    Args:
        file_path: åˆä½µè³‡æ–™é›†çš„è™›æ“¬è·¯å¾‘
        search_query: æœå°‹æŸ¥è©¢è©
        search_columns: è¦æœå°‹çš„æ¬„ä½åç¨±åˆ—è¡¨
        session_id: æœƒè©±ID
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼
        max_results: æœ€å¤§è¿”å›çµæœæ•¸
        save_results: æ˜¯å¦ä¿å­˜çµæœ

    Returns:
        æœå°‹çµæœçš„å­—å…¸
    """
    try:
        logger.info(f"ğŸ”„ é–‹å§‹è™•ç†åˆä½µè³‡æ–™é›†æœå°‹: {file_path}")

        # å¾Agentçš„ä¸Šä¸‹æ–‡ç²å–åˆä½µè³‡æ–™é›†çš„å…§å®¹
        # é€™éœ€è¦é€šéAgentçš„contextåƒæ•¸å‚³é
        from backend.api.routers.agent import get_agent
        agent = get_agent(session_id)

        # å˜—è©¦å¾Agentçš„ä¸Šä¸‹æ–‡ä¸­ç²å–åˆä½µè³‡æ–™é›†
        combined_data = None
        if hasattr(agent, 'current_context') and agent.current_context:
            context_data = agent.current_context.get('context_data', {})
            if 'file_summary' in context_data:
                file_summary = context_data['file_summary']
                # æª¢æŸ¥æ˜¯å¦ç‚ºåˆä½µè³‡æ–™é›†æ ¼å¼
                if isinstance(file_summary, dict) and 'segments' in file_summary:
                    segments = file_summary['segments']
                    # é‡æ§‹åˆä½µè³‡æ–™é›†æ ¼å¼
                    combined_data = []
                    for segment in segments:
                        if 'content_type' in segment:
                            combined_data.append({
                                'source': segment.get('content_type', 'unknown'),
                                'date': '2025-01-26',  # å¾segment summaryä¸­æå–
                                'time': '12:00:00',
                                'data': []  # å¯¦éš›è³‡æ–™éœ€è¦å¾å…¶ä»–åœ°æ–¹ç²å–
                            })

        if not combined_data:
            return {
                "success": False,
                "error": "ç„¡æ³•ç²å–åˆä½µè³‡æ–™é›†çš„ä¸Šä¸‹æ–‡è³‡æ–™ï¼Œè«‹ç¢ºä¿è³‡æ–™é›†å·²æ­£ç¢ºè¼‰å…¥"
            }
        if not combined_data:
            return {
                "success": False,
                "error": "åˆä½µè³‡æ–™é›†ç‚ºç©º"
            }

        logger.info(f"ğŸ“Š åˆä½µè³‡æ–™é›†åŒ…å« {len(combined_data)} å€‹è³‡æ–™æº")

        # å°æ¯å€‹è³‡æ–™æºé€²è¡Œæœå°‹
        all_results = []
        total_matches = 0
        total_processed = 0

        for dataset in combined_data:
            source = dataset.get('source', 'unknown')
            data = dataset.get('data', [])

            if not data:
                continue

            logger.info(f"ğŸ” æœå°‹ {source} è³‡æ–™æº ({len(data)} ç­†è³‡æ–™)")

            # å°‡è³‡æ–™è½‰æ›ç‚ºDataFrameé€²è¡Œæœå°‹
            import pandas as pd
            df = pd.DataFrame(data)

            # æª¢æŸ¥æœå°‹æ¬„ä½æ˜¯å¦å­˜åœ¨
            available_columns = df.columns.tolist()
            valid_columns = [col for col in search_columns if col in available_columns]
            invalid_columns = [col for col in search_columns if col not in available_columns]

            if invalid_columns:
                logger.warning(f"âš ï¸ {source} è³‡æ–™æºä¸­ä»¥ä¸‹æ¬„ä½ä¸å­˜åœ¨: {invalid_columns}")

            if not valid_columns:
                logger.warning(f"âš ï¸ {source} è³‡æ–™æºä¸­æ²’æœ‰æœ‰æ•ˆçš„æœå°‹æ¬„ä½")
                continue

            # ä½¿ç”¨éˆæ´»æŒ‡ç´‹æœå°‹å¼•æ“é€²è¡Œæœå°‹
            from .flexible_fingerprint_search_tool import flexible_search_engine

            # å‰µå»ºè‡¨æ™‚CSVæª”æ¡ˆé€²è¡Œæœå°‹
            import tempfile
            import os

            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8-sig') as temp_file:
                df.to_csv(temp_file.name, index=False)
                temp_path = temp_file.name

            try:
                # åŸ·è¡Œæœå°‹
                result_df, search_info = await flexible_search_engine.search_csv_flexible(
                    temp_path, search_query, valid_columns, similarity_threshold, max_results
                )

                # ç‚ºçµæœæ·»åŠ è³‡æ–™æºæ¨™è­˜
                if not result_df.empty:
                    result_df['_data_source'] = source
                    result_df['_dataset_date'] = dataset.get('date', '')
                    result_df['_dataset_time'] = dataset.get('time', '')

                    # æ·»åŠ åˆ°ç¸½çµæœä¸­
                    for _, row in result_df.iterrows():
                        all_results.append({
                            "data_source": source,
                            "dataset_date": dataset.get('date', ''),
                            "dataset_time": dataset.get('time', ''),
                            "similarity_score": row.get('_similarity_score', 0),
                            "data": {k: v for k, v in row.items() if not k.startswith('_')}
                        })

                total_matches += search_info.get('matches_found', 0)
                total_processed += search_info.get('total_processed', 0)

                logger.info(f"âœ… {source} æœå°‹å®Œæˆ: {search_info.get('matches_found', 0)} ç­†åŒ¹é…")

            finally:
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        # æŒ‰ç›¸ä¼¼åº¦æ’åºæ‰€æœ‰çµæœ
        all_results.sort(key=lambda x: x['similarity_score'], reverse=True)

        # é™åˆ¶çµæœæ•¸é‡
        if max_results and len(all_results) > max_results:
            all_results = all_results[:max_results]

        # ä¿å­˜çµæœï¼ˆå¦‚æœéœ€è¦ï¼‰
        results_file = None
        if save_results and all_results:
            from src.tools.session_data_manager import session_data_manager
            from datetime import datetime

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"combined_search_results_{timestamp}.json"
            results_file = session_data_manager.get_temp_file_path(session_id, filename)

            import json
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… åˆä½µæœå°‹çµæœå·²ä¿å­˜åˆ°: {results_file}")

        return {
            "success": True,
            "total_matches": total_matches,
            "total_processed": total_processed,
            "search_columns": search_columns,
            "results_file": results_file,
            "sample_results": all_results[:5],  # è¿”å›å‰5å€‹çµæœä½œç‚ºæ¨£æœ¬
            "search_info": {
                "matches_found": total_matches,
                "total_processed": total_processed,
                "datasets_searched": len(combined_data),
                "query": search_query,
                "threshold": similarity_threshold
            },
            "message": f"åœ¨ {len(combined_data)} å€‹è³‡æ–™æºçš„æ¬„ä½ {search_columns} ä¸­æ‰¾åˆ° {total_matches} ç­†åŒ¹é…çµæœ"
        }

    except Exception as e:
        logger.error(f"âŒ åˆä½µè³‡æ–™é›†æœå°‹å¤±æ•—: {e}")
        import traceback
        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return {
            "success": False,
            "error": f"åˆä½µè³‡æ–™é›†æœå°‹å¤±æ•—: {str(e)}"
        }


@tool
async def convert_gmail_csv_format_tool(
    input_file: str,
    output_file: str = None,
    preview_only: bool = False,
    session_id: str = "default"
) -> str:
    """
    è½‰æ› Gmail CSV æ ¼å¼ç‚ºæ¨™æº–åŒ–æ ¼å¼

    ä¿®æ”¹å…§å®¹ï¼š
    1. ç§»é™¤ id æ¬„ä½
    2. ç°¡åŒ– sender æ ¼å¼ï¼ˆåªä¿ç•™åç¨±å’Œéƒµç®±ï¼Œç§»é™¤å¤šé¤˜å¼•è™Ÿï¼‰
    3. æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼ç‚ºæ•¸å­—é¡¯ç¤ºï¼ˆYYYY-MM-DD HH:MM:SSï¼‰

    Args:
        input_file: è¼¸å…¥ CSV æ–‡ä»¶è·¯å¾‘
        output_file: è¼¸å‡º CSV æ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼Œé è¨­ç‚ºåŸæ–‡ä»¶å_formatted.csvï¼‰
        preview_only: æ˜¯å¦åªé è¦½è½‰æ›çµæœè€Œä¸å¯¦éš›è½‰æ›
        session_id: æœƒè©±ID

    Returns:
        è½‰æ›çµæœçš„JSONå­—ç¬¦ä¸²

    Examples:
        åŸå§‹æ ¼å¼: "èˆ’åŸ¹åŸ¹" <peipeishu93@gmail.com>, Wed, 20 Aug 2025 15:12:34 +0800
        è½‰æ›å¾Œ: èˆ’åŸ¹åŸ¹ <peipeishu93@gmail.com>, 2025-08-20 15:12:34
    """
    try:
        if not CSV_CONVERTER_AVAILABLE:
            return json.dumps({
                "success": False,
                "error": "CSV æ ¼å¼è½‰æ›åŠŸèƒ½ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç›¸é—œä¾è³´"
            }, ensure_ascii=False)

        logger.info(f"ğŸ”„ {'é è¦½' if preview_only else 'è½‰æ›'} Gmail CSV æ ¼å¼: {input_file}")

        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        if not os.path.exists(input_file):
            return json.dumps({
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }, ensure_ascii=False)

        if preview_only:
            # åªé è¦½è½‰æ›çµæœ
            preview_result = preview_gmail_csv_conversion(input_file, num_rows=3)
            return json.dumps({
                "success": True,
                "preview": True,
                "original_columns": preview_result["original"]["columns"],
                "converted_columns": preview_result["converted"]["columns"],
                "sample_original": preview_result["original"]["sample_data"],
                "sample_converted": preview_result["converted"]["sample_data"],
                "changes": preview_result["changes"],
                "message": "é è¦½è½‰æ›çµæœï¼Œæœªå¯¦éš›ä¿®æ”¹æ–‡ä»¶"
            }, ensure_ascii=False)
        else:
            # å¯¦éš›è½‰æ›æ–‡ä»¶
            result_file = convert_gmail_csv_format(input_file, output_file)
            return json.dumps({
                "success": True,
                "preview": False,
                "input_file": input_file,
                "output_file": result_file,
                "message": f"CSV æ ¼å¼è½‰æ›å®Œæˆï¼Œçµæœä¿å­˜åˆ°: {result_file}"
            }, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ CSV æ ¼å¼è½‰æ›å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": f"CSV æ ¼å¼è½‰æ›å¤±æ•—: {str(e)}"
        }, ensure_ascii=False)


# ç²å–æ‰€æœ‰æœ¬åœ°æ–‡ä»¶å·¥å…·
def get_langchain_local_file_tools() -> List:
    """ç²å–æ‰€æœ‰ LangChain å…¼å®¹çš„æœ¬åœ°æ–‡ä»¶å·¥å…·"""
    tools = [
        # åŸºæœ¬æ–‡ä»¶æ“ä½œå·¥å…·
        read_file_with_summary_tool,
        edit_file_by_lines_tool,
        highlight_file_sections_tool,
        save_file_tool,
        create_file_tool,
        delete_file_tool,
        # æ•¸æ“šæ–‡ä»¶å·¥å…·
        read_data_file_tool,
        edit_data_file_tool,
        # æ•¸æ“šåˆ†æå·¥å…·
        get_data_info_tool,
        group_by_analysis_tool,
        threshold_analysis_tool,
        correlation_analysis_tool,
        linear_prediction_tool,
        # æ–°å¢çš„æ•¸æ“šCRUDå·¥å…·
        filter_data_tool,
        filter_and_analyze_tool,
        cleanup_temp_files_tool,
        get_session_data_status_tool,
        clear_session_data_tool,
        suggest_analysis_operation_tool,
        create_data_file_tool,
        update_data_rows_tool,
        delete_data_rows_tool,
        # âŒ analyze_combined_datasets_tool å·²åˆªé™¤ï¼Œè«‹ä½¿ç”¨æ–°çš„å¤šæª”æ¡ˆå·¥å…·
    ]

    # æ·»åŠ å¤šæª”æ¡ˆåˆ†æå·¥å…·
    if MULTI_FILE_TOOLS_AVAILABLE:
        tools.extend([
            multi_file_reader_tool,
            multi_file_filter_tool,
            multi_file_analyzer_tool,
            multi_file_data_analyzer_tool  # æ–°å¢ï¼šè™•ç†é è™•ç†æ•¸æ“šçš„åˆ†æå·¥å…·
        ])
        logger.info("âœ… å¤šæª”æ¡ˆåˆ†æå·¥å…·å·²æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨")

    # æ·»åŠ  Gmail æŒ‡ç´‹æœå°‹å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if FINGERPRINT_SEARCH_AVAILABLE:
        tools.append(gmail_fingerprint_search_tool)
        logger.info("âœ… Gmail æŒ‡ç´‹æœå°‹å·¥å…·å·²æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨")

    # æ·»åŠ é€šç”¨æŒ‡ç´‹æœå°‹å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if FLEXIBLE_FINGERPRINT_SEARCH_AVAILABLE:
        tools.append(fingerprint_search_tool)
        logger.info("âœ… é€šç”¨æŒ‡ç´‹æœå°‹å·¥å…·å·²æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨")

    # æ·»åŠ  CSV æ ¼å¼è½‰æ›å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if CSV_CONVERTER_AVAILABLE:
        tools.append(convert_gmail_csv_format_tool)
        logger.info("âœ… CSV æ ¼å¼è½‰æ›å·¥å…·å·²æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨")

    # æ·»åŠ æ“´å±•å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if EXTENDED_TOOLS_AVAILABLE:
        try:
            # æ·»åŠ  Task Memory å·¥å…·
            tools.extend(get_langchain_task_memory_tools())
            logger.info("âœ… Task Memory å·¥å…·å·²æ·»åŠ ")

            # ç¹ªåœ–å·¥å…·å·²ç§»é™¤ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒæ–‡ä»¶æ“ä½œåŠŸèƒ½
            logger.info("âš ï¸ ç¹ªåœ–å·¥å…·å·²ç§»é™¤")

            # æ·»åŠ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·
            tools.extend(get_batch_processor_tools())
            logger.info("âœ… æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·å·²æ·»åŠ ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ·»åŠ æ“´å±•å·¥å…·å¤±æ•—: {e}")

    return tools