"""
ç¶²é åˆ†æå·¥å…·
æä¾›ç¶²é å…§å®¹æŠ“å–ã€é€£çµæå–ã€æ–‡å­—æå–å’Œå…§å®¹ç¸½çµåŠŸèƒ½
"""

import os
import logging
import httpx
from typing import List, Dict, Any
from langchain_core.tools import tool
from langchain_openai import AzureChatOpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– LLM
llm = AzureChatOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
    temperature=0.3,
)


async def fetch_webpage_content(url: str, timeout: int = 30) -> Dict[str, Any]:
    """
    æŠ“å–ç¶²é å…§å®¹
    
    Args:
        url: ç›®æ¨™ç¶²å€
        timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    
    Returns:
        åŒ…å« HTML å…§å®¹å’Œç‹€æ…‹çš„å­—å…¸
    """
    try:
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
            # è¨­ç½® User-Agent é¿å…è¢«æ“‹
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            
            return {
                "success": True,
                "url": str(response.url),  # æœ€çµ‚çš„ URLï¼ˆå¯èƒ½è¢«é‡å®šå‘ï¼‰
                "status_code": response.status_code,
                "content": response.text,
                "content_type": response.headers.get("content-type", ""),
            }
    except httpx.TimeoutException:
        logger.error(f"æŠ“å–ç¶²é è¶…æ™‚: {url}")
        return {"success": False, "error": "è«‹æ±‚è¶…æ™‚"}
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTP éŒ¯èª¤ {e.response.status_code}: {url}")
        return {"success": False, "error": f"HTTP {e.response.status_code}"}
    except Exception as e:
        logger.error(f"æŠ“å–ç¶²é å¤±æ•—: {str(e)}")
        return {"success": False, "error": str(e)}


def extract_links_from_html(html_content: str, base_url: str) -> List[Dict[str, str]]:
    """
    å¾ HTML æå–æ‰€æœ‰é€£çµ
    
    Args:
        html_content: HTML å…§å®¹
        base_url: åŸºç¤ URLï¼ˆç”¨æ–¼ç›¸å°é€£çµè½‰æ›ï¼‰
    
    Returns:
        é€£çµåˆ—è¡¨ï¼Œæ¯å€‹é€£çµåŒ…å« urlã€textã€title
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    links = []
    
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        # è½‰æ›ç›¸å°é€£çµç‚ºçµ•å°é€£çµ
        absolute_url = urljoin(base_url, href)
        
        # æå–é€£çµæ–‡å­—å’Œæ¨™é¡Œ
        text = link.get_text(strip=True)
        title = link.get('title', '')
        
        links.append({
            "url": absolute_url,
            "text": text or "(ç„¡æ–‡å­—)",
            "title": title,
            "original_href": href
        })
    
    return links


def extract_text_content(html_content: str) -> Dict[str, Any]:
    """
    å¾ HTML æå–çµæ§‹åŒ–æ–‡å­—å…§å®¹
    
    Args:
        html_content: HTML å…§å®¹
    
    Returns:
        åŒ…å«æ¨™é¡Œã€æ®µè½ã€åˆ—è¡¨ç­‰çµæ§‹åŒ–æ–‡å­—
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤ script å’Œ style æ¨™ç±¤
    for script in soup(["script", "style"]):
        script.decompose()
    
    # æå–æ¨™é¡Œ
    title = soup.find('title')
    title_text = title.get_text(strip=True) if title else ""
    
    # æå–å„ç´šæ¨™é¡Œ
    headings = []
    for i in range(1, 7):
        for heading in soup.find_all(f'h{i}'):
            headings.append({
                "level": i,
                "text": heading.get_text(strip=True)
            })
    
    # æå–æ®µè½
    paragraphs = []
    for p in soup.find_all('p'):
        text = p.get_text(strip=True)
        if text:  # éæ¿¾ç©ºæ®µè½
            paragraphs.append(text)
    
    # æå–åˆ—è¡¨é …ç›®
    list_items = []
    for li in soup.find_all('li'):
        text = li.get_text(strip=True)
        if text:
            list_items.append(text)
    
    # æå– meta æè¿°
    meta_description = ""
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if meta_desc:
        meta_description = meta_desc.get('content', '')
    
    # ç²å–å…¨æ–‡ï¼ˆç”¨æ–¼ç¸½çµï¼‰
    full_text = soup.get_text(separator=' ', strip=True)
    
    return {
        "title": title_text,
        "meta_description": meta_description,
        "headings": headings,
        "paragraphs": paragraphs,
        "list_items": list_items,
        "full_text": full_text[:5000]  # é™åˆ¶é•·åº¦é¿å… token éå¤š
    }


async def summarize_content(text_content: Dict[str, Any]) -> str:
    """
    ä½¿ç”¨ LLM ç¸½çµç¶²é å…§å®¹
    
    Args:
        text_content: çµæ§‹åŒ–æ–‡å­—å…§å®¹
    
    Returns:
        å…§å®¹ç¸½çµ
    """
    try:
        # æº–å‚™ç¸½çµçš„å…§å®¹
        summary_parts = []
        
        if text_content.get("title"):
            summary_parts.append(f"æ¨™é¡Œ: {text_content['title']}")
        
        if text_content.get("meta_description"):
            summary_parts.append(f"æè¿°: {text_content['meta_description']}")
        
        # æ·»åŠ å‰å¹¾å€‹æ¨™é¡Œ
        headings = text_content.get("headings", [])[:5]
        if headings:
            heading_texts = [f"H{h['level']}: {h['text']}" for h in headings]
            summary_parts.append("ä¸»è¦æ¨™é¡Œ:\n" + "\n".join(heading_texts))
        
        # æ·»åŠ å‰å¹¾å€‹æ®µè½
        paragraphs = text_content.get("paragraphs", [])[:3]
        if paragraphs:
            summary_parts.append("ä¸»è¦å…§å®¹:\n" + "\n".join(paragraphs))
        
        content_to_summarize = "\n\n".join(summary_parts)
        
        # ä½¿ç”¨ LLM ç”Ÿæˆç¸½çµ
        prompt = f"""è«‹ç‚ºä»¥ä¸‹ç¶²é å…§å®¹æä¾›ä¸€å€‹ç°¡æ½”çš„ä¸­æ–‡ç¸½çµï¼ˆç´„100-200å­—ï¼‰ï¼š

{content_to_summarize}

ç¸½çµè¦é»ï¼š
1. ç¶²é çš„ä¸»è¦ä¸»é¡Œ
2. é—œéµä¿¡æ¯å’Œé‡é»
3. ç¶²é çš„ç”¨é€”æˆ–ç›®æ¨™å—çœ¾"""
        
        response = await llm.ainvoke(prompt)
        return response.content
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç¸½çµå¤±æ•—: {str(e)}")
        return "ç„¡æ³•ç”Ÿæˆç¸½çµ"


@tool
async def analyze_webpage(url: str, extract_links: bool = True, summarize: bool = True) -> str:
    """
    åˆ†æç¶²é å…§å®¹ï¼Œæå–é€£çµã€æ–‡å­—ä¸¦ç”Ÿæˆç¸½çµ
    
    Args:
        url: è¦åˆ†æçš„ç¶²é  URL
        extract_links: æ˜¯å¦æå–é€£çµï¼ˆé è¨­ç‚º Trueï¼‰
        summarize: æ˜¯å¦ç”Ÿæˆå…§å®¹ç¸½çµï¼ˆé è¨­ç‚º Trueï¼‰
    
    Returns:
        ç¶²é åˆ†æçµæœï¼ŒåŒ…å«é€£çµã€æ–‡å­—å…§å®¹å’Œç¸½çµ
    """
    try:
        logger.info(f"é–‹å§‹åˆ†æç¶²é : {url}")
        
        # æŠ“å–ç¶²é å…§å®¹
        fetch_result = await fetch_webpage_content(url)
        if not fetch_result["success"]:
            return f"âŒ ç„¡æ³•æŠ“å–ç¶²é : {fetch_result['error']}"
        
        html_content = fetch_result["content"]
        final_url = fetch_result["url"]
        
        # æå–æ–‡å­—å…§å®¹
        text_content = extract_text_content(html_content)
        
        # æº–å‚™çµæœ
        result_parts = []
        result_parts.append(f"ğŸŒ ç¶²é åˆ†æçµæœ")
        result_parts.append(f"URL: {final_url}")
        result_parts.append(f"æ¨™é¡Œ: {text_content['title']}")
        
        if text_content.get("meta_description"):
            result_parts.append(f"æè¿°: {text_content['meta_description']}")
        
        # æå–é€£çµï¼ˆå¦‚æœéœ€è¦ï¼‰
        if extract_links:
            links = extract_links_from_html(html_content, final_url)
            result_parts.append(f"\nğŸ“ æ‰¾åˆ° {len(links)} å€‹é€£çµ")
            
            # é¡¯ç¤ºå‰10å€‹é€£çµ
            for i, link in enumerate(links[:10]):
                result_parts.append(f"{i+1}. [{link['text']}]({link['url']})")
            
            if len(links) > 10:
                result_parts.append(f"... é‚„æœ‰ {len(links) - 10} å€‹é€£çµ")
        
        # ç”Ÿæˆç¸½çµï¼ˆå¦‚æœéœ€è¦ï¼‰
        if summarize:
            summary = await summarize_content(text_content)
            result_parts.append(f"\nğŸ“ å…§å®¹ç¸½çµ:\n{summary}")
        
        # é¡¯ç¤ºä¸»è¦æ¨™é¡Œ
        headings = text_content.get("headings", [])[:5]
        if headings:
            result_parts.append("\nğŸ“‹ ä¸»è¦ç« ç¯€:")
            for h in headings:
                indent = "  " * (h['level'] - 1)
                result_parts.append(f"{indent}â€¢ {h['text']}")
        
        return "\n".join(result_parts)
        
    except Exception as e:
        logger.error(f"åˆ†æç¶²é å¤±æ•—: {str(e)}", exc_info=True)
        return f"âŒ åˆ†æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


@tool
async def extract_webpage_links(url: str) -> str:
    """
    æå–ç¶²é ä¸­çš„æ‰€æœ‰é€£çµ
    
    Args:
        url: è¦åˆ†æçš„ç¶²é  URL
    
    Returns:
        ç¶²é ä¸­æ‰€æœ‰é€£çµçš„åˆ—è¡¨
    """
    try:
        logger.info(f"æå–ç¶²é é€£çµ: {url}")
        
        # æŠ“å–ç¶²é å…§å®¹
        fetch_result = await fetch_webpage_content(url)
        if not fetch_result["success"]:
            return f"âŒ ç„¡æ³•æŠ“å–ç¶²é : {fetch_result['error']}"
        
        html_content = fetch_result["content"]
        final_url = fetch_result["url"]
        
        # æå–é€£çµ
        links = extract_links_from_html(html_content, final_url)
        
        # æŒ‰åŸŸååˆ†çµ„é€£çµ
        links_by_domain = {}
        for link in links:
            parsed = urlparse(link['url'])
            domain = parsed.netloc or 'relative'
            if domain not in links_by_domain:
                links_by_domain[domain] = []
            links_by_domain[domain].append(link)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        result_parts = []
        result_parts.append(f"ğŸ”— å¾ {final_url} æå–åˆ° {len(links)} å€‹é€£çµ")
        
        for domain, domain_links in sorted(links_by_domain.items()):
            result_parts.append(f"\nğŸ“Œ {domain} ({len(domain_links)} å€‹é€£çµ):")
            for link in domain_links[:5]:  # æ¯å€‹åŸŸåé¡¯ç¤ºå‰5å€‹
                result_parts.append(f"  â€¢ [{link['text']}]({link['url']})")
            if len(domain_links) > 5:
                result_parts.append(f"  ... é‚„æœ‰ {len(domain_links) - 5} å€‹é€£çµ")
        
        return "\n".join(result_parts)
        
    except Exception as e:
        logger.error(f"æå–é€£çµå¤±æ•—: {str(e)}", exc_info=True)
        return f"âŒ æå–é€£çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


@tool
async def extract_webpage_text(url: str) -> str:
    """
    æå–ç¶²é çš„ç´”æ–‡å­—å…§å®¹
    
    Args:
        url: è¦åˆ†æçš„ç¶²é  URL
    
    Returns:
        ç¶²é çš„çµæ§‹åŒ–æ–‡å­—å…§å®¹
    """
    try:
        logger.info(f"æå–ç¶²é æ–‡å­—: {url}")
        
        # æŠ“å–ç¶²é å…§å®¹
        fetch_result = await fetch_webpage_content(url)
        if not fetch_result["success"]:
            return f"âŒ ç„¡æ³•æŠ“å–ç¶²é : {fetch_result['error']}"
        
        html_content = fetch_result["content"]
        
        # æå–æ–‡å­—å…§å®¹
        text_content = extract_text_content(html_content)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        result_parts = []
        result_parts.append(f"ğŸ“„ ç¶²é æ–‡å­—å…§å®¹")
        result_parts.append(f"æ¨™é¡Œ: {text_content['title']}")
        
        if text_content.get("meta_description"):
            result_parts.append(f"æè¿°: {text_content['meta_description']}")
        
        # é¡¯ç¤ºæ¨™é¡Œçµæ§‹
        headings = text_content.get("headings", [])
        if headings:
            result_parts.append("\nğŸ“‘ å…§å®¹çµæ§‹:")
            for h in headings[:10]:  # é¡¯ç¤ºå‰10å€‹æ¨™é¡Œ
                indent = "  " * (h['level'] - 1)
                result_parts.append(f"{indent}H{h['level']}: {h['text']}")
        
        # é¡¯ç¤ºå‰å¹¾å€‹æ®µè½
        paragraphs = text_content.get("paragraphs", [])
        if paragraphs:
            result_parts.append("\nğŸ“ ä¸»è¦æ®µè½:")
            for i, p in enumerate(paragraphs[:5]):
                result_parts.append(f"\næ®µè½ {i+1}:\n{p}")
        
        # é¡¯ç¤ºåˆ—è¡¨é …ç›®
        list_items = text_content.get("list_items", [])
        if list_items:
            result_parts.append(f"\nğŸ“‹ åˆ—è¡¨é …ç›® (å…± {len(list_items)} é …):")
            for item in list_items[:10]:
                result_parts.append(f"  â€¢ {item}")
        
        return "\n".join(result_parts)
        
    except Exception as e:
        logger.error(f"æå–æ–‡å­—å¤±æ•—: {str(e)}", exc_info=True)
        return f"âŒ æå–æ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


# å·¥å…·åˆ—è¡¨ä¾› tool_manager ä½¿ç”¨
webpage_tools = [
    analyze_webpage,
    extract_webpage_links,
    extract_webpage_text,
]