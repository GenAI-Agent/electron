"""
å¤šæª”æ¡ˆåˆ†æå·¥å…·é›†
æä¾›å¤šæª”æ¡ˆé–±è®€ã€éæ¿¾ã€ç¶œåˆåˆ†æåŠŸèƒ½
"""

import os
import json
import asyncio
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Optional
from langchain_core.tools import tool
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
import logging

load_dotenv()
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– LLM (ä½¿ç”¨ç¬¬äºŒçµ„é…ç½®)
llm = AzureChatOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT_2"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY_2"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    temperature=0.4,
)

# è³‡æ–™ç›®éŒ„
SANDBOX_DATA_DIR = Path(__file__).parent.parent.parent.parent / "data" / "sandbox"

async def _text_search_filter(file_result: Dict[str, Any], filter_condition: str) -> Dict[str, Any]:
    """
    æ–‡å­—å…§å®¹éæ¿¾å‡½æ•¸ - æ”¯æŒé—œéµå­—æœå°‹å’Œ fingerprint åŒ¹é…

    Args:
        file_result: æª”æ¡ˆè®€å–çµæœ
        filter_condition: éæ¿¾æ¢ä»¶ï¼ˆè‡ªç„¶èªè¨€æˆ–é—œéµå­—ï¼‰

    Returns:
        éæ¿¾çµæœ
    """
    try:
        data = file_result.get("data", [])
        if not data:
            return {"success": False, "error": "ç„¡æ•¸æ“šå¯éæ¿¾"}

        # å°‡éæ¿¾æ¢ä»¶è½‰æ›ç‚ºé—œéµå­—åˆ—è¡¨
        keywords = []
        if "," in filter_condition:
            # é€—è™Ÿåˆ†éš”çš„é—œéµå­—
            keywords = [k.strip() for k in filter_condition.split(",")]
        else:
            # è‡ªç„¶èªè¨€ï¼Œæå–é—œéµå­—
            keywords = _extract_keywords(filter_condition)

        print(f"ğŸ” æå–çš„é—œéµå­—: {keywords}")

        # å°æ¯è¡Œæ•¸æ“šé€²è¡Œæ–‡å­—æœå°‹
        filtered_data = []
        for row in data:
            if _match_keywords(row, keywords):
                filtered_data.append(row)

        return {
            "success": True,
            "data": filtered_data,
            "original_rows": len(data),
            "filtered_rows": len(filtered_data),
            "columns": list(data[0].keys()) if data else [],
            "filter_keywords": keywords
        }

    except Exception as e:
        logger.error(f"æ–‡å­—æœå°‹éæ¿¾å¤±æ•—: {e}")
        return {"success": False, "error": f"æ–‡å­—æœå°‹å¤±æ•—: {str(e)}"}

def _extract_keywords(text: str) -> List[str]:
    """å¾è‡ªç„¶èªè¨€ä¸­æå–é—œéµå­—"""
    # æ•™è‚²ç›¸é—œé—œéµå­—æ˜ å°„
    education_keywords = {
        "æ•™è‚²": ["æ•™è‚²", "å­¸æ ¡", "è€å¸«", "å­¸ç”Ÿ", "èª²ç¨‹", "æ•™å­¸", "å­¸ç¿’", "è€ƒè©¦", "æˆç¸¾", "ç•¢æ¥­"],
        "é†«ç™‚": ["é†«ç™‚", "é†«é™¢", "é†«ç”Ÿ", "è­·å£«", "ç—…äºº", "æ²»ç™‚", "è—¥ç‰©", "å¥åº·", "ç–¾ç—…"],
        "ç§‘æŠ€": ["ç§‘æŠ€", "æŠ€è¡“", "è»Ÿé«”", "ç¡¬é«”", "ç¨‹å¼", "é–‹ç™¼", "AI", "äººå·¥æ™ºæ…§", "é›»è…¦"],
        "æ”¿æ²»": ["æ”¿æ²»", "æ”¿åºœ", "é¸èˆ‰", "æ”¿ç­–", "æ³•å¾‹", "è­°å“¡", "å¸‚é•·", "ç¸½çµ±", "ç«‹æ³•"],
        "ç¶“æ¿Ÿ": ["ç¶“æ¿Ÿ", "é‡‘è", "æŠ•è³‡", "è‚¡ç¥¨", "éŠ€è¡Œ", "è²¿æ˜“", "å•†æ¥­", "ä¼æ¥­", "å¸‚å ´"]
    }

    text_lower = text.lower()
    keywords = []

    # æª¢æŸ¥æ˜¯å¦åŒ…å«é å®šç¾©çš„ä¸»é¡Œé—œéµå­—
    for topic, topic_keywords in education_keywords.items():
        if topic in text or any(kw in text for kw in topic_keywords):
            keywords.extend(topic_keywords)
            break

    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°é å®šç¾©ä¸»é¡Œï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡ä½œç‚ºé—œéµå­—
    if not keywords:
        keywords = [text.strip()]

    return keywords

def _match_keywords(row: Dict[str, Any], keywords: List[str]) -> bool:
    """æª¢æŸ¥æ•¸æ“šè¡Œæ˜¯å¦åŒ¹é…é—œéµå­—"""
    # å°‡æ‰€æœ‰æ¬„ä½å€¼è½‰æ›ç‚ºå­—ä¸²ä¸¦åˆä½µ
    text_content = " ".join(str(value) for value in row.values()).lower()

    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€é—œéµå­—
    return any(keyword.lower() in text_content for keyword in keywords)

async def read_single_file_async(file_path: str, sample_size: int = 100) -> Dict[str, Any]:
    """ç•°æ­¥è®€å–å–®å€‹æª”æ¡ˆ"""
    try:
        # è™•ç†ç›¸å°è·¯å¾‘
        if not Path(file_path).is_absolute():
            if file_path.startswith("../data/sandbox/"):
                # å‰ç«¯å‚³ä¾†çš„ç›¸å°è·¯å¾‘
                filename = file_path.split("/")[-1]
                full_path = SANDBOX_DATA_DIR / filename
            else:
                full_path = SANDBOX_DATA_DIR / file_path
        else:
            full_path = Path(file_path)
        
        if not full_path.exists():
            return {
                "filename": str(file_path),
                "success": False,
                "error": f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}"
            }
        
        # è®€å–CSVæª”æ¡ˆ
        df = pd.read_csv(full_path, encoding='utf-8-sig')
        
        # å–æ¨£æœ¬æ•¸æ“š
        sample_data = df.head(sample_size).to_dict('records')
        
        # è¨ˆç®—çµ±è¨ˆ
        stats = {
            "total_rows": len(df),
            "columns": list(df.columns),
            "sample_rows": len(sample_data),
            "file_size": full_path.stat().st_size,
            "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}
        }
        
        return {
            "filename": str(file_path),
            "success": True,
            "data": sample_data,
            "stats": stats,
            "full_dataframe": df  # ä¿ç•™å®Œæ•´æ•¸æ“šæ¡†ç”¨æ–¼å¾ŒçºŒè™•ç†
        }
        
    except Exception as e:
        logger.error(f"è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}")
        return {
            "filename": str(file_path),
            "success": False,
            "error": str(e)
        }

@tool
async def multi_file_reader_tool(
    file_paths: str,
    sample_size: int = 100,
    session_id: str = "default"
) -> str:
    """
    å¤šæª”æ¡ˆé–±è®€å·¥å…· - ä¸¦è¡Œè®€å–å¤šå€‹æª”æ¡ˆä¸¦è¿”å›åŸºæœ¬çµ±è¨ˆå’Œæ¨£æœ¬æ•¸æ“š

    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨çš„JSONå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: ["file1.csv", "file2.csv"]
        sample_size: æ¯å€‹æª”æ¡ˆçš„æ¨£æœ¬æ•¸æ“šè¡Œæ•¸ï¼Œé»˜èª100
        session_id: æœƒè©±ID

    Returns:
        è®€å–çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”„ multi_file_reader_tool é–‹å§‹åŸ·è¡Œ")
        print(f"ğŸ“¥ æ¥æ”¶åˆ°çš„ file_paths åƒæ•¸: {file_paths}")
        print(f"ğŸ“¥ file_paths é¡å‹: {type(file_paths)}")
        print(f"ğŸ“¥ sample_size: {sample_size}, session_id: {session_id}")

        logger.info(f"ğŸ”„ multi_file_reader_tool é–‹å§‹åŸ·è¡Œ")
        logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°çš„ file_paths åƒæ•¸: {file_paths}")
        logger.info(f"ğŸ“¥ file_paths é¡å‹: {type(file_paths)}")
        logger.info(f"ğŸ“¥ sample_size: {sample_size}, session_id: {session_id}")

        # æª¢æŸ¥ file_paths åƒæ•¸
        if not file_paths:
            logger.error(f"âŒ file_paths åƒæ•¸ç‚ºç©º")
            return json.dumps({
                "success": False,
                "error": "æ²’æœ‰æä¾›æª”æ¡ˆè·¯å¾‘ã€‚è«‹å…ˆåœ¨ AI Sandbox é é¢é¸æ“‡è¦åˆ†æçš„è³‡æ–™é›†ã€‚",
                "session_id": session_id,
                "user_action_required": "è«‹å…ˆé¸æ“‡è³‡æ–™é›†"
            }, ensure_ascii=False)

        # è§£ææª”æ¡ˆè·¯å¾‘
        if isinstance(file_paths, str):
            try:
                paths = json.loads(file_paths)
                logger.info(f"âœ… JSON è§£ææˆåŠŸ: {paths}")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON è§£æå¤±æ•—: {e}")
                return json.dumps({
                    "success": False,
                    "error": f"æª”æ¡ˆè·¯å¾‘æ ¼å¼éŒ¯èª¤: {str(e)}ã€‚è«‹ç¢ºä¿æä¾›æ­£ç¢ºçš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ã€‚",
                    "session_id": session_id
                }, ensure_ascii=False)
        else:
            paths = file_paths
            logger.info(f"âœ… ç›´æ¥ä½¿ç”¨è·¯å¾‘åˆ—è¡¨: {paths}")

        # æª¢æŸ¥è·¯å¾‘åˆ—è¡¨æ˜¯å¦ç‚ºç©º
        if not paths or len(paths) == 0:
            logger.error(f"âŒ æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ç‚ºç©º")
            return json.dumps({
                "success": False,
                "error": "æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ç‚ºç©ºã€‚è«‹å…ˆåœ¨ AI Sandbox é é¢é¸æ“‡è¦åˆ†æçš„è³‡æ–™é›†ã€‚",
                "session_id": session_id,
                "user_action_required": "è«‹å…ˆé¸æ“‡è³‡æ–™é›†"
            }, ensure_ascii=False)

        logger.info(f"ğŸ“ æº–å‚™è®€å– {len(paths)} å€‹æª”æ¡ˆ: {paths}")
        
        # ä¸¦è¡Œè®€å–æ‰€æœ‰æª”æ¡ˆ
        tasks = [read_single_file_async(path, sample_size) for path in paths]
        results = await asyncio.gather(*tasks)
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        successful_files = [r for r in results if r["success"]]
        total_rows = sum(r["stats"]["total_rows"] for r in successful_files)
        
        summary = {
            "total_files": len(paths),
            "successful_files": len(successful_files),
            "failed_files": len(paths) - len(successful_files),
            "total_rows": total_rows,
            "sample_size": sample_size
        }
        
        # ç§»é™¤å®Œæ•´æ•¸æ“šæ¡†ï¼ˆé¿å…åºåˆ—åŒ–å•é¡Œï¼‰
        for result in results:
            if "full_dataframe" in result:
                del result["full_dataframe"]
        
        response = {
            "success": True,
            "results": results,
            "summary": summary,
            "session_id": session_id
        }
        
        print(f"âœ… å¤šæª”æ¡ˆè®€å–å®Œæˆ: {len(successful_files)}/{len(paths)} æˆåŠŸ")
        logger.info(f"âœ… å¤šæª”æ¡ˆè®€å–å®Œæˆ: {len(successful_files)}/{len(paths)} æˆåŠŸ")

        # ğŸ”§ ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½å¯ä»¥ JSON åºåˆ—åŒ–
        try:
            result_json = json.dumps(response, ensure_ascii=False)
            print(f"âœ… JSON åºåˆ—åŒ–æˆåŠŸ")
            return result_json
        except TypeError as e:
            print(f"âŒ JSON åºåˆ—åŒ–å¤±æ•—: {e}")
            print(f"âŒ å•é¡Œå¯èƒ½åœ¨ response ä¸­çš„æŸå€‹å­—æ®µ")
            logger.error(f"âŒ JSON åºåˆ—åŒ–å¤±æ•—: {e}")

            # æª¢æŸ¥æ¯å€‹çµæœä¸­çš„å•é¡Œå­—æ®µ
            for i, result in enumerate(results):
                try:
                    json.dumps(result)
                except TypeError as result_error:
                    print(f"âŒ çµæœ {i} åºåˆ—åŒ–å¤±æ•—: {result_error}")
                    print(f"âŒ çµæœ {i} çš„ keys: {list(result.keys())}")

            # è¿”å›ç°¡åŒ–ç‰ˆæœ¬
            simplified_response = {
                "success": True,
                "message": f"æˆåŠŸè®€å– {len(successful_files)}/{len(paths)} å€‹æª”æ¡ˆ",
                "summary": summary,
                "session_id": session_id
            }
            return json.dumps(simplified_response, ensure_ascii=False)
        
    except Exception as e:
        logger.error(f"âŒ å¤šæª”æ¡ˆè®€å–å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "session_id": session_id
        }, ensure_ascii=False)

@tool
async def multi_file_filter_tool(
    file_paths: str,
    filter_condition: str,
    sample_size: int = 1000,
    session_id: str = "default"
) -> str:
    """
    å¤šæª”æ¡ˆéæ¿¾å·¥å…· - ä½¿ç”¨æ¢ä»¶éæ¿¾å¤šå€‹æª”æ¡ˆçš„æ•¸æ“š

    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        filter_condition: éæ¿¾æ¢ä»¶ï¼Œæ”¯æŒï¼š
                         1. è‡ªç„¶èªè¨€: "è·Ÿæ•™è‚²æœ‰é—œçš„è³‡æ–™"
                         2. çµæ§‹åŒ–æ¢ä»¶: {"column": "age", "operator": ">", "value": 30}
                         3. é—œéµå­—æœå°‹: "æ•™è‚²,å­¸æ ¡,è€å¸«"
        sample_size: æ¯å€‹æª”æ¡ˆçš„è™•ç†è¡Œæ•¸ï¼Œé»˜èª1000
        session_id: æœƒè©±ID

    Returns:
        éæ¿¾çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”„ multi_file_filter_tool é–‹å§‹åŸ·è¡Œ")
        print(f"ğŸ“¥ æ¥æ”¶åˆ°çš„ file_paths: {file_paths}")
        print(f"ğŸ” éæ¿¾æ¢ä»¶: {filter_condition}")
        print(f"ğŸ“¥ sample_size: {sample_size}")

        logger.info(f"ğŸ”„ multi_file_filter_tool é–‹å§‹åŸ·è¡Œ")
        logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°çš„ file_paths: {file_paths}")
        logger.info(f"ğŸ“¥ file_paths é¡å‹: {type(file_paths)}")
        logger.info(f"ğŸ” éæ¿¾æ¢ä»¶: {filter_condition}")
        logger.info(f"ğŸ“¥ sample_size: {sample_size}, session_id: {session_id}")

        # æª¢æŸ¥ file_paths åƒæ•¸
        if not file_paths:
            logger.error(f"âŒ file_paths åƒæ•¸ç‚ºç©º")
            return json.dumps({
                "success": False,
                "error": "æ²’æœ‰æä¾›æª”æ¡ˆè·¯å¾‘ã€‚è«‹å…ˆåœ¨ AI Sandbox é é¢é¸æ“‡è¦åˆ†æçš„è³‡æ–™é›†ã€‚",
                "session_id": session_id,
                "user_action_required": "è«‹å…ˆé¸æ“‡è³‡æ–™é›†"
            }, ensure_ascii=False)
        
        if not llm:
            return json.dumps({
                "success": False,
                "error": "LLM ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œéæ¿¾",
                "session_id": session_id
            }, ensure_ascii=False)
        
        # è§£ææª”æ¡ˆè·¯å¾‘
        paths = json.loads(file_paths) if isinstance(file_paths, str) else file_paths
        
        print(f"ğŸ“ æº–å‚™éæ¿¾ {len(paths)} å€‹æª”æ¡ˆ")

        # ğŸ”§ ä½¿ç”¨ç¾æœ‰çš„ filter_data_tool é€²è¡Œéæ¿¾
        from supervisor_agent.tools.langchain_local_file_tools import filter_data_tool

        filtered_results = []
        for path in paths:
            try:
                print(f"ğŸ” éæ¿¾æª”æ¡ˆ: {path}")
                print(f"ğŸ” éæ¿¾æ¢ä»¶: {filter_condition}")

                # å…ˆè®€å–æª”æ¡ˆä»¥åˆ¤æ–·æ•¸æ“šé¡å‹
                file_result = await read_single_file_async(path, 10)  # å…ˆè®€å–å°‘é‡æ•¸æ“šåˆ¤æ–·é¡å‹

                if not file_result["success"]:
                    filtered_results.append({
                        "filename": path,
                        "success": False,
                        "error": file_result.get("error", "è®€å–å¤±æ•—")
                    })
                    continue

                # åˆ¤æ–·éæ¿¾é¡å‹
                sample_data = file_result.get("data", [])
                if not sample_data:
                    filtered_results.append({
                        "filename": path,
                        "success": False,
                        "error": "æª”æ¡ˆç„¡æ•¸æ“š"
                    })
                    continue

                # æ™ºèƒ½åˆ¤æ–·éæ¿¾æ–¹å¼
                if filter_condition.startswith("{") and filter_condition.endswith("}"):
                    # çµæ§‹åŒ–æ¢ä»¶ï¼šä½¿ç”¨ filter_data_tool
                    filter_result_str = await filter_data_tool(
                        file_path=path,
                        filter_conditions=filter_condition,
                        session_id=session_id,
                        save_filtered_data=False
                    )
                    filter_result = json.loads(filter_result_str)
                else:
                    # è‡ªç„¶èªè¨€/é—œéµå­—æ¢ä»¶ï¼šä½¿ç”¨æ–‡å­—æœå°‹
                    filter_result = await _text_search_filter(file_result, filter_condition)

                if filter_result.get("success", False):
                    print(f"âœ… éæ¿¾æˆåŠŸ: {path}")
                    filtered_results.append({
                        "filename": path,
                        "success": True,
                        "data": filter_result.get("data", []),
                        "original_rows": filter_result.get("original_rows", 0),
                        "filtered_rows": len(filter_result.get("data", [])),
                        "filter_condition": filter_condition,
                        "stats": {
                            "total_rows": len(filter_result.get("data", [])),
                            "columns": filter_result.get("columns", [])
                        }
                    })
                else:
                    print(f"âŒ éæ¿¾å¤±æ•—: {path}")
                    filtered_results.append({
                        "filename": path,
                        "success": False,
                        "error": f"éæ¿¾å¤±æ•—: {filter_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}"
                    })
            except Exception as e:
                print(f"âŒ éæ¿¾ç•°å¸¸: {path} - {e}")
                logger.error(f"éæ¿¾æª”æ¡ˆå¤±æ•— {path}: {e}")
                filtered_results.append({
                    "filename": path,
                    "success": False,
                    "error": f"éæ¿¾å¤±æ•—: {str(e)}"
                })
        
        # è¨ˆç®—éæ¿¾çµ±è¨ˆ
        successful_results = [r for r in filtered_results if r["success"]]
        total_original_rows = sum(r.get("original_rows", 0) for r in successful_results)
        total_filtered_rows = sum(r.get("filtered_rows", 0) for r in successful_results)

        print(f"ğŸ“Š éæ¿¾çµ±è¨ˆ: {len(successful_results)}/{len(paths)} æª”æ¡ˆæˆåŠŸ")
        print(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ: {total_original_rows} â†’ {total_filtered_rows} è¡Œ")

        summary = {
            "total_files": len(paths),
            "successful_files": len(successful_results),
            "failed_files": len(paths) - len(successful_results),
            "total_original_rows": total_original_rows,
            "total_filtered_rows": total_filtered_rows,
            "filter_condition": filter_condition,
            "filter_rate": f"{(total_filtered_rows/total_original_rows*100):.1f}%" if total_original_rows > 0 else "0%"
        }
        
        response = {
            "success": True,
            "results": filtered_results,
            "summary": summary,
            "session_id": session_id
        }
        
        logger.info(f"âœ… å¤šæª”æ¡ˆéæ¿¾å®Œæˆ: {len(successful_results)}/{len(paths)} æˆåŠŸ")
        return json.dumps(response, ensure_ascii=False)
        
    except Exception as e:
        logger.error(f"âŒ å¤šæª”æ¡ˆéæ¿¾å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "session_id": session_id
        }, ensure_ascii=False)

@tool
async def multi_file_analyzer_tool(
    file_paths: str,
    analysis_type: str = "summary",
    analysis_question: str = None,
    session_id: str = "default"
) -> str:
    """
    å¤šæª”æ¡ˆç¶œåˆåˆ†æå·¥å…· - å°å¤šå€‹æª”æ¡ˆé€²è¡Œè·¨æª”æ¡ˆçš„ç¶œåˆåˆ†æ

    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨çš„JSONå­—ç¬¦ä¸²
        analysis_type: åˆ†æé¡å‹ï¼Œå¯é¸: "summary"(æ‘˜è¦), "trend"(è¶¨å‹¢), "comparison"(å°æ¯”), "statistics"(çµ±è¨ˆ)
        analysis_question: å…·é«”åˆ†æå•é¡Œï¼Œä¾‹å¦‚: "æ¯”è¼ƒä¸åŒè³‡æ–™æºçš„è¨è«–ä¸»é¡Œå·®ç•°"
        session_id: æœƒè©±ID

    Returns:
        åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”„ multi_file_analyzer_tool é–‹å§‹åŸ·è¡Œ")
        print(f"ğŸ“Š åˆ†æé¡å‹: {analysis_type}")
        print(f"â“ åˆ†æå•é¡Œ: {analysis_question}")

        logger.info(f"ğŸ”„ multi_file_analyzer_tool é–‹å§‹åŸ·è¡Œ")
        logger.info(f"ğŸ“Š åˆ†æé¡å‹: {analysis_type}")
        logger.info(f"â“ åˆ†æå•é¡Œ: {analysis_question}")

        # æª¢æŸ¥ file_paths åƒæ•¸
        if not file_paths:
            logger.error(f"âŒ file_paths åƒæ•¸ç‚ºç©º")
            return json.dumps({
                "success": False,
                "error": "æ²’æœ‰æä¾›æª”æ¡ˆè·¯å¾‘ã€‚è«‹å…ˆåœ¨ AI Sandbox é é¢é¸æ“‡è¦åˆ†æçš„è³‡æ–™é›†ã€‚",
                "session_id": session_id,
                "user_action_required": "è«‹å…ˆé¸æ“‡è³‡æ–™é›†"
            }, ensure_ascii=False)

        if not llm:
            return json.dumps({
                "success": False,
                "error": "LLM ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œåˆ†æ",
                "session_id": session_id
            }, ensure_ascii=False)

        # è§£ææª”æ¡ˆè·¯å¾‘
        paths = json.loads(file_paths) if isinstance(file_paths, str) else file_paths

        # è®€å–æ‰€æœ‰æª”æ¡ˆï¼ˆç”¨æ›´å¤šæ¨£æœ¬é€²è¡Œåˆ†æï¼‰
        file_results = []
        for path in paths:
            result = await read_single_file_async(path, 200)  # åˆ†æç”¨æ›´å¤šæ¨£æœ¬

            # ğŸ”§ ç§»é™¤ DataFrame ä»¥é¿å… JSON åºåˆ—åŒ–å•é¡Œ
            if "full_dataframe" in result:
                del result["full_dataframe"]
                print(f"ğŸ”§ å·²ç§»é™¤ {path} çš„ DataFrame")

            file_results.append(result)

        # æº–å‚™åˆ†ææ•¸æ“š
        analysis_data = []
        for result in file_results:
            if result["success"] and result["data"]:
                analysis_data.append({
                    "filename": result["filename"],
                    "stats": result["stats"],
                    "sample_data": result["data"][:20],  # å–å‰20ç­†ä½œç‚ºåˆ†ææ¨£æœ¬
                    "total_rows": result["stats"]["total_rows"],
                    "columns": result["stats"]["columns"]
                })

        if not analysis_data:
            return json.dumps({
                "success": False,
                "error": "æ²’æœ‰å¯ç”¨çš„æ•¸æ“šé€²è¡Œåˆ†æ",
                "session_id": session_id
            }, ensure_ascii=False)

        # æ§‹å»ºåˆ†æprompt
        analysis_prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ•¸æ“šåˆ†æå¸«ã€‚è«‹å°ä»¥ä¸‹å¤šå€‹æ•¸æ“šé›†é€²è¡Œ{analysis_type}åˆ†æã€‚

æ•¸æ“šé›†è³‡è¨Šï¼š
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

åˆ†æé¡å‹ï¼š{analysis_type}
"""

        if analysis_question:
            analysis_prompt += f"\nå…·é«”åˆ†æå•é¡Œï¼š{analysis_question}"

        analysis_prompt += """

è«‹æä¾›è©³ç´°çš„åˆ†æå ±å‘Šï¼ŒåŒ…å«ï¼š
1. **æ•¸æ“šæ¦‚è¦½** - å„æ•¸æ“šé›†çš„åŸºæœ¬æƒ…æ³
2. **ä¸»è¦ç™¼ç¾** - é—œéµæ´å¯Ÿå’Œç™¼ç¾
3. **è©³ç´°åˆ†æ** - æ ¹æ“šåˆ†æé¡å‹é€²è¡Œæ·±å…¥åˆ†æï¼š
   - summary: æä¾›å„æ•¸æ“šé›†çš„æ‘˜è¦å’Œç‰¹é»
   - trend: åˆ†ææ•¸æ“šçš„è¶¨å‹¢è®ŠåŒ–
   - comparison: å°æ¯”ä¸åŒæ•¸æ“šé›†çš„å·®ç•°å’Œç›¸ä¼¼æ€§
   - statistics: æä¾›çµ±è¨ˆåˆ†æå’Œæ•¸æ“šåˆ†ä½ˆ
4. **çµè«–å’Œå»ºè­°** - åŸºæ–¼åˆ†æçš„çµè«–å’Œè¡Œå‹•å»ºè­°

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦æä¾›å…·é«”çš„æ•¸æ“šæ”¯æŒã€‚æ ¼å¼è¦æ¸…æ™°æ˜“è®€ã€‚
"""

        # èª¿ç”¨LLMé€²è¡Œåˆ†æ
        response = await llm.ainvoke([{"role": "user", "content": analysis_prompt}])
        analysis_result = response.content

        # è¨ˆç®—çµ±è¨ˆ
        successful_files = len(analysis_data)
        total_rows = sum(data["total_rows"] for data in analysis_data)

        summary = {
            "total_files": len(paths),
            "successful_files": successful_files,
            "failed_files": len(paths) - successful_files,
            "total_rows": total_rows,
            "analysis_type": analysis_type,
            "analysis_question": analysis_question
        }

        response_data = {
            "success": True,
            "results": file_results,
            "summary": summary,
            "analysis": analysis_result,
            "session_id": session_id
        }

        print(f"âœ… å¤šæª”æ¡ˆåˆ†æå®Œæˆ: {successful_files}/{len(paths)} æª”æ¡ˆæˆåŠŸåˆ†æ")
        logger.info(f"âœ… å¤šæª”æ¡ˆåˆ†æå®Œæˆ: {successful_files}/{len(paths)} æª”æ¡ˆæˆåŠŸåˆ†æ")

        # ğŸ”§ å®‰å…¨çš„ JSON åºåˆ—åŒ–æª¢æŸ¥
        try:
            result_json = json.dumps(response_data, ensure_ascii=False)
            print(f"âœ… JSON åºåˆ—åŒ–æˆåŠŸ")
            return result_json
        except TypeError as json_error:
            print(f"âŒ JSON åºåˆ—åŒ–å¤±æ•—: {json_error}")
            # æª¢æŸ¥å“ªå€‹çµæœæœ‰å•é¡Œ
            for i, result in enumerate(file_results):
                try:
                    json.dumps(result)
                except TypeError:
                    print(f"âŒ çµæœ {i} ç„¡æ³•åºåˆ—åŒ–: {result.keys()}")
                    # ç§»é™¤æœ‰å•é¡Œçš„å­—æ®µ
                    for key in list(result.keys()):
                        try:
                            json.dumps(result[key])
                        except TypeError:
                            print(f"âŒ ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„å­—æ®µ: {key}")
                            del result[key]

            # é‡æ–°å˜—è©¦åºåˆ—åŒ–
            result_json = json.dumps(response_data, ensure_ascii=False)
            print(f"âœ… æ¸…ç†å¾Œ JSON åºåˆ—åŒ–æˆåŠŸ")
            return result_json

    except Exception as e:
        logger.error(f"âŒ å¤šæª”æ¡ˆåˆ†æå¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "session_id": session_id
        }, ensure_ascii=False)


@tool
async def multi_file_data_analyzer_tool(
    files_data: str,
    analysis_question: str,
    session_id: str = "default"
) -> str:
    """
    åŸºæ–¼é è™•ç†çš„å¤šæª”æ¡ˆæ•¸æ“šé€²è¡Œåˆ†æ

    Args:
        files_data: é è™•ç†çš„æª”æ¡ˆæ•¸æ“š JSON å­—ç¬¦ä¸²
        analysis_question: åˆ†æå•é¡Œ
        session_id: æœƒè©±ID

    Returns:
        åˆ†æçµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        print(f"ğŸ”„ multi_file_data_analyzer_tool é–‹å§‹åŸ·è¡Œ")
        print(f"ğŸ“Š åˆ†æå•é¡Œ: {analysis_question}")
        print(f"ğŸ“¥ æ”¶åˆ°çš„ files_data é¡å‹: {type(files_data)}")
        print(f"ğŸ“¥ files_data é•·åº¦: {len(str(files_data))}")
        print(f"ğŸ“¥ files_data å‰100å­—ç¬¦: {str(files_data)[:100]}...")

        # è§£æé è™•ç†çš„æª”æ¡ˆæ•¸æ“š
        if isinstance(files_data, str):
            if not files_data.strip():
                print(f"âŒ files_data æ˜¯ç©ºå­—ç¬¦ä¸²")
                return json.dumps({
                    "success": False,
                    "error": "files_data åƒæ•¸ç‚ºç©º",
                    "session_id": session_id
                }, ensure_ascii=False)
            data = json.loads(files_data)
        else:
            data = files_data

        print(f"ğŸ“ æ”¶åˆ°é è™•ç†æ•¸æ“šï¼ŒåŒ…å« {len(data.get('results', []))} å€‹æª”æ¡ˆ")

        if not data.get("success"):
            return json.dumps({
                "success": False,
                "error": "é è™•ç†æ•¸æ“šç„¡æ•ˆ",
                "session_id": session_id
            }, ensure_ascii=False)

        # æå–æ‰€æœ‰æª”æ¡ˆçš„æ•¸æ“šé€²è¡Œåˆ†æ
        all_data = []
        file_summaries = []

        print(f"ğŸ” æª¢æŸ¥æ•¸æ“šçµæ§‹...")
        for i, result in enumerate(data.get("results", [])):
            print(f"ğŸ“„ æª”æ¡ˆ {i}: {result.get('filename', 'unknown')}")
            print(f"   - success: {result.get('success')}")
            print(f"   - keys: {list(result.keys())}")

            if result.get("success"):
                filename = result.get("filename", "unknown")
                platform_name = result.get("platform_name", "Unknown")
                platform_type = result.get("platform_type", "æœªçŸ¥å¹³å°")

                # ğŸ”§ é©é…æ–°çš„æ•¸æ“šçµæ§‹ï¼šä½¿ç”¨ data_summary è€Œä¸æ˜¯ data
                data_summary = result.get("data_summary", {})
                sample_data = result.get("sample_data", [])
                total_rows = result.get("total_rows", 0)
                columns = result.get("columns", [])

                print(f"   - platform: {platform_name} ({platform_type})")
                print(f"   - total_rows: {total_rows}")
                print(f"   - sample_data é•·åº¦: {len(sample_data)}")

                # ğŸ¯ é‡é»ï¼šä½¿ç”¨çµ±è¨ˆæ‘˜è¦è€Œä¸æ˜¯æ¨£æœ¬æ•¸æ“šé€²è¡Œåˆ†æ
                # sample_data åªæ˜¯å°‘é‡æ¨£æœ¬ï¼ŒçœŸæ­£çš„åˆ†ææ‡‰è©²åŸºæ–¼ data_summary ä¸­çš„çµ±è¨ˆä¿¡æ¯
                all_data.extend(sample_data)  # ä¿ç•™æ¨£æœ¬ç”¨æ–¼å±•ç¤º

                # æå–è©³ç´°çš„çµ±è¨ˆä¿¡æ¯
                column_stats = data_summary.get("column_stats", {})
                numeric_columns = data_summary.get("numeric_columns", [])
                categorical_columns = data_summary.get("categorical_columns", [])

                file_summaries.append({
                    "filename": filename,
                    "platform_name": platform_name,
                    "platform_type": platform_type,
                    "total_rows": total_rows,  # çœŸå¯¦çš„ç¸½è¡Œæ•¸
                    "columns": columns,
                    "sample_data": sample_data[:1] if sample_data else [],  # åªä¿ç•™1è¡Œæ¨£æœ¬ç”¨æ–¼æ ¼å¼åƒè€ƒ
                    "sample_note": f"ä»¥ä¸Šåƒ…ç‚º1è¡Œæ¨£æœ¬ç”¨æ–¼æ ¼å¼åƒè€ƒï¼Œå¯¦éš›æ•¸æ“šæœ‰ {total_rows} è¡Œ",
                    "numeric_columns": numeric_columns,
                    "categorical_columns": categorical_columns,
                    "column_statistics": column_stats,  # è©³ç´°çš„æ¬„ä½çµ±è¨ˆ
                    "data_shape": data_summary.get("data_shape", [total_rows, len(columns)])
                })

                print(f"   - çµ±è¨ˆæ‘˜è¦åŒ…å« {len(column_stats)} å€‹æ¬„ä½çš„è©³ç´°çµ±è¨ˆ")

        # è¨ˆç®—çœŸå¯¦çš„ç¸½è¡Œæ•¸ï¼ˆåŸºæ–¼çµ±è¨ˆæ‘˜è¦ï¼‰
        actual_total_rows = sum(summary.get("total_rows", 0) for summary in file_summaries)
        print(f"ğŸ“Š æ•¸æ“šæ‘˜è¦ï¼š{len(file_summaries)} å€‹æª”æ¡ˆï¼Œå¯¦éš›ç¸½è¡Œæ•¸ {actual_total_rows} è¡Œ")
        print(f"ğŸ“Š æ¨£æœ¬æ•¸æ“šï¼š{len(all_data)} è¡Œæ¨£æœ¬ç”¨æ–¼å±•ç¤º")

        # ä½¿ç”¨ LLM é€²è¡Œåˆ†æ
        if not llm:
            return json.dumps({
                "success": False,
                "error": "LLM ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œåˆ†æ",
                "session_id": session_id
            }, ensure_ascii=False)

        # æ§‹å»ºåˆ†æ prompt
        analysis_prompt = f"""
åŸºæ–¼ä»¥ä¸‹å¤šæª”æ¡ˆçµ±è¨ˆæ‘˜è¦å›ç­”å•é¡Œï¼š{analysis_question}

## é‡è¦èªªæ˜ï¼š
ä»¥ä¸‹æ•¸æ“šåŒ…å«å®Œæ•´çš„çµ±è¨ˆæ‘˜è¦ä¿¡æ¯ï¼Œæ¯å€‹æª”æ¡ˆçš„ total_rows æ˜¯çœŸå¯¦çš„æ•¸æ“šè¡Œæ•¸ï¼Œsample_data åªæ˜¯å°‘é‡æ¨£æœ¬ç”¨æ–¼å±•ç¤ºã€‚
è«‹åŸºæ–¼ column_statistics ä¸­çš„è©³ç´°çµ±è¨ˆä¿¡æ¯é€²è¡Œåˆ†æï¼Œè€Œä¸æ˜¯åƒ…çœ‹æ¨£æœ¬æ•¸æ“šã€‚

## æª”æ¡ˆçµ±è¨ˆæ‘˜è¦ï¼š
{json.dumps(file_summaries, ensure_ascii=False, indent=2)}

## åˆ†æè¦æ±‚ï¼š
è«‹æä¾›è©³ç´°çš„æ¯”è¼ƒåˆ†æï¼ŒåŒ…æ‹¬ï¼š

### 1. å¹³å°åŸºæœ¬ç‰¹å¾µå°æ¯”
- æ•¸æ“šè¦æ¨¡ï¼šå„å¹³å°çš„å¯¦éš›æ•¸æ“šé‡ï¼ˆtotal_rowsï¼‰
- æ¬„ä½çµæ§‹ï¼šæ•¸å€¼æ¬„ä½ vs åˆ†é¡æ¬„ä½çš„å·®ç•°
- å¹³å°é¡å‹ï¼šç¤¾ç¾¤è¨è«–ä¸² vs PTTè«–å£‡ çš„æœ¬è³ªå·®ç•°

### 2. æ•¸å€¼æ¬„ä½çµ±è¨ˆå°æ¯”
- åŸºæ–¼ column_statistics ä¸­çš„ mean, std, min, max é€²è¡Œæ¯”è¼ƒ
- ä¾‹å¦‚ï¼šé—œæ³¨æ•¸ã€ç€è¦½æ•¸ã€å¹´é½¡ç­‰æ•¸å€¼æ¬„ä½çš„åˆ†å¸ƒå·®ç•°
- æŒ‡å‡ºå“ªå€‹å¹³å°çš„ç”¨æˆ¶æ›´æ´»èºã€åƒèˆ‡åº¦æ›´é«˜

### 3. åˆ†é¡æ¬„ä½åˆ†å¸ƒå°æ¯”
- åŸºæ–¼ column_statistics ä¸­çš„ top_values é€²è¡Œæ¯”è¼ƒ
- ä¾‹å¦‚ï¼šåœ°å€åˆ†å¸ƒã€æ€§åˆ¥æ¯”ä¾‹ã€ä¸»é¡Œæ¨™ç±¤çš„å·®ç•°
- åˆ†æç”¨æˆ¶ç¾¤é«”ç‰¹å¾µå’Œå…§å®¹åå¥½

### 4. å…·é«”å·®ç•°åˆ†æ
- å›ç­”ç”¨æˆ¶çš„å…·é«”å•é¡Œ
- æä¾›å…·é«”çš„çµ±è¨ˆæ•¸å­—ä½œç‚ºæ”¯æŒ
- æŒ‡å‡ºæœ€é¡¯è‘—çš„å·®ç•°é»å’Œç‰¹è‰²

### 5. çµè«–å’Œæ´å¯Ÿ
- ç¸½çµå…©å€‹å¹³å°çš„ä¸»è¦å·®ç•°
- æä¾›åŸºæ–¼æ•¸æ“šçš„æ·±å…¥æ´å¯Ÿ
- çµ¦å‡ºä½¿ç”¨å»ºè­°æˆ–è¶¨å‹¢åˆ†æ

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨æ¸…æ™°çš„æ ¼å¼ï¼Œä¸¦å¼•ç”¨å…·é«”çš„çµ±è¨ˆæ•¸æ“šã€‚
"""

        print(f"ğŸ¤– èª¿ç”¨ LLM é€²è¡Œåˆ†æ...")
        analysis_result = await llm.ainvoke(analysis_prompt)

        response = {
            "success": True,
            "analysis": analysis_result.content if hasattr(analysis_result, 'content') else str(analysis_result),
            "file_summaries": file_summaries,
            "total_rows": len(all_data),
            "total_files": len(file_summaries),
            "session_id": session_id
        }

        print(f"âœ… å¤šæª”æ¡ˆæ•¸æ“šåˆ†æå®Œæˆ")
        return json.dumps(response, ensure_ascii=False)

    except Exception as e:
        print(f"âŒ å¤šæª”æ¡ˆæ•¸æ“šåˆ†æå¤±æ•—: {e}")
        logger.error(f"âŒ å¤šæª”æ¡ˆæ•¸æ“šåˆ†æå¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": f"å¤šæª”æ¡ˆæ•¸æ“šåˆ†æå¤±æ•—: {str(e)}",
            "session_id": session_id
        }, ensure_ascii=False)

# å°å‡ºå·¥å…·åˆ—è¡¨
MULTI_FILE_TOOLS = [
    multi_file_reader_tool,
    multi_file_filter_tool,
    multi_file_analyzer_tool
]
