"""
éˆæ´»æŒ‡ç´‹æœå°‹å·¥å…· - å¯æŒ‡å®šæœå°‹æ¬„ä½çš„é€²éšæ–‡å­—æœå°‹å·¥å…·
åŸºæ–¼åŸæœ‰çš„æŒ‡ç´‹æœå°‹æ¦‚å¿µï¼Œä½†å…è¨±ç”¨æˆ¶æŒ‡å®šè¦æœå°‹çš„æ¬„ä½åç¨±
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import logging
from datetime import datetime
import re
import math
import time
import asyncio
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Azure OpenAI Embedding é…ç½®
from openai import AzureOpenAI
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
EMBED_KEY = os.getenv("EMBED_KEY")
EMBED_END = os.getenv("EMBED_END")

# åˆå§‹åŒ– Azure OpenAI client
embed_client = None
if EMBED_KEY and EMBED_END:
    try:
        embed_client = AzureOpenAI(
            api_key=EMBED_KEY,
            api_version="2024-02-01",
            azure_endpoint=EMBED_END
        )
    except Exception as e:
        logging.warning(f"ç„¡æ³•åˆå§‹åŒ– Azure OpenAI client: {e}")

def embedder(query: str) -> List[float]:
    """å‘¼å« Azure OpenAI embedder å°‡ query è½‰å‘é‡"""
    if not embed_client:
        logging.warning("Embedding client ä¸å¯ç”¨ï¼Œè¿”å›ç©ºå‘é‡")
        return []

    try:
        response = embed_client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"Embedding è«‹æ±‚å¤±æ•—: {e}")
        return []

from supervisor_agent.core.session_data_manager import session_data_manager

logger = logging.getLogger(__name__)


class FlexibleFingerprintSearchEngine:
    """éˆæ´»æŒ‡ç´‹æœå°‹å¼•æ“ - å¯æŒ‡å®šæœå°‹æ¬„ä½"""

    def __init__(self):
        self.embedder = embedder
        self.llm_client = embed_client

        # BM25 åƒæ•¸
        self.k1 = 1.2  # term frequency saturation parameter
        self.b = 0.75  # length normalization parameter

        # è©•åˆ†æ¬Šé‡ (å¯å‹•æ…‹èª¿æ•´)
        self.weights = {
            'bm25_primary': 2.5,    # ä¸»è¦æ¬„ä½æ¬Šé‡
            'bm25_secondary': 1.2,  # æ¬¡è¦æ¬„ä½æ¬Šé‡
            'semantic': 1.5,
            'entity': 1.0
        }

    async def batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡è™•ç† embeddings ä»¥æé«˜æ€§èƒ½"""
        all_embeddings = []
        
        try:
            # æ‰¹æ¬¡è™•ç†æ‰€æœ‰æ–‡æœ¬
            if embed_client and texts:
                response = embed_client.embeddings.create(
                    model="text-embedding-3-small",
                    input=texts
                )
                all_embeddings = [data.embedding for data in response.data]
                logger.info(f"âœ… æ‰¹æ¬¡ embedding æˆåŠŸï¼Œè™•ç† {len(texts)} å€‹æ–‡æœ¬")

        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ embedding å¤±æ•—: {e}")
            # å›é€€åˆ°é€ä¸€è™•ç†
            for text in texts:
                try:
                    embedding = embedder(text)
                    all_embeddings.append(embedding)
                except:
                    all_embeddings.append([0.0] * 1536)  # é è¨­ç¶­åº¦

        return all_embeddings

    async def expand_query_keywords(self, query: str) -> List[str]:
        """ä½¿ç”¨é å®šç¾©è¦å‰‡æ“´å±•æŸ¥è©¢é—œéµå­—"""
        expansion_rules = {
            'è²¡å‹™': ['è²¡å‹™', 'finance', 'é‡‘éŒ¢', 'money', 'ä»˜æ¬¾', 'payment', 'ç™¼ç¥¨', 'invoice', 'å¸³å–®', 'bill', 'è²»ç”¨', 'cost'],
            'å®¢æˆ¶': ['å®¢æˆ¶', 'customer', 'ç”¨æˆ¶', 'user', 'æœƒå“¡', 'member', 'é¡§å®¢', 'client', 'æ¶ˆè²»è€…', 'consumer'],
            'ç”¢å“': ['ç”¢å“', 'product', 'æœå‹™', 'service', 'å“è³ª', 'quality', 'åŠŸèƒ½', 'feature', 'è¦æ ¼', 'specification', 'åƒ¹æ ¼', 'price', 'éŠ·å”®', 'sales']
        }

        # åŸºæ–¼æŸ¥è©¢å…§å®¹åŒ¹é…æ“´å±•è¦å‰‡
        expanded_keywords = [query]

        for key, keywords in expansion_rules.items():
            if key in query:
                expanded_keywords.extend(keywords)
                break

        # ç§»é™¤é‡è¤‡
        expanded_keywords = list(dict.fromkeys(expanded_keywords))

        logger.info(f"ğŸ” æŸ¥è©¢æ“´å±•: {query} -> {len(expanded_keywords)} å€‹é—œéµå­—")
        return expanded_keywords

    def _tokenize(self, text: str) -> List[str]:
        """ç°¡å–®çš„ä¸­è‹±æ–‡åˆ†è©"""
        if not text or pd.isna(text):
            return []
        
        text = str(text).lower()
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—
        text = re.sub(r'[^\w\u4e00-\u9fff\s]', ' ', text)
        # åˆ†å‰²ä¸¦éæ¿¾ç©ºå­—ç¬¦ä¸²
        tokens = [token.strip() for token in text.split() if token.strip()]
        return tokens

    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å¯¦é«”ï¼ˆé‡‘é¡ã€æ—¥æœŸç­‰ï¼‰"""
        if not text or pd.isna(text):
            return {}
        
        text = str(text)
        entities = {}
        
        # æå–é‡‘é¡
        amount_patterns = [
            r'\$[\d,]+\.?\d*',  # $1,000.00
            r'NT\$[\d,]+',      # NT$1000
            r'[\d,]+å…ƒ',        # 1000å…ƒ
            r'[\d,]+å¡Š',        # 1000å¡Š
        ]
        amounts = []
        for pattern in amount_patterns:
            amounts.extend(re.findall(pattern, text))
        if amounts:
            entities['amounts'] = amounts
        
        # æå–æ—¥æœŸ
        date_patterns = [
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',  # 2023-01-01
            r'\d{1,2}[-/]\d{1,2}[-/]\d{4}',  # 01-01-2023
        ]
        dates = []
        for pattern in date_patterns:
            dates.extend(re.findall(pattern, text))
        if dates:
            entities['dates'] = dates
        
        return entities

    def calculate_bm25_score(self, query_terms: List[str], doc_terms: List[str], 
                           doc_length: int, avg_doc_length: float, 
                           corpus_size: int, term_freq: Dict[str, int]) -> float:
        """è¨ˆç®— BM25 åˆ†æ•¸"""
        if not query_terms or not doc_terms:
            return 0.0
        
        doc_term_freq = Counter(doc_terms)
        score = 0.0
        
        for term in query_terms:
            if term in doc_term_freq:
                tf = doc_term_freq[term]
                df = term_freq.get(term, 1)
                idf = math.log((corpus_size - df + 0.5) / (df + 0.5))
                
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / avg_doc_length))
                
                score += idf * (numerator / denominator)
        
        return max(0.0, score)

    def _normalize_bm25(self, bm25_score: float, max_expected: float = 8.0) -> float:
        """ç·šæ€§æ¨™æº–åŒ– BM25 åˆ†æ•¸åˆ° 0-1 ç¯„åœ"""
        if bm25_score <= 0:
            return 0.0
        normalized = min(1.0, bm25_score / max_expected)
        return normalized

    def _calculate_entity_match(self, query_entities: Dict[str, List[str]], 
                              doc_entities: Dict[str, List[str]]) -> float:
        """è¨ˆç®—å¯¦é«”åŒ¹é…åˆ†æ•¸"""
        if not query_entities:
            return 0.0

        total_matches = 0
        total_query_entities = 0

        for entity_type in query_entities:
            query_set = set(query_entities[entity_type])
            doc_set = set(doc_entities.get(entity_type, []))
            exact_matches = len(query_set & doc_set)
            total_matches += exact_matches
            total_query_entities += len(query_set)

        return total_matches / total_query_entities if total_query_entities > 0 else 0.0

    async def search_csv_flexible(self, file_path: str, search_query: str,
                                search_columns: List[str],
                                similarity_threshold: float = 0.1,
                                max_results: int = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """åœ¨ CSV æ–‡ä»¶ä¸­é€²è¡Œéˆæ´»çš„æŒ‡ç´‹æœå°‹"""

        start_time = time.time()

        # è®€å– CSV æ–‡ä»¶
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='big5')
            except:
                df = pd.read_csv(file_path, encoding='cp1252')

        if df.empty:
            return df, {"total_processed": 0, "matches_found": 0}

        # æª¢æŸ¥æŒ‡å®šçš„æ¬„ä½æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in search_columns if col not in df.columns]
        if missing_columns:
            logger.warning(f"âš ï¸ ä»¥ä¸‹æ¬„ä½ä¸å­˜åœ¨æ–¼CSVä¸­: {missing_columns}")
            search_columns = [col for col in search_columns if col in df.columns]
            
        if not search_columns:
            logger.error("âŒ æ²’æœ‰æœ‰æ•ˆçš„æœå°‹æ¬„ä½")
            return pd.DataFrame(), {"total_processed": 0, "matches_found": 0, "error": "æ²’æœ‰æœ‰æ•ˆçš„æœå°‹æ¬„ä½"}

        logger.info(f"ğŸ” é–‹å§‹éˆæ´»æœå°‹ï¼Œè™•ç† {len(df)} ç­†è³‡æ–™ï¼Œæœå°‹æ¬„ä½: {search_columns}")

        # 1. æ“´å±•æŸ¥è©¢é—œéµå­—
        expanded_keywords = await self.expand_query_keywords(search_query)
        logger.info(f"ğŸ”‘ æ“´å±•é—œéµå­—: {expanded_keywords[:5]}...")

        # 2. è¨ˆç®—èªæ–™åº«çµ±è¨ˆä¿¡æ¯
        corpus_stats = self._calculate_corpus_stats_flexible(df, search_columns)

        # 3. æ‰¹æ¬¡è™•ç† embeddings
        logger.info(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡ embedding è™•ç†...")

        # æº–å‚™æ‰€æœ‰æ–‡æœ¬
        all_texts = []
        query_text = search_query

        for _, row in df.iterrows():
            # åˆä½µæŒ‡å®šæ¬„ä½çš„å…§å®¹
            combined_text = ' '.join([str(row.get(col, '')) for col in search_columns])
            all_texts.append(combined_text.strip())

        # æ‰¹æ¬¡è™•ç† embeddings
        all_texts_with_query = [query_text] + all_texts
        all_embeddings = await self.batch_embeddings(all_texts_with_query)

        # åˆ†é›¢æŸ¥è©¢å’Œæ–‡æª” embeddings
        query_embedding = all_embeddings[0] if all_embeddings else []
        doc_embeddings = all_embeddings[1:] if len(all_embeddings) > 1 else []

        logger.info(f"âœ… Embedding å®Œæˆï¼Œé–‹å§‹è¨ˆç®—ç›¸ä¼¼åº¦...")

        # 4. ä¸¦è¡Œè¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
        similarities = []
        detailed_scores = []

        for i, (_, row) in enumerate(df.iterrows()):
            doc_data = {col: row.get(col, '') for col in search_columns}

            # ä½¿ç”¨é è¨ˆç®—çš„ embeddings
            doc_embedding = doc_embeddings[i] if i < len(doc_embeddings) else []

            score_dict = self.calculate_flexible_similarity(
                search_query, expanded_keywords, doc_data, corpus_stats,
                query_embedding, doc_embedding, search_columns
            )

            similarities.append(score_dict['total'])
            detailed_scores.append(score_dict)

        # 5. æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•¸å’Œè©³ç´°åˆ†æ•¸
        df['_similarity_score'] = similarities

        # æ·»åŠ è©³ç´°åˆ†æ•¸æ¬„ä½
        for i, score_dict in enumerate(detailed_scores):
            df.loc[i, '_bm25_primary'] = score_dict.get('bm25_primary', 0)
            df.loc[i, '_bm25_secondary'] = score_dict.get('bm25_secondary', 0)
            df.loc[i, '_semantic'] = score_dict.get('semantic', 0)
            df.loc[i, '_entity'] = score_dict.get('entity', 0)

        # 6. éæ¿¾å’Œæ’åºçµæœ
        filtered_df = df[df['_similarity_score'] >= similarity_threshold]
        filtered_df = filtered_df.sort_values('_similarity_score', ascending=False)

        # 7. é™åˆ¶çµæœæ•¸é‡ï¼ˆå¯é¸ï¼‰
        if max_results and len(filtered_df) > max_results:
            filtered_df = filtered_df.head(max_results)

        end_time = time.time()
        processing_time = end_time - start_time

        search_info = {
            "total_processed": len(df),
            "matches_found": len(filtered_df),
            "query": search_query,
            "search_columns": search_columns,
            "expanded_keywords": expanded_keywords,
            "threshold": similarity_threshold,
            "max_results": max_results,
            "avg_similarity": filtered_df['_similarity_score'].mean() if not filtered_df.empty else 0,
            "total_score": filtered_df['_similarity_score'].sum() if not filtered_df.empty else 0,
            "processing_time": processing_time
        }

        logger.info(f"âš¡ éˆæ´»æœå°‹å®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’")

        return filtered_df, search_info

    def _calculate_corpus_stats_flexible(self, df: pd.DataFrame, search_columns: List[str]) -> Dict[str, Any]:
        """è¨ˆç®—èªæ–™åº«çµ±è¨ˆä¿¡æ¯ï¼ˆéˆæ´»ç‰ˆæœ¬ï¼‰"""
        corpus_size = len(df)

        # è¨ˆç®—æ¯å€‹æ¬„ä½çš„å¹³å‡é•·åº¦å’Œè©é »
        column_stats = {}

        for col in search_columns:
            contents = df[col].fillna('').astype(str).tolist()
            lengths = [len(self._tokenize(content)) for content in contents]
            avg_length = sum(lengths) / len(lengths) if lengths else 0

            # è¨ˆç®—è©é »
            term_freq = Counter()
            for content in contents:
                terms = set(self._tokenize(content))
                for term in terms:
                    term_freq[term] += 1

            column_stats[col] = {
                'avg_length': avg_length,
                'term_freq': dict(term_freq)
            }

        return {
            'corpus_size': corpus_size,
            'column_stats': column_stats
        }

    def calculate_flexible_similarity(self, query: str, expanded_keywords: List[str],
                                    doc_data: Dict[str, str], corpus_stats: Dict[str, Any],
                                    query_embedding: List[float], doc_embedding: List[float],
                                    search_columns: List[str]) -> Dict[str, float]:
        """è¨ˆç®—éˆæ´»çš„ç›¸ä¼¼åº¦åˆ†æ•¸"""
        scores = {}

        # æº–å‚™æŸ¥è©¢è©å½™
        query_terms = self._tokenize(' '.join(expanded_keywords))

        # 1. è¨ˆç®—æ¯å€‹æ¬„ä½çš„ BM25 åˆ†æ•¸
        primary_col = search_columns[0] if search_columns else None
        secondary_cols = search_columns[1:] if len(search_columns) > 1 else []

        # ä¸»è¦æ¬„ä½ BM25
        if primary_col and primary_col in doc_data:
            primary_content = doc_data[primary_col]
            if primary_content:
                primary_terms = self._tokenize(primary_content)
                primary_stats = corpus_stats['column_stats'].get(primary_col, {})
                bm25_primary_raw = self.calculate_bm25_score(
                    query_terms, primary_terms, len(primary_terms),
                    primary_stats.get('avg_length', 1), corpus_stats['corpus_size'],
                    primary_stats.get('term_freq', {})
                )
                scores['bm25_primary'] = self._normalize_bm25(bm25_primary_raw)
            else:
                scores['bm25_primary'] = 0.0
        else:
            scores['bm25_primary'] = 0.0

        # æ¬¡è¦æ¬„ä½ BM25ï¼ˆåˆä½µæ‰€æœ‰æ¬¡è¦æ¬„ä½ï¼‰
        if secondary_cols:
            secondary_content = ' '.join([str(doc_data.get(col, '')) for col in secondary_cols])
            if secondary_content.strip():
                secondary_terms = self._tokenize(secondary_content)
                # ä½¿ç”¨ç¬¬ä¸€å€‹æ¬¡è¦æ¬„ä½çš„çµ±è¨ˆä¿¡æ¯ä½œç‚ºä»£è¡¨
                secondary_stats = corpus_stats['column_stats'].get(secondary_cols[0], {})
                bm25_secondary_raw = self.calculate_bm25_score(
                    query_terms, secondary_terms, len(secondary_terms),
                    secondary_stats.get('avg_length', 1), corpus_stats['corpus_size'],
                    secondary_stats.get('term_freq', {})
                )
                scores['bm25_secondary'] = self._normalize_bm25(bm25_secondary_raw)
            else:
                scores['bm25_secondary'] = 0.0
        else:
            scores['bm25_secondary'] = 0.0

        # 2. èªç¾©ç›¸ä¼¼åº¦
        if query_embedding and doc_embedding and len(query_embedding) == len(doc_embedding):
            try:
                query_vec = np.array(query_embedding).reshape(1, -1)
                doc_vec = np.array(doc_embedding).reshape(1, -1)
                semantic_sim = cosine_similarity(query_vec, doc_vec)[0][0]
                scores['semantic'] = max(0.0, semantic_sim)
            except Exception as e:
                logger.warning(f"èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                scores['semantic'] = 0.0
        else:
            scores['semantic'] = 0.0

        # 3. å¯¦é«”åŒ¹é…
        full_text = ' '.join([str(doc_data.get(col, '')) for col in search_columns])
        query_entities = self._extract_entities(query)
        doc_entities = self._extract_entities(full_text)
        scores['entity'] = self._calculate_entity_match(query_entities, doc_entities)

        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        total_score = (
            self.weights['bm25_primary'] * scores['bm25_primary'] +
            self.weights['bm25_secondary'] * scores['bm25_secondary'] +
            self.weights['semantic'] * scores['semantic'] +
            self.weights['entity'] * scores['entity']
        )

        scores['total'] = total_score
        return scores


# å‰µå»ºæœå°‹å¼•æ“å¯¦ä¾‹
flexible_search_engine = FlexibleFingerprintSearchEngine()


async def flexible_fingerprint_search_csv(
    file_path: str,
    search_query: str,
    search_columns: List[str],
    session_id: str = "default",
    similarity_threshold: float = 0.1,
    max_results: int = None,
    save_results: bool = True
) -> Dict[str, Any]:
    """
    ä½¿ç”¨éˆæ´»æŒ‡ç´‹æœå°‹æŠ€è¡“åœ¨ CSV æ–‡ä»¶ä¸­æœå°‹ç›¸é—œå…§å®¹

    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        search_query: æœå°‹æŸ¥è©¢è©
        search_columns: è¦æœå°‹çš„æ¬„ä½åç¨±åˆ—è¡¨
        session_id: æœƒè©± ID
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)
        max_results: æœ€å¤§è¿”å›çµæœæ•¸ (None è¡¨ç¤ºä¸é™åˆ¶)
        save_results: æ˜¯å¦ä¿å­˜çµæœåˆ°æª”æ¡ˆ

    Returns:
        æœå°‹çµæœçš„å­—å…¸
    """
    try:
        logger.info(f"ğŸ” é–‹å§‹éˆæ´»æŒ‡ç´‹æœå°‹: '{search_query}' in {search_columns} from {file_path}")

        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            return {
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }

        # åŸ·è¡Œæœå°‹
        result_df, search_info = await flexible_search_engine.search_csv_flexible(
            file_path, search_query, search_columns, similarity_threshold, max_results
        )

        # æº–å‚™çµæœ
        results_file = None
        if save_results and not result_df.empty:
            # ç”Ÿæˆçµæœæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"flexible_fingerprint_search_{timestamp}.csv"
            results_file = session_data_manager.get_temp_file_path(session_id, filename)

            # ä¿å­˜çµæœ
            result_df.to_csv(results_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… æœå°‹çµæœå·²ä¿å­˜åˆ°: {results_file}")

        # æº–å‚™æ¨£æœ¬çµæœ
        sample_results = []
        if not result_df.empty:
            sample_df = result_df.head(5)
            for _, row in sample_df.iterrows():
                # ç§»é™¤å…§éƒ¨è©•åˆ†æ¬„ä½
                clean_row = row.drop([col for col in row.index if col.startswith('_')])
                sample_results.append({
                    "similarity_score": round(row.get('_similarity_score', 0), 4),
                    "data": clean_row.to_dict()
                })

        return {
            "success": True,
            "total_matches": search_info["matches_found"],
            "total_processed": search_info["total_processed"],
            "search_columns": search_columns,
            "results_file": results_file,
            "sample_results": sample_results,
            "search_info": search_info,
            "message": f"åœ¨æ¬„ä½ {search_columns} ä¸­æ‰¾åˆ° {search_info['matches_found']} ç­†åŒ¹é…çµæœ"
        }

    except Exception as e:
        logger.error(f"âŒ éˆæ´»æŒ‡ç´‹æœå°‹å¤±æ•—: {e}")
        return {
            "success": False,
            "error": f"æœå°‹å¤±æ•—: {str(e)}"
        }
