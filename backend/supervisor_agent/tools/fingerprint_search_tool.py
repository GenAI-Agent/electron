"""
æŒ‡ç´‹æœå°‹å·¥å…· - åŸºæ–¼ Google æŒ‡ç´‹æœå°‹æ¦‚å¿µçš„æ–‡å­—æœå°‹å·¥å…·
æ”¯æ´èªç¾©æœå°‹å’Œé—œéµå­—åŒ¹é…çš„æ··åˆæª¢ç´¢
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import logging
from datetime import datetime
import re
import math
import time
import asyncio
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Azure OpenAI Embedding é…ç½®
from openai import AzureOpenAI
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
EMBED_KEY = os.getenv("EMBED_KEY")
EMBED_END = os.getenv("EMBED_END")

# å‰µå»º Azure OpenAI embedding client
embed_client = None
if EMBED_KEY and EMBED_END:
    try:
        embed_client = AzureOpenAI(
            api_key=EMBED_KEY,
            api_version="2024-08-01-preview",
            azure_endpoint=EMBED_END,
        )
        logging.info("âœ… Azure OpenAI embedding client åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logging.error(f"âŒ Azure OpenAI embedding client åˆå§‹åŒ–å¤±æ•—: {e}")
        embed_client = None
else:
    logging.warning("âš ï¸ ç¼ºå°‘ EMBED_KEY æˆ– EMBED_END ç’°å¢ƒè®Šæ•¸")

def embedder(query: str) -> List[float]:
    """å‘¼å« Azure OpenAI embedder å°‡ query è½‰å‘é‡"""
    if not embed_client:
        logging.warning("Embedding client ä¸å¯ç”¨ï¼Œè¿”å›ç©ºå‘é‡")
        return []

    try:
        response = embed_client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"Embedding è«‹æ±‚å¤±æ•—: {e}")
        return []

from supervisor_agent.core.session_data_manager import session_data_manager

logger = logging.getLogger(__name__)


class AdvancedFingerprintSearchEngine:
    """é€²éšæŒ‡ç´‹æœå°‹å¼•æ“ - åŸºæ–¼ Google æŒ‡ç´‹æœå°‹æ¦‚å¿µ"""

    def __init__(self):
        self.embedder = embedder
        self.llm_client = embed_client  # é‡ç”¨ Azure OpenAI client åšé—œéµå­—æ“´å±•

        # BM25 åƒæ•¸
        self.k1 = 1.2  # term frequency saturation parameter
        self.b = 0.75  # length normalization parameter

        # è©•åˆ†æ¬Šé‡ (ç§»é™¤ sender å’Œ recency)
        self.weights = {
            'bm25_subject': 2.5,
            'bm25_body': 1.2,
            'semantic': 1.5,
            'entity': 1.0
        }

    def batch_embeddings(self, texts: List[str], batch_size: int = 35) -> List[List[float]]:
        """æ‰¹æ¬¡è™•ç† embeddingsï¼Œæå‡é€Ÿåº¦"""
        if not texts:
            return []

        all_embeddings = []

        try:
            # åˆ†æ‰¹è™•ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]

                # èª¿ç”¨ Azure OpenAI batch embedding
                response = embed_client.embeddings.create(
                    model="text-embedding-3-small",
                    input=batch_texts
                )

                # æå– embeddings
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)

                logger.info(f"ğŸ”„ è™•ç† embedding æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")

        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ embedding å¤±æ•—: {e}")
            # å›é€€åˆ°é€ä¸€è™•ç†
            for text in texts:
                try:
                    embedding = embedder(text)
                    all_embeddings.append(embedding)
                except:
                    all_embeddings.append([0.0] * 1536)  # é è¨­ç¶­åº¦

        return all_embeddings

    async def expand_query_keywords(self, query: str) -> List[str]:
        """ä½¿ç”¨é å®šç¾©è¦å‰‡æ“´å±•æŸ¥è©¢é—œéµå­—ï¼ˆæš«æ™‚æ›¿ä»£ LLMï¼‰"""

        # é å®šç¾©çš„é—œéµå­—æ“´å±•è¦å‰‡
        expansion_rules = {
            'è²¡å‹™': ['è²¡å‹™', 'finance', 'æœƒè¨ˆ', 'accounting', 'ç™¼ç¥¨', 'invoice', 'ä»˜æ¬¾', 'payment', 'æ”¶è²»', 'billing', 'é ç®—', 'budget', 'æˆæœ¬', 'cost', 'å ±éŠ·', 'reimbursement', 'ç¨…å‹™', 'tax', 'å¸³å–®', 'bill'],
            'å°ˆæ¡ˆ': ['å°ˆæ¡ˆ', 'project', 'é€²åº¦', 'progress', 'é–‹ç™¼', 'development', 'ç®¡ç†', 'management', 'æœƒè­°', 'meeting', 'è¨è«–', 'discussion', 'è¨ˆç•«', 'plan', 'åŸ·è¡Œ', 'execution'],
            'å®¢æˆ¶': ['å®¢æˆ¶', 'customer', 'å®¢æœ', 'service', 'æŠ•è¨´', 'complaint', 'å•é¡Œ', 'issue', 'æ”¯æ´', 'support', 'è«®è©¢', 'inquiry', 'å›è¦†', 'reply', 'è™•ç†', 'handle'],
            'å“¡å·¥': ['å“¡å·¥', 'employee', 'äººäº‹', 'HR', 'ç”³è¨´', 'grievance', 'è–ªè³‡', 'salary', 'ç¦åˆ©', 'benefit', 'å·¥ä½œ', 'work', 'ç’°å¢ƒ', 'environment', 'ç®¡ç†', 'management'],
            'ç”¢å“': ['ç”¢å“', 'product', 'æœå‹™', 'service', 'å“è³ª', 'quality', 'åŠŸèƒ½', 'feature', 'è¦æ ¼', 'specification', 'åƒ¹æ ¼', 'price', 'éŠ·å”®', 'sales']
        }

        # åŸºæ–¼æŸ¥è©¢å…§å®¹åŒ¹é…æ“´å±•è¦å‰‡
        expanded_keywords = [query]

        for key, keywords in expansion_rules.items():
            if key in query:
                expanded_keywords.extend(keywords)
                break

        # ç§»é™¤é‡è¤‡
        expanded_keywords = list(dict.fromkeys(expanded_keywords))

        logger.info(f"ğŸ” æŸ¥è©¢æ“´å±•: {query} -> {len(expanded_keywords)} å€‹é—œéµå­—")
        return expanded_keywords

    def calculate_bm25_score(self, query_terms: List[str], doc_terms: List[str],
                           doc_length: int, avg_doc_length: float,
                           corpus_size: int, term_doc_freq: Dict[str, int]) -> float:
        """è¨ˆç®— BM25 åˆ†æ•¸"""
        score = 0.0

        for term in query_terms:
            if term in doc_terms:
                # Term frequency in document
                tf = doc_terms.count(term)

                # Document frequency (how many docs contain this term)
                df = term_doc_freq.get(term, 1)

                # Inverse document frequency
                idf = math.log((corpus_size - df + 0.5) / (df + 0.5))

                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / avg_doc_length))

                score += idf * (numerator / denominator)

        return score

    def calculate_recency_score(self, date_str: str) -> float:
        """è¨ˆç®—æ™‚é–“æ–°è¿‘åº¦åˆ†æ•¸"""
        try:
            if pd.isna(date_str) or not date_str:
                return 0.0

            # è§£ææ—¥æœŸ
            doc_date = pd.to_datetime(date_str)
            now = pd.Timestamp.now()

            # è¨ˆç®—å¤©æ•¸å·®ç•°
            days_diff = (now - doc_date).days

            # æ™‚é–“è¡°æ¸›å‡½æ•¸ï¼šè¶Šæ–°çš„éƒµä»¶åˆ†æ•¸è¶Šé«˜
            if days_diff <= 7:
                return 1.0  # ä¸€é€±å…§
            elif days_diff <= 30:
                return 0.8  # ä¸€å€‹æœˆå…§
            elif days_diff <= 90:
                return 0.6  # ä¸‰å€‹æœˆå…§
            elif days_diff <= 365:
                return 0.4  # ä¸€å¹´å…§
            else:
                return 0.2  # è¶…éä¸€å¹´

        except Exception:
            return 0.0

    def check_sender_whitelist(self, sender: str) -> float:
        """æª¢æŸ¥ç™¼ä»¶äººç™½åå–®ï¼ˆå¯æ“´å±•ï¼‰"""
        if pd.isna(sender) or not sender:
            return 0.0

        sender = sender.lower()

        # å…§éƒ¨éƒµä»¶åŠ åˆ†
        if 'lenstech.ai' in sender:
            return 1.0

        # é‡è¦å®¢æˆ¶æˆ–åˆä½œå¤¥ä¼´ï¼ˆå¯é…ç½®ï¼‰
        important_domains = ['gmail.com', 'outlook.com']
        for domain in important_domains:
            if domain in sender:
                return 0.5

        return 0.0

    def create_text_fingerprint(self, text: str) -> Dict[str, Any]:
        """ç‚ºæ–‡å­—å‰µå»ºæŒ‡ç´‹"""
        if not text or pd.isna(text):
            return {
                "semantic_vector": None,
                "lexical_tokens": [],
                "entities": {},
                "text_length": 0
            }
        
        text = str(text).strip()
        
        # èªç¾©æŒ‡ç´‹ - ä½¿ç”¨ embedding
        semantic_vector = None
        if self.embedder:
            try:
                semantic_vector = self.embedder(text)
            except Exception as e:
                logger.warning(f"Embedding å¤±æ•—: {e}")
        
        # è©å½™æŒ‡ç´‹ - ç°¡å–®çš„åˆ†è©
        lexical_tokens = self._tokenize(text)
        
        # å¯¦é«”æŒ‡ç´‹ - æå–é‡‘é¡ã€æ—¥æœŸç­‰
        entities = self._extract_entities(text)
        
        return {
            "semantic_vector": semantic_vector,
            "lexical_tokens": lexical_tokens,
            "entities": entities,
            "text_length": len(text)
        }
    
    def _tokenize(self, text: str) -> List[str]:
        """ç°¡å–®åˆ†è©"""
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œä¿ç•™ä¸­è‹±æ–‡å’Œæ•¸å­—
        text = re.sub(r'[^\w\s]', ' ', text)
        tokens = text.lower().split()
        return [token for token in tokens if len(token) > 1]
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å¯¦é«”ï¼ˆé‡‘é¡ã€æ—¥æœŸç­‰ï¼‰"""
        entities = {}
        
        # é‡‘é¡åŒ¹é…
        amount_pattern = r'(?:NT\$|\$|USD|TWD|å…ƒ)\s*[\d,]+(?:\.\d{1,2})?|\d+(?:,\d{3})*(?:\.\d{1,2})?\s*(?:å…ƒ|USD|TWD)'
        amounts = re.findall(amount_pattern, text, re.IGNORECASE)
        if amounts:
            entities['amounts'] = amounts
        
        # æ—¥æœŸåŒ¹é…
        date_pattern = r'\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/]\d{4}'
        dates = re.findall(date_pattern, text)
        if dates:
            entities['dates'] = dates
        
        # é›»å­éƒµä»¶
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        if emails:
            entities['emails'] = emails
        
        return entities
    
    async def calculate_advanced_similarity(self, query: str, expanded_keywords: List[str],
                                          doc_data: Dict[str, Any], corpus_stats: Dict[str, Any]) -> Dict[str, float]:
        """
        è¨ˆç®—é€²éšç›¸ä¼¼åº¦åˆ†æ•¸ (æ¨™æº–åŒ–ç‰ˆæœ¬)
        åŸºæ–¼å…¬å¼: 2.5Ã—BM25_subject + 1.2Ã—BM25_body + 1.5Ã—cos(q,d) + 1.0Ã—EntityMatch
        æ‰€æœ‰åˆ†æ•¸éƒ½æ¨™æº–åŒ–åˆ° 0-1 ç¯„åœå¾Œå†åŠ æ¬Š
        """
        scores = {}

        # æº–å‚™æŸ¥è©¢è©å½™
        query_terms = self._tokenize(' '.join(expanded_keywords))

        # 1. BM25 for Subject
        subject = doc_data.get('subject', '')
        if subject:
            subject_terms = self._tokenize(subject)
            bm25_subject_raw = self.calculate_bm25_score(
                query_terms, subject_terms, len(subject_terms),
                corpus_stats['avg_subject_length'], corpus_stats['corpus_size'],
                corpus_stats['subject_term_freq']
            )
            # æ¨™æº–åŒ– BM25 åˆ†æ•¸ (ä½¿ç”¨ sigmoid å‡½æ•¸)
            scores['bm25_subject'] = self._normalize_bm25(bm25_subject_raw)
        else:
            scores['bm25_subject'] = 0.0

        # 2. BM25 for Body/Content
        content = doc_data.get('snippet', '') + ' ' + doc_data.get('content', '')
        if content.strip():
            content_terms = self._tokenize(content)
            bm25_body_raw = self.calculate_bm25_score(
                query_terms, content_terms, len(content_terms),
                corpus_stats['avg_content_length'], corpus_stats['corpus_size'],
                corpus_stats['content_term_freq']
            )
            # æ¨™æº–åŒ– BM25 åˆ†æ•¸
            scores['bm25_body'] = self._normalize_bm25(bm25_body_raw)
        else:
            scores['bm25_body'] = 0.0

        # 3. Semantic Similarity (å·²ç¶“æ˜¯ 0-1 ç¯„åœ)
        full_text = f"{subject} {content}".strip()
        if full_text and self.embedder:
            try:
                doc_vector = self.embedder(full_text)
                query_vector = self.embedder(query)

                if doc_vector and query_vector:
                    doc_vec = np.array(doc_vector).reshape(1, -1)
                    query_vec = np.array(query_vector).reshape(1, -1)
                    scores['semantic'] = cosine_similarity(query_vec, doc_vec)[0][0]
                else:
                    scores['semantic'] = 0.0
            except Exception as e:
                logger.warning(f"èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                scores['semantic'] = 0.0
        else:
            scores['semantic'] = 0.0

        # 4. Entity Match (å·²ç¶“æ˜¯ 0-1 ç¯„åœ)
        query_entities = self._extract_entities(query)
        doc_entities = self._extract_entities(full_text)
        scores['entity'] = self._calculate_entity_match(query_entities, doc_entities)

        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        total_score = (
            self.weights['bm25_subject'] * scores['bm25_subject'] +
            self.weights['bm25_body'] * scores['bm25_body'] +
            self.weights['semantic'] * scores['semantic'] +
            self.weights['entity'] * scores['entity']
        )

        # è¿”å›è©³ç´°åˆ†æ•¸å’Œç¸½åˆ†
        scores['total'] = total_score
        return scores

    def calculate_advanced_similarity_fast(self, query: str, expanded_keywords: List[str],
                                         doc_data: Dict[str, Any], corpus_stats: Dict[str, Any],
                                         query_embedding: List[float], doc_embedding: List[float]) -> Dict[str, float]:
        """
        å¿«é€Ÿè¨ˆç®—é€²éšç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆä½¿ç”¨é è¨ˆç®—çš„ embeddingsï¼‰
        """
        scores = {}

        # æº–å‚™æŸ¥è©¢è©å½™
        query_terms = self._tokenize(' '.join(expanded_keywords))

        # 1. BM25 for Subject
        subject = doc_data.get('subject', '')
        if subject:
            subject_terms = self._tokenize(subject)
            bm25_subject_raw = self.calculate_bm25_score(
                query_terms, subject_terms, len(subject_terms),
                corpus_stats['avg_subject_length'], corpus_stats['corpus_size'],
                corpus_stats['subject_term_freq']
            )
            scores['bm25_subject'] = self._normalize_bm25(bm25_subject_raw)
        else:
            scores['bm25_subject'] = 0.0

        # 2. BM25 for Body/Content
        content = doc_data.get('snippet', '') + ' ' + doc_data.get('content', '')
        if content.strip():
            content_terms = self._tokenize(content)
            bm25_body_raw = self.calculate_bm25_score(
                query_terms, content_terms, len(content_terms),
                corpus_stats['avg_content_length'], corpus_stats['corpus_size'],
                corpus_stats['content_term_freq']
            )
            scores['bm25_body'] = self._normalize_bm25(bm25_body_raw)
        else:
            scores['bm25_body'] = 0.0

        # 3. Semantic Similarityï¼ˆä½¿ç”¨é è¨ˆç®—çš„ embeddingsï¼‰
        if query_embedding and doc_embedding:
            try:
                query_vec = np.array(query_embedding).reshape(1, -1)
                doc_vec = np.array(doc_embedding).reshape(1, -1)
                scores['semantic'] = cosine_similarity(query_vec, doc_vec)[0][0]
            except Exception as e:
                logger.warning(f"èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                scores['semantic'] = 0.0
        else:
            scores['semantic'] = 0.0

        # 4. Entity Match
        full_text = f"{subject} {content}".strip()
        query_entities = self._extract_entities(query)
        doc_entities = self._extract_entities(full_text)
        scores['entity'] = self._calculate_entity_match(query_entities, doc_entities)

        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        total_score = (
            self.weights['bm25_subject'] * scores['bm25_subject'] +
            self.weights['bm25_body'] * scores['bm25_body'] +
            self.weights['semantic'] * scores['semantic'] +
            self.weights['entity'] * scores['entity']
        )

        scores['total'] = total_score
        return scores

    def _normalize_bm25(self, bm25_score: float, max_expected: float = 8.0) -> float:
        """
        ç·šæ€§æ¨™æº–åŒ– BM25 åˆ†æ•¸åˆ° 0-1 ç¯„åœ
        ä¿æŒæ›´å¥½çš„å€åˆ†åº¦å’Œå½±éŸ¿åŠ›
        """
        if bm25_score <= 0:
            return 0.0

        # ç·šæ€§æ¨™æº–åŒ–ï¼šBM25 åˆ†æ•¸é™¤ä»¥æœŸæœ›æœ€å¤§å€¼
        normalized = min(1.0, bm25_score / max_expected)
        return normalized

    def _calculate_entity_match(self, query_entities: Dict[str, List[str]],
                               doc_entities: Dict[str, List[str]]) -> float:
        """
        è¨ˆç®—å¯¦é«”åŒ¹é…åˆ†æ•¸ (å›åˆ°ç°¡å–®æ­£ç¢ºçš„é‚è¼¯)
        åªæœ‰ç•¶æŸ¥è©¢å’Œæ–‡æª”éƒ½æœ‰å¯¦é«”æ™‚æ‰é€²è¡ŒåŒ¹é…
        """
        # å¦‚æœæŸ¥è©¢ä¸­æ²’æœ‰å¯¦é«”ï¼Œè¿”å› 0ï¼ˆä¸åŠ åˆ†ä¹Ÿä¸æ‰£åˆ†ï¼‰
        if not query_entities:
            return 0.0

        # å¦‚æœæ–‡æª”ä¸­æ²’æœ‰å¯¦é«”ï¼Œè¿”å› 0
        if not doc_entities:
            return 0.0

        # è¨ˆç®—å¯¦é«”é¡å‹åŒ¹é…åº¦
        total_matches = 0
        total_query_entities = 0

        for entity_type in query_entities:
            query_set = set(query_entities[entity_type])
            doc_set = set(doc_entities.get(entity_type, []))

            # ç²¾ç¢ºåŒ¹é…
            exact_matches = len(query_set & doc_set)

            # å¦‚æœæ˜¯é‡‘é¡ï¼Œä¹Ÿæª¢æŸ¥æ•¸å€¼ç›¸ä¼¼æ€§
            if entity_type == 'amounts' and exact_matches == 0:
                similarity_score = self._calculate_amount_similarity(
                    query_entities[entity_type],
                    doc_entities.get(entity_type, [])
                )
                # å¦‚æœç›¸ä¼¼åº¦é«˜ï¼Œç®—ä½œéƒ¨åˆ†åŒ¹é…
                if similarity_score > 0.7:
                    exact_matches = similarity_score

            total_matches += exact_matches
            total_query_entities += len(query_set)

        # è¿”å›åŒ¹é…æ¯”ä¾‹
        return total_matches / total_query_entities if total_query_entities > 0 else 0.0

    def _calculate_amount_similarity(self, query_amounts: List[str], doc_amounts: List[str]) -> float:
        """è¨ˆç®—é‡‘é¡ç›¸ä¼¼æ€§"""
        try:
            # æå–æ•¸å€¼
            query_values = []
            for amount in query_amounts:
                value = re.findall(r'[\d,]+(?:\.\d+)?', amount.replace(',', ''))
                if value:
                    query_values.append(float(value[0]))

            doc_values = []
            for amount in doc_amounts:
                value = re.findall(r'[\d,]+(?:\.\d+)?', amount.replace(',', ''))
                if value:
                    doc_values.append(float(value[0]))

            if not query_values or not doc_values:
                return 0.0

            # è¨ˆç®—æœ€æ¥è¿‘çš„é‡‘é¡ç›¸ä¼¼åº¦
            max_similarity = 0.0
            for q_val in query_values:
                for d_val in doc_values:
                    # ä½¿ç”¨æ¯”ä¾‹è¨ˆç®—ç›¸ä¼¼åº¦
                    if q_val > 0 and d_val > 0:
                        ratio = min(q_val, d_val) / max(q_val, d_val)
                        max_similarity = max(max_similarity, ratio)

            return max_similarity

        except:
            return 0.0
    
    async def search_csv(self, file_path: str, search_query: str,
                        similarity_threshold: float = 0.1,
                        max_results: int = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """åœ¨ CSV æ–‡ä»¶ä¸­é€²è¡Œé€²éšæŒ‡ç´‹æœå°‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""

        start_time = time.time()

        # è®€å– CSV æ–‡ä»¶
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='big5')
            except:
                df = pd.read_csv(file_path, encoding='cp1252')

        if df.empty:
            return df, {"total_processed": 0, "matches_found": 0}

        logger.info(f"ğŸ” é–‹å§‹é€²éšæœå°‹ï¼Œè™•ç† {len(df)} ç­†è³‡æ–™")

        # 1. æ“´å±•æŸ¥è©¢é—œéµå­—
        expanded_keywords = await self.expand_query_keywords(search_query)
        logger.info(f"ğŸ”‘ æ“´å±•é—œéµå­—: {expanded_keywords[:5]}...")

        # 2. è¨ˆç®—èªæ–™åº«çµ±è¨ˆä¿¡æ¯
        corpus_stats = self._calculate_corpus_stats(df)

        # 3. æ‰¹æ¬¡è™•ç† embeddingsï¼ˆæ€§èƒ½å„ªåŒ–é—œéµï¼ï¼‰
        logger.info(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡ embedding è™•ç†...")

        # æº–å‚™æ‰€æœ‰æ–‡æœ¬
        all_texts = []
        query_text = search_query

        for _, row in df.iterrows():
            subject = row.get('subject', '')
            content = row.get('snippet', '') + ' ' + row.get('content', '')
            full_text = f"{subject} {content}".strip()
            all_texts.append(full_text)

        # æ‰¹æ¬¡ç²å– embeddingsï¼ˆä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰æ–‡æª” + æŸ¥è©¢ï¼‰
        all_input_texts = [query_text] + all_texts
        all_embeddings = self.batch_embeddings(all_input_texts, batch_size=35)

        if not all_embeddings:
            logger.error("âŒ Embedding è™•ç†å¤±æ•—")
            return df, {"total_processed": len(df), "matches_found": 0}

        # åˆ†é›¢æŸ¥è©¢å’Œæ–‡æª” embeddings
        query_embedding = all_embeddings[0]
        doc_embeddings = all_embeddings[1:]

        logger.info(f"âœ… Embedding å®Œæˆï¼Œé–‹å§‹è¨ˆç®—ç›¸ä¼¼åº¦...")

        # 4. ä¸¦è¡Œè¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
        similarities = []
        detailed_scores = []

        for i, (_, row) in enumerate(df.iterrows()):
            doc_data = {
                'subject': row.get('subject', ''),
                'snippet': row.get('snippet', ''),
                'content': row.get('content', ''),
            }

            # ä½¿ç”¨é è¨ˆç®—çš„ embeddings
            doc_embedding = doc_embeddings[i] if i < len(doc_embeddings) else None

            score_dict = self.calculate_advanced_similarity_fast(
                search_query, expanded_keywords, doc_data, corpus_stats,
                query_embedding, doc_embedding
            )

            similarities.append(score_dict['total'])
            detailed_scores.append(score_dict)

        # 5. æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•¸å’Œè©³ç´°åˆ†æ•¸
        df['_similarity_score'] = similarities

        # æ·»åŠ è©³ç´°åˆ†æ•¸æ¬„ä½
        for i, score_dict in enumerate(detailed_scores):
            df.loc[i, '_bm25_subject'] = score_dict['bm25_subject']
            df.loc[i, '_bm25_body'] = score_dict['bm25_body']
            df.loc[i, '_semantic'] = score_dict['semantic']
            df.loc[i, '_entity'] = score_dict['entity']

        # 6. éæ¿¾å’Œæ’åºçµæœ
        filtered_df = df[df['_similarity_score'] >= similarity_threshold]
        filtered_df = filtered_df.sort_values('_similarity_score', ascending=False)

        # 7. é™åˆ¶çµæœæ•¸é‡ï¼ˆå¯é¸ï¼‰
        if max_results and len(filtered_df) > max_results:
            filtered_df = filtered_df.head(max_results)

        end_time = time.time()
        processing_time = end_time - start_time

        search_info = {
            "total_processed": len(df),
            "matches_found": len(filtered_df),
            "query": search_query,
            "expanded_keywords": expanded_keywords,
            "threshold": similarity_threshold,
            "max_results": max_results,
            "avg_similarity": filtered_df['_similarity_score'].mean() if not filtered_df.empty else 0,
            "score_distribution": self._analyze_score_distribution(similarities),
            "total_score": filtered_df['_similarity_score'].sum() if not filtered_df.empty else 0,
            "processing_time": processing_time
        }

        logger.info(f"âš¡ æœå°‹å®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’")

        return filtered_df, search_info

    def _calculate_corpus_stats(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¨ˆç®—èªæ–™åº«çµ±è¨ˆä¿¡æ¯ï¼Œç”¨æ–¼ BM25"""
        corpus_size = len(df)

        # Subject çµ±è¨ˆ
        subjects = df['subject'].fillna('').astype(str)
        subject_lengths = [len(self._tokenize(s)) for s in subjects]
        avg_subject_length = np.mean(subject_lengths) if subject_lengths else 1.0

        # Content çµ±è¨ˆ
        contents = df['snippet'].fillna('').astype(str)
        content_lengths = [len(self._tokenize(c)) for c in contents]
        avg_content_length = np.mean(content_lengths) if content_lengths else 1.0

        # Term frequency çµ±è¨ˆ
        subject_term_freq = Counter()
        content_term_freq = Counter()

        for subject in subjects:
            terms = set(self._tokenize(subject))
            for term in terms:
                subject_term_freq[term] += 1

        for content in contents:
            terms = set(self._tokenize(content))
            for term in terms:
                content_term_freq[term] += 1

        return {
            'corpus_size': corpus_size,
            'avg_subject_length': avg_subject_length,
            'avg_content_length': avg_content_length,
            'subject_term_freq': dict(subject_term_freq),
            'content_term_freq': dict(content_term_freq)
        }

    def _analyze_score_distribution(self, scores: List[float]) -> Dict[str, float]:
        """åˆ†æåˆ†æ•¸åˆ†ä½ˆ"""
        if not scores:
            return {}

        scores_array = np.array(scores)
        return {
            'min': float(np.min(scores_array)),
            'max': float(np.max(scores_array)),
            'mean': float(np.mean(scores_array)),
            'std': float(np.std(scores_array)),
            'median': float(np.median(scores_array)),
            'q25': float(np.percentile(scores_array, 25)),
            'q75': float(np.percentile(scores_array, 75))
        }


# å…¨å±€æœå°‹å¼•æ“å¯¦ä¾‹
search_engine = AdvancedFingerprintSearchEngine()


async def fingerprint_search_csv(
    file_path: str,
    search_query: str,
    session_id: str = "default",
    similarity_threshold: float = 0.1,
    max_results: int = None,
    save_results: bool = True
) -> Dict[str, Any]:
    """
    ä½¿ç”¨æŒ‡ç´‹æœå°‹æŠ€è¡“åœ¨ CSV æ–‡ä»¶ä¸­æœå°‹ç›¸é—œå…§å®¹
    
    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        search_query: æœå°‹æŸ¥è©¢è©
        session_id: æœƒè©± ID
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0.0-1.0)
        max_results: æœ€å¤§è¿”å›çµæœæ•¸ (None è¡¨ç¤ºä¸é™åˆ¶)
        save_results: æ˜¯å¦ä¿å­˜çµæœåˆ°æª”æ¡ˆ
        
    Returns:
        æœå°‹çµæœçš„å­—å…¸
    """
    try:
        logger.info(f"ğŸ” é–‹å§‹æŒ‡ç´‹æœå°‹: {search_query} in {file_path}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            return {
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }
        
        # åŸ·è¡Œæœå°‹
        result_df, search_info = await search_engine.search_csv(
            file_path, search_query, similarity_threshold, max_results
        )
        
        # æº–å‚™çµæœ
        results_file = None
        if save_results and not result_df.empty:
            # ç”Ÿæˆçµæœæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"fingerprint_search_{timestamp}.csv"
            results_file = session_data_manager.get_temp_file_path(session_id, filename)
            
            # ä¿å­˜çµæœ
            result_df.to_csv(results_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… æœå°‹çµæœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æº–å‚™æ¨£æœ¬çµæœ
        sample_results = []
        if not result_df.empty:
            sample_df = result_df.head(5)
            for _, row in sample_df.iterrows():
                sample_results.append({
                    "similarity_score": round(row.get('_similarity_score', 0), 4),
                    "data": row.drop('_similarity_score').to_dict()
                })
        
        return {
            "success": True,
            "total_matches": search_info["matches_found"],
            "total_processed": search_info["total_processed"],
            "results_file": results_file,
            "sample_results": sample_results,
            "search_info": search_info,
            "message": f"æ‰¾åˆ° {search_info['matches_found']} ç­†åŒ¹é…çµæœ"
        }
        
    except Exception as e:
        logger.error(f"âŒ æŒ‡ç´‹æœå°‹å¤±æ•—: {e}")
        return {
            "success": False,
            "error": f"æœå°‹å¤±æ•—: {str(e)}"
        }
