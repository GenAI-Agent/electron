"""
æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·

é€™å€‹å·¥å…·è®“ Supervisor Agent å¯ä»¥è‡ªå‹•è­˜åˆ¥å’Œè™•ç†éœ€è¦æ‰¹æ¬¡è™•ç†çš„ä»»å‹™ã€‚
"""

import json
import asyncio
from typing import Dict, Any, List, Optional, Callable
from langchain_core.tools import tool

from ...src.task_memory.task_memory_manager import TaskMemoryManager
from ..utils.logger import get_logger

logger = get_logger(__name__)

# å…¨å±€ Task Memory Manager å¯¦ä¾‹
task_memory_manager = TaskMemoryManager()


@tool
async def smart_batch_processor_tool(
    session_id: str,
    task_description: str,
    data_items: List[Any],
    processing_instruction: str,
    batch_size: int = 50
) -> str:
    """
    æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…· - è‡ªå‹•è™•ç†å¤§é‡æ•¸æ“šçš„æ ¸å¿ƒå·¥å…·
    
    é€™å€‹å·¥å…·æœƒè‡ªå‹•ï¼š
    1. å‰µå»ºæ‰¹æ¬¡ä»»å‹™
    2. å¾ªç’°è™•ç†æ¯å€‹æ‰¹æ¬¡
    3. ä¿å­˜ä¸­é–“çµæœåˆ° tmp ç©ºé–“
    4. ç´¯ç©æ‰€æœ‰ tool results
    5. ç”Ÿæˆæœ€çµ‚å ±å‘Š
    
    Args:
        session_id: æœƒè©±ID
        task_description: ä»»å‹™æè¿°ï¼ˆå¦‚"åˆ†æå®¢æˆ¶è³¼è²·æ¨¡å¼"ï¼‰
        data_items: è¦è™•ç†çš„æ•¸æ“šé …ç›®åˆ—è¡¨
        processing_instruction: æ¯å€‹é …ç›®çš„è™•ç†æŒ‡ä»¤
        batch_size: æ¯æ‰¹è™•ç†çš„æ•¸é‡ï¼Œé»˜èª50
        
    Returns:
        è™•ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        total_items = len(data_items)
        task_id = f"batch_task_{session_id}_{hash(task_description) % 10000}"
        
        logger.info(f"ğŸš€ é–‹å§‹æ™ºèƒ½æ‰¹æ¬¡è™•ç†: {task_description}")
        logger.info(f"ğŸ“Š ç¸½é …ç›®æ•¸: {total_items}, æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # æ­¥é©Ÿ1: å‰µå»ºæ‰¹æ¬¡ä»»å‹™
        task_info = await task_memory_manager.create_batch_task(
            session_id=session_id,
            task_id=task_id,
            items=data_items,
            batch_size=batch_size
        )
        
        # æ­¥é©Ÿ2: åˆå§‹åŒ–çµæœç´¯ç©å™¨
        accumulated_results = []
        batch_summaries = []
        total_success = 0
        total_errors = 0
        
        # æ­¥é©Ÿ3: å¾ªç’°è™•ç†æ¯å€‹æ‰¹æ¬¡
        total_batches = (total_items + batch_size - 1) // batch_size
        
        for batch_num in range(1, total_batches + 1):
            start_idx = (batch_num - 1) * batch_size
            end_idx = min(start_idx + batch_size, total_items)
            current_batch = data_items[start_idx:end_idx]
            
            logger.info(f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (é …ç›® {start_idx+1}-{end_idx})")
            
            try:
                # è™•ç†ç•¶å‰æ‰¹æ¬¡
                batch_results = await _process_single_batch(
                    batch_data=current_batch,
                    batch_number=batch_num,
                    processing_instruction=processing_instruction,
                    task_description=task_description
                )
                
                # ç´¯ç©çµæœ
                accumulated_results.extend(batch_results["items"])
                batch_summaries.append(batch_results["summary"])
                total_success += batch_results["success_count"]
                total_errors += batch_results["error_count"]
                
                # ä¿å­˜ä¸­é–“çµæœåˆ° tmp ç©ºé–“
                await task_memory_manager.storage.save_temp_data(
                    session_id, 
                    f"batch_{batch_num}_results",
                    {
                        "batch_number": batch_num,
                        "items_processed": len(current_batch),
                        "results": batch_results["items"],
                        "summary": batch_results["summary"],
                        "timestamp": batch_results["timestamp"]
                    }
                )
                
                # ä¿å­˜ç´¯ç©çµæœ
                await task_memory_manager.storage.save_temp_data(
                    session_id,
                    f"accumulated_results_{task_id}",
                    {
                        "total_processed": end_idx,
                        "total_items": total_items,
                        "completion_percentage": (end_idx / total_items) * 100,
                        "accumulated_results": accumulated_results,
                        "batch_summaries": batch_summaries,
                        "success_count": total_success,
                        "error_count": total_errors
                    }
                )
                
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {batch_results['success_count']} æˆåŠŸ, {batch_results['error_count']} å¤±æ•—")
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} è™•ç†å¤±æ•—: {e}")
                total_errors += len(current_batch)
                
                # ä¿å­˜éŒ¯èª¤ä¿¡æ¯
                await task_memory_manager.storage.save_temp_data(
                    session_id,
                    f"batch_{batch_num}_error",
                    {
                        "batch_number": batch_num,
                        "error": str(e),
                        "failed_items": len(current_batch)
                    }
                )
        
        # æ­¥é©Ÿ4: ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = {
            "task_description": task_description,
            "total_items": total_items,
            "total_batches": total_batches,
            "batch_size": batch_size,
            "success_count": total_success,
            "error_count": total_errors,
            "success_rate": (total_success / total_items) * 100 if total_items > 0 else 0,
            "batch_summaries": batch_summaries,
            "accumulated_results": accumulated_results[:100],  # åªè¿”å›å‰100å€‹çµæœï¼Œå®Œæ•´çµæœåœ¨ tmp ä¸­
            "results_location": f"accumulated_results_{task_id}",
            "completion_status": "completed"
        }
        
        # ä¿å­˜æœ€çµ‚å ±å‘Š
        await task_memory_manager.storage.save_temp_data(
            session_id,
            f"final_report_{task_id}",
            final_report
        )
        
        logger.info(f"ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆ: {total_success}/{total_items} æˆåŠŸ")
        
        return json.dumps({
            "success": True,
            "task_id": task_id,
            "report": final_report,
            "message": f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {total_success}/{total_items} é …ç›® ({final_report['success_rate']:.1f}%)"
        }, ensure_ascii=False)
        
    except Exception as e:
        logger.error(f"âŒ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)


async def _process_single_batch(
    batch_data: List[Any],
    batch_number: int,
    processing_instruction: str,
    task_description: str
) -> Dict[str, Any]:
    """
    è™•ç†å–®å€‹æ‰¹æ¬¡çš„æ•¸æ“š
    
    é€™å€‹å‡½æ•¸æ¨¡æ“¬äº†å°æ¯å€‹æ•¸æ“šé …ç›®çš„è™•ç†é‚è¼¯
    åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œé€™è£¡æœƒèª¿ç”¨å…·é«”çš„è™•ç†é‚è¼¯
    """
    import time
    from datetime import datetime
    
    results = []
    success_count = 0
    error_count = 0
    
    # æ¨¡æ“¬è™•ç†æ¯å€‹é …ç›®
    for i, item in enumerate(batch_data):
        try:
            # é€™è£¡æ˜¯å¯¦éš›çš„è™•ç†é‚è¼¯
            # æ ¹æ“š processing_instruction è™•ç† item
            processed_result = await _process_single_item(item, processing_instruction)
            
            results.append({
                "item_index": i,
                "original_item": item,
                "processed_result": processed_result,
                "status": "success",
                "processing_time": time.time()
            })
            success_count += 1
            
        except Exception as e:
            results.append({
                "item_index": i,
                "original_item": item,
                "error": str(e),
                "status": "error",
                "processing_time": time.time()
            })
            error_count += 1
    
    # ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
    batch_summary = {
        "batch_number": batch_number,
        "items_count": len(batch_data),
        "success_count": success_count,
        "error_count": error_count,
        "success_rate": (success_count / len(batch_data)) * 100,
        "processing_instruction": processing_instruction,
        "task_description": task_description
    }
    
    return {
        "items": results,
        "summary": batch_summary,
        "success_count": success_count,
        "error_count": error_count,
        "timestamp": datetime.now().isoformat()
    }


async def _process_single_item(item: Any, instruction: str) -> Dict[str, Any]:
    """
    è™•ç†å–®å€‹æ•¸æ“šé …ç›®
    
    é€™å€‹å‡½æ•¸æ ¹æ“šæŒ‡ä»¤è™•ç†å–®å€‹é …ç›®
    åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œé€™è£¡æœƒåŒ…å«å…·é«”çš„æ¥­å‹™é‚è¼¯
    """
    # æ¨¡æ“¬è™•ç†æ™‚é–“
    await asyncio.sleep(0.01)
    
    # æ ¹æ“šæŒ‡ä»¤é¡å‹é€²è¡Œä¸åŒçš„è™•ç†
    if "åˆ†æ" in instruction:
        return {
            "analysis_result": f"å·²åˆ†æé …ç›®: {item}",
            "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2"],
            "confidence": 0.85
        }
    elif "åˆ†é¡" in instruction:
        return {
            "category": "é¡åˆ¥A",
            "confidence": 0.92,
            "features": ["ç‰¹å¾µ1", "ç‰¹å¾µ2"]
        }
    elif "æå–" in instruction:
        return {
            "extracted_data": {"key1": "value1", "key2": "value2"},
            "extraction_confidence": 0.88
        }
    else:
        return {
            "processed": True,
            "result": f"å·²è™•ç†: {item}",
            "method": "default_processing"
        }


@tool
async def get_batch_processing_status_tool(session_id: str, task_id: str) -> str:
    """
    ç²å–æ‰¹æ¬¡è™•ç†ä»»å‹™çš„è©³ç´°ç‹€æ…‹
    
    Args:
        session_id: æœƒè©±ID
        task_id: ä»»å‹™ID
        
    Returns:
        è©³ç´°ç‹€æ…‹ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        # ç²å–ç´¯ç©çµæœ
        accumulated_data = task_memory_manager.storage.load_temp_data(
            session_id, f"accumulated_results_{task_id}"
        )
        
        if accumulated_data:
            return json.dumps({
                "success": True,
                "status": accumulated_data,
                "message": f"ä»»å‹™é€²åº¦: {accumulated_data.get('completion_percentage', 0):.1f}%"
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "success": False,
                "error": "ä»»å‹™ç‹€æ…‹ä¸å­˜åœ¨"
            }, ensure_ascii=False)
            
    except Exception as e:
        logger.error(f"âŒ ç²å–æ‰¹æ¬¡è™•ç†ç‹€æ…‹å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)


# å°å‡ºå·¥å…·
def get_batch_processor_tools():
    """ç²å–æ‰¹æ¬¡è™•ç†å·¥å…·"""
    return [
        smart_batch_processor_tool,
        get_batch_processing_status_tool
    ]
