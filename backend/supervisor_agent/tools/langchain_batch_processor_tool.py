"""
æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…·

é€™å€‹å·¥å…·è®“ Supervisor Agent å¯ä»¥è‡ªå‹•è­˜åˆ¥å’Œè™•ç†éœ€è¦æ‰¹æ¬¡è™•ç†çš„ä»»å‹™ã€‚
"""

import json
import asyncio
from typing import Dict, Any, List, Optional, Callable
from langchain_core.tools import tool

import logging
import sys
from pathlib import Path

# ä¿®å¾©å°å…¥è·¯å¾‘å•é¡Œ
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

try:
    from task_memory.task_memory_manager import TaskMemoryManager
    task_memory_manager = TaskMemoryManager()
except ImportError:
    # å¦‚æœå°å…¥å¤±æ•—ï¼Œå‰µå»ºä¸€å€‹ç°¡åŒ–çš„æ›¿ä»£å“
    class SimpleTaskMemoryManager:
        def __init__(self):
            self.storage = None  # è¨­ç½®ç‚ºNoneè€Œä¸æ˜¯boolean

        async def create_batch_task(self, session_id, task_id, items, batch_size):
            return {"task_id": task_id, "total_items": len(items), "batch_size": batch_size}

        async def save_batch_result(self, session_id, task_id, result):
            return True

    task_memory_manager = SimpleTaskMemoryManager()

logger = logging.getLogger(__name__)


@tool
async def smart_batch_processor_tool(
    session_id: str,
    task_description: str,
    data_items: List[Any],
    processing_instruction: str,
    batch_size: int = 50,
    use_file_backup: bool = True
) -> str:
    """
    æ™ºèƒ½æ‰¹æ¬¡è™•ç†å·¥å…· - è‡ªå‹•è™•ç†å¤§é‡æ•¸æ“šçš„æ ¸å¿ƒå·¥å…·

    é€™å€‹å·¥å…·æœƒè‡ªå‹•ï¼š
    1. å‰µå»ºæ‰¹æ¬¡ä»»å‹™
    2. å¾ªç’°è™•ç†æ¯å€‹æ‰¹æ¬¡
    3. ä¿å­˜ä¸­é–“çµæœåˆ° tmp ç©ºé–“
    4. ç®¡ç†Tokenä½¿ç”¨ï¼Œåªä¿ç•™é€²åº¦ä¿¡æ¯
    5. ç”Ÿæˆæœ€çµ‚å ±å‘Š

    Args:
        session_id: æœƒè©±ID
        task_description: ä»»å‹™æè¿°ï¼ˆå¦‚"åˆ†æå®¢æˆ¶è³¼è²·æ¨¡å¼"ï¼‰
        data_items: è¦è™•ç†çš„æ•¸æ“šé …ç›®åˆ—è¡¨
        processing_instruction: æ¯å€‹é …ç›®çš„è™•ç†æŒ‡ä»¤
        batch_size: æ¯æ‰¹è™•ç†çš„æ•¸é‡ï¼Œé»˜èª50
        use_file_backup: æ˜¯å¦åœ¨ç·¨è¼¯å‰å‚™ä»½æ–‡ä»¶ï¼Œé»˜èªTrue

    Returns:
        è™•ç†çµæœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        total_items = len(data_items)

        # å‰µå»ºå‚™ä»½ç›®éŒ„
        import os
        import shutil
        from datetime import datetime
        from pathlib import Path

        # ä½¿ç”¨çµ±ä¸€çš„ task_memory è·¯å¾‘çµæ§‹
        session_dir = Path("task_memory") / "sessions" / session_id
        backup_dir = session_dir / "batch_processing" / f"batch_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        backup_dir.mkdir(parents=True, exist_ok=True)

        # å¦‚æœéœ€è¦æ–‡ä»¶å‚™ä»½ï¼Œå…ˆå‚™ä»½ç›¸é—œæ–‡ä»¶
        if use_file_backup:
            logger.info(f"ğŸ“ å‰µå»ºæ–‡ä»¶å‚™ä»½ç›®éŒ„: {backup_dir}")

            # æª¢æŸ¥data_itemsä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾‘
            file_paths = []
            for item in data_items:
                if isinstance(item, str) and (item.endswith('.txt') or item.endswith('.csv') or item.endswith('.json')):
                    if os.path.exists(item):
                        file_paths.append(item)
                elif isinstance(item, dict) and 'file_path' in item:
                    file_path = item['file_path']
                    if os.path.exists(file_path):
                        file_paths.append(file_path)

            # å‚™ä»½æ–‡ä»¶
            for file_path in file_paths:
                try:
                    backup_path = os.path.join(backup_dir, os.path.basename(file_path))
                    shutil.copy2(file_path, backup_path)
                    logger.info(f"âœ… å‚™ä»½æ–‡ä»¶: {file_path} â†’ {backup_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ å‚™ä»½æ–‡ä»¶å¤±æ•— {file_path}: {e}")

        # æ¨™è¨˜ç‚ºbatch processingæ¨¡å¼
        context_update = {
            "is_batch_processing": True,
            "batch_session_id": session_id,
            "backup_dir": backup_dir,
            "total_items": total_items
        }
        task_id = f"batch_task_{session_id}_{hash(task_description) % 10000}"
        
        logger.info(f"ğŸš€ é–‹å§‹æ™ºèƒ½æ‰¹æ¬¡è™•ç†: {task_description}")
        logger.info(f"ğŸ“Š ç¸½é …ç›®æ•¸: {total_items}, æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # æ­¥é©Ÿ1: å‰µå»ºæ‰¹æ¬¡ä»»å‹™
        task_info = await task_memory_manager.create_batch_task(
            session_id=session_id,
            task_id=task_id,
            items=data_items,
            batch_size=batch_size
        )
        
        # æ­¥é©Ÿ2: åˆå§‹åŒ–çµæœç´¯ç©å™¨
        accumulated_results = []
        batch_summaries = []
        total_success = 0
        total_errors = 0
        
        # æ­¥é©Ÿ3: å¾ªç’°è™•ç†æ¯å€‹æ‰¹æ¬¡
        total_batches = (total_items + batch_size - 1) // batch_size
        
        for batch_num in range(1, total_batches + 1):
            start_idx = (batch_num - 1) * batch_size
            end_idx = min(start_idx + batch_size, total_items)
            current_batch = data_items[start_idx:end_idx]
            
            logger.info(f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (é …ç›® {start_idx+1}-{end_idx})")
            
            try:
                # è™•ç†ç•¶å‰æ‰¹æ¬¡
                batch_results = await _process_single_batch(
                    batch_data=current_batch,
                    batch_number=batch_num,
                    processing_instruction=processing_instruction,
                    task_description=task_description
                )
                
                # ç´¯ç©çµæœ
                accumulated_results.extend(batch_results["items"])
                batch_summaries.append(batch_results["summary"])
                total_success += batch_results["success_count"]
                total_errors += batch_results["error_count"]
                
                # ä¿å­˜ä¸­é–“çµæœåˆ° tmp ç©ºé–“
                try:
                    if hasattr(task_memory_manager, 'storage') and task_memory_manager.storage:
                        await task_memory_manager.storage.save_temp_data(
                            session_id,
                            f"batch_{batch_num}_results",
                            {
                                "batch_number": batch_num,
                                "items_processed": len(current_batch),
                                "results": batch_results["items"],
                                "summary": batch_results["summary"],
                                "timestamp": batch_results["timestamp"]
                            }
                        )
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜ä¸­é–“çµæœå¤±æ•—: {e}")
                
                # ä¿å­˜ç´¯ç©çµæœ
                try:
                    if hasattr(task_memory_manager, 'storage') and task_memory_manager.storage:
                        await task_memory_manager.storage.save_temp_data(
                            session_id,
                            f"accumulated_results_{task_id}",
                            {
                                "total_processed": end_idx,
                                "total_items": total_items,
                                "completion_percentage": (end_idx / total_items) * 100,
                                "accumulated_results": accumulated_results,
                                "batch_summaries": batch_summaries,
                                "success_count": total_success,
                                "error_count": total_errors
                            }
                        )
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜ç´¯ç©çµæœå¤±æ•—: {e}")
                
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {batch_results['success_count']} æˆåŠŸ, {batch_results['error_count']} å¤±æ•—")
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} è™•ç†å¤±æ•—: {e}")
                total_errors += len(current_batch)
                
                # ä¿å­˜éŒ¯èª¤ä¿¡æ¯
                try:
                    if hasattr(task_memory_manager, 'storage') and task_memory_manager.storage:
                        await task_memory_manager.storage.save_temp_data(
                            session_id,
                            f"batch_{batch_num}_error",
                            {
                                "batch_number": batch_num,
                                "error": str(e),
                                "failed_items": len(current_batch)
                            }
                        )
                except Exception as storage_e:
                    logger.warning(f"âš ï¸ ä¿å­˜éŒ¯èª¤ä¿¡æ¯å¤±æ•—: {storage_e}")
        
        # æ­¥é©Ÿ4: ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = {
            "task_description": task_description,
            "total_items": total_items,
            "total_batches": total_batches,
            "batch_size": batch_size,
            "success_count": total_success,
            "error_count": total_errors,
            "success_rate": (total_success / total_items) * 100 if total_items > 0 else 0,
            "batch_summaries": batch_summaries,
            "accumulated_results": accumulated_results[:100],  # åªè¿”å›å‰100å€‹çµæœï¼Œå®Œæ•´çµæœåœ¨ tmp ä¸­
            "results_location": f"accumulated_results_{task_id}",
            "completion_status": "completed"
        }
        
        # ä¿å­˜æœ€çµ‚å ±å‘Š
        try:
            if hasattr(task_memory_manager, 'storage') and task_memory_manager.storage:
                await task_memory_manager.storage.save_temp_data(
                    session_id,
                    f"final_report_{task_id}",
                    final_report
                )
            else:
                # ç°¡åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä¿å­˜åˆ°æ–‡ä»¶
                import os
                temp_dir = f"tmp/{session_id}"
                os.makedirs(temp_dir, exist_ok=True)
                with open(f"{temp_dir}/final_report_{task_id}.json", 'w', encoding='utf-8') as f:
                    json.dump(final_report, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜æœ€çµ‚å ±å‘Šå¤±æ•—: {e}")
        
        logger.info(f"ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆ: {total_success}/{total_items} æˆåŠŸ")
        
        return json.dumps({
            "success": True,
            "task_id": task_id,
            "report": final_report,
            "message": f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {total_success}/{total_items} é …ç›® ({final_report['success_rate']:.1f}%)"
        }, ensure_ascii=False)
        
    except Exception as e:
        logger.error(f"âŒ æ™ºèƒ½æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)


async def _process_single_batch(
    batch_data: List[Any],
    batch_number: int,
    processing_instruction: str,
    task_description: str
) -> Dict[str, Any]:
    """
    è™•ç†å–®å€‹æ‰¹æ¬¡çš„æ•¸æ“š
    
    é€™å€‹å‡½æ•¸æ¨¡æ“¬äº†å°æ¯å€‹æ•¸æ“šé …ç›®çš„è™•ç†é‚è¼¯
    åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œé€™è£¡æœƒèª¿ç”¨å…·é«”çš„è™•ç†é‚è¼¯
    """
    import time
    from datetime import datetime
    
    results = []
    success_count = 0
    error_count = 0
    
    # æ¨¡æ“¬è™•ç†æ¯å€‹é …ç›®
    for i, item in enumerate(batch_data):
        try:
            # é€™è£¡æ˜¯å¯¦éš›çš„è™•ç†é‚è¼¯
            # æ ¹æ“š processing_instruction è™•ç† item
            processed_result = await _process_single_item(item, processing_instruction)
            
            results.append({
                "item_index": i,
                "original_item": item,
                "processed_result": processed_result,
                "status": "success",
                "processing_time": time.time()
            })
            success_count += 1
            
        except Exception as e:
            results.append({
                "item_index": i,
                "original_item": item,
                "error": str(e),
                "status": "error",
                "processing_time": time.time()
            })
            error_count += 1
    
    # ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
    batch_summary = {
        "batch_number": batch_number,
        "items_count": len(batch_data),
        "success_count": success_count,
        "error_count": error_count,
        "success_rate": (success_count / len(batch_data)) * 100,
        "processing_instruction": processing_instruction,
        "task_description": task_description
    }
    
    return {
        "items": results,
        "summary": batch_summary,
        "success_count": success_count,
        "error_count": error_count,
        "timestamp": datetime.now().isoformat()
    }


async def _process_single_item(item: Any, instruction: str) -> Dict[str, Any]:
    """
    è™•ç†å–®å€‹æ•¸æ“šé …ç›® - æ”¯æ´çœŸæ­£çš„æ–‡ä»¶æ“ä½œå’Œä»»å‹™è™•ç†

    é€™å€‹å‡½æ•¸æ ¹æ“šæŒ‡ä»¤è™•ç†å–®å€‹é …ç›®ï¼Œæ”¯æ´ï¼š
    1. æ–‡ä»¶å¯«å…¥/ç·¨è¼¯æ“ä½œ
    2. æ•¸æ“šåˆ†æä»»å‹™
    3. å…¶ä»–é‡è¤‡æ€§ä»»å‹™
    """
    import os
    from datetime import datetime

    try:
        # è§£ææŒ‡ä»¤å’Œé …ç›®
        if isinstance(item, dict):
            task_data = item
        else:
            task_data = {"content": str(item)}

        # æ ¹æ“šæŒ‡ä»¤é¡å‹é€²è¡Œä¸åŒçš„è™•ç†
        if "å¯«å…¥" in instruction or "æ·»åŠ " in instruction or "åŠ å¯«" in instruction:
            # æ–‡ä»¶å¯«å…¥ä»»å‹™
            file_path = task_data.get("file_path", "")
            content = task_data.get("content", str(item))

            if file_path and os.path.exists(os.path.dirname(file_path)):
                try:
                    # è®€å–ç¾æœ‰å…§å®¹
                    if os.path.exists(file_path):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            existing_content = f.read()
                    else:
                        existing_content = ""

                    # æ·»åŠ æ–°å…§å®¹
                    new_content = existing_content + content + "\n"

                    # å¯«å…¥æ–‡ä»¶
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(new_content)

                    return {
                        "success": True,
                        "action": "file_write",
                        "file_path": file_path,
                        "content_added": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as e:
                    return {
                        "success": False,
                        "action": "file_write",
                        "error": str(e),
                        "file_path": file_path
                    }
            else:
                return {
                    "success": False,
                    "action": "file_write",
                    "error": "Invalid file path",
                    "file_path": file_path
                }

        elif "åˆ†æ" in instruction:
            # æ•¸æ“šåˆ†æä»»å‹™
            await asyncio.sleep(0.01)  # æ¨¡æ“¬è™•ç†æ™‚é–“
            return {
                "success": True,
                "action": "analysis",
                "analysis_result": f"å·²åˆ†æé …ç›®: {item}",
                "key_insights": ["æ•¸æ“šæ¨¡å¼è­˜åˆ¥", "è¶¨å‹¢åˆ†æ"],
                "confidence": 0.85,
                "timestamp": datetime.now().isoformat()
            }

        elif "åˆ†é¡" in instruction:
            # åˆ†é¡ä»»å‹™
            await asyncio.sleep(0.01)
            return {
                "success": True,
                "action": "classification",
                "category": "é¡åˆ¥A",
                "confidence": 0.92,
                "features": ["ç‰¹å¾µ1", "ç‰¹å¾µ2"],
                "timestamp": datetime.now().isoformat()
            }

        elif "æå–" in instruction:
            # æ•¸æ“šæå–ä»»å‹™
            await asyncio.sleep(0.01)
            return {
                "success": True,
                "action": "extraction",
                "extracted_data": {"key1": "value1", "key2": "value2"},
                "extraction_confidence": 0.88,
                "timestamp": datetime.now().isoformat()
            }

        else:
            # é»˜èªè™•ç†
            await asyncio.sleep(0.01)
            return {
                "success": True,
                "action": "default_processing",
                "result": f"å·²è™•ç†: {item}",
                "method": "batch_processing",
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ è™•ç†å–®å€‹é …ç›®å¤±æ•—: {e}")
        return {
            "success": False,
            "action": "error",
            "error": str(e),
            "item": str(item),
            "timestamp": datetime.now().isoformat()
        }


@tool
async def get_batch_processing_status_tool(session_id: str, task_id: str) -> str:
    """
    ç²å–æ‰¹æ¬¡è™•ç†ä»»å‹™çš„è©³ç´°ç‹€æ…‹
    
    Args:
        session_id: æœƒè©±ID
        task_id: ä»»å‹™ID
        
    Returns:
        è©³ç´°ç‹€æ…‹ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        # ç²å–ç´¯ç©çµæœ
        accumulated_data = task_memory_manager.storage.load_temp_data(
            session_id, f"accumulated_results_{task_id}"
        )
        
        if accumulated_data:
            return json.dumps({
                "success": True,
                "status": accumulated_data,
                "message": f"ä»»å‹™é€²åº¦: {accumulated_data.get('completion_percentage', 0):.1f}%"
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "success": False,
                "error": "ä»»å‹™ç‹€æ…‹ä¸å­˜åœ¨"
            }, ensure_ascii=False)
            
    except Exception as e:
        logger.error(f"âŒ ç²å–æ‰¹æ¬¡è™•ç†ç‹€æ…‹å¤±æ•—: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)


# å°å‡ºå·¥å…·
def get_batch_processor_tools():
    """ç²å–æ‰¹æ¬¡è™•ç†å·¥å…·"""
    return [
        smart_batch_processor_tool,
        get_batch_processing_status_tool
    ]
